user_input,retrieved_contexts,response,reference,faithfulness,answer_relevancy,context_precision,context_recall,context_entity_recall,answer_correctness
"What are the required components of a dissertation proposal in the SJSU Ed.D. program?,","['A desk in a classroom with a blackboard behind it', 'a desk with a laptop and holding a disc', 'a woman writing something down on paper while the laptop sits on the table ']",The required components of a dissertation proposal in the SJSU Ed.D. program are not mentioned in the context provided.,"The dissertation proposal includes a draft of the introduction, a preliminary literature review, methodology, and references. Students must present this proposal to their doctoral committee for approval, and approval is necessary before the data collection phase.,",0.5,0.0,0.0,0.0,0.0,0.20592174473609426
"When is the literature review milestone completed during the doctoral program?,","[""During the 3rd summer of the program, students complete the required courses, and at the same \ntime continue work on the dissertation proposal + IRB.  \n1. LITERATURE REVIEW + CONCEPTUAL FRAMEWORK \nA preliminary literature review is required of each doctoral student during the 2nd summer of the \ndoctoral program. The faculty member teaching the 2nd Proseminar (EDD 591B) will require the \nliterature review as part of the course requirements. The literature review milestone is deﬁned as an \nintegrative review of literature that cuts across individual courses and is based on a student’s \nparticular area of interest and research. It is expected that the literature review will include a 25-30 \npage review of relevant literature, followed by 3-5 page description of the research methods \nanticipated for answering the proposed questions. The student’s advisor will also review student \nperformance on the literature review, which is graded on a pass/fail basis. Successful completion of \nthe literature review is required for the student to continue courses and to move to the next phase of \nthe program, the dissertation proposal.\n    \n2. DISSERTATION PROPOSAL & IRB\nFollowing the literature review milestone and the required coursework, each student is expected to \ncomplete a dissertation proposal and formally present the proposal to his/her doctoral committee, \nincluding the chair, another member of the SJSU faculty, and third member of the committee \nselected from the ﬁeld in which the student works. It is expected that all committee members will \nhave a terminal degree.\nThe traditional dissertation proposal consists of a draft of the introduction, preliminary literature \nreview, and methodology and references. The proposal informs committee members of tentative \nresearch plans. The student is also required to submit a separate application based on the research \nproposal to the University's Institutional Review Board (IRB) based on the actual research proposed \nin the study and proposed data collection activities. The data collection phase cannot be formally \nstarted until IRB application is approved. \nApproval of the dissertation proposal requires a formal meeting of the candidate and dissertation \ncommittee, in which the student presents his/her proposal and receives feedback. In order to move \nforward, the Dissertation Committee must formally (in writing) approve for the dissertation proposal \nand its willingness to support student’s movement to accomplish the research proposed. While \nsuggestions for improving the proposal are typical, the committee must ultimately decide if the \nproposal is sufﬁciently developed as a roadmap of what is to be accomplished in the dissertation \nresearch. At this stage, the Committee has three options: 1) to approve the student moving forward \nto the dissertation; 2) to approve the student moving forward with revisions as stated in the approval; \nand 3) to require that the student submit a revised dissertation proposal at a separate meeting of the \ndissertation committee. All students will have two chances to successfully defend the dissertation \nproposal. Successful completion of the dissertation proposal is required for a student to move to the \nnext phase of the program, the dissertation. \nOral Defense of Dissertation Proposal and Dissertation\nThe Dissertation Committee provides guidance to the candidate, is responsible for approving the \nﬁnal written dissertation and oral defense, and for assuring dissertation conformance with program \nrequirements and rigorous research expectations as established by the College Doctoral Faculty, the \nUniversity Graduate Council, and Graduate School.\nBoth the preliminary defense of the proposal and ﬁnal defense of the dissertation provide a \nmechanism for the faculty to assess oral expression, mastery of advanced disciplinary knowledge, \nDISSERTATION \nPROCESS: 3 STEPS\n6"", ""- Student becomes familiar with theoretical \nframework and conceptual models that have \nguided the research in the selected topic of \ninterest\nforms depending upon the type of research undertaken. The conventional ﬁve-chapter dissertation, \nconsisting of an introduction, literature review, methodology, results/analysis, and conclusions/\nrecommendations may be revised to reﬂect a speciﬁc design and purpose.\nFinal Examination: Oral Defense of Dissertation \nAt the ﬁnal oral defense, the student will make a public presentation of the dissertation research. The \nﬁnal oral defense is conducted as an open forum guided by the candidate's Chair and Dissertation \nCommittee. The purpose of this formal meeting is: (1) to examine and assess the quality of the \ndissertation, (2) to evaluate the ability of the student to present research, and (3) to provide an \nopportunity to share the research with the campus community.\nUnanimous agreement of the Dissertation Committee is required for approval of the dissertation and \nrecommendation that the Ed.D. degree be conferred. There are three possible outcomes of the ﬁnal \ndefense:\n 1. The oral defense is satisfactory and the manuscript is accepted by the Dissertation \n Committee as submitted, or with only minor copy edition revisions.\n 2. The oral defense is satisfactory but there is a need for substantive revisions of the \n manuscript.\n 3. The oral defense is judged to be unsatisfactory. The decision may be reached because the  \n dissertation is judged to be unacceptable or that the student fails to present it satisfactorily. A  \n second oral defense may be scheduled when the chair determines that the student is   \n prepared and the committee members agree that the required remediation has been   \n accomplished. The following table summarizes the milestones throughout doctoral program  \n and through the research process culminating with completion of the dissertation.\nTentative Milestones for Students in the SJSU Ed.D. Program in Educational Leadership \nStep\nTopic \nDevelopment\n- Student becomes familiar with the literature in \nthe area of proposed or potential topic interest\nTask Semester\nYears 1 and 2\n- Student becomes familiar with previous \nresearch conducted in topic area\n- Student becomes familiar with the \nmethodologies used in research in the selected \ntopic of interest\nDissertation \nResearch \nTopic \nIdentiﬁcation\n- Student should seek advice to ensure that \ntheir topic of research will be manageable in \nscope and possible to complete within the time \nframe of the program\nSpring, Year 1\nLiterature \nReview with \nReference to \nMethods\n- Student will complete literature review/ \npre-proposal as part of EDD 591B; Successful \ncompletion is required to move forward\nSummer, Year 2\n8"", '- Two Dissertation Committee Members \nselected from SJSU faculty and third member \nfrom P-12 educational system or workplace\nDissertation \nCommittee\n- Dissertation Chair is identiﬁed Spring, Year 2\n- Student meets with Dissertation Chair to \nreview research topic, and develop a timeline for \nthe completion of the dissertation\nDissertation \nResearch \nProposal\n- Student successfully defends research \nproposal before Dissertation Committee \nMembers\nSpring, Year 2 & \nSummer Year 3\nInstitution \nReview Board \n(IRB)\n- Dissertation Chair and committee members \ncomplete documentation for the successful \ndefense of the dissertation proposal and \nsubmits to the EdD Director\n- Student completes SJSU IRB application with \nDissertation Chair (and possibly with committee \nmembers)\n- Student submits completed IRB application \nand all supporting documents to IRB Program \nCoordinator for approval\nSpring & \nSummer, Year 2\nSpring, Year 2 &\nSummer, Year 3\nSummer & Fall, \nYear 3\nDissertation \nWriting\n- Student completes chapters 1-5 of the \ndissertation with feedback from Chair and \ncommittee members\nOngoing, Year 3\nOral Defense \nof Dissertation\n- With completed draft of dissertation, student \ncompletes dissertation defense scheduling form \nand submits to EdD Program to schedule \ndefense; Copy of completed draft of entire \ndissertation must be sent to all committee \nmembers at least two weeks prior to holding \ndefense\nYear 3\nSubmit Final \nCopy and \nSigned \nPaperwork to \nGraduate \nStudies for \nDissertation \nFormat \nReview and \nPublication\nAfter successful defense and all required \nsignatures and changes in place, dissertation \nand paperwork is submitted to Graduate \nStudies for ﬁnal format review (See Master’s \nThesis and Doctoral Dissertation Guidelines at: \nhttps://www.sjsu.edu/cgs/current-\nstudents/thesis-and-dissertation-information). \nAt this point, Graduate Studies makes the \ndecisiion to approve or reject the dissertation \nformatting. When the dissertation is approved, \nthe corrected ﬁnal copy of the dissertation is \nuploaded to ProQuest. If rejected, the student \nmust wait until the next submission dates and \nupdate graduation application (small fee \ninvolved).\nProQuest \nsubmission \ndeadline listed \non Graduate \nStudies \nwebsite. This \ndeadline is \nrevised every \nyear.\nAs Dissertation \nproposal is \napproved\n- Student defends dissertation and makes all \nchanges required by the Committee; All \ncommittee member signatures required on the \nDissertation Approval Form\n9']",The literature review milestone is completed during the 2nd summer of the doctoral program.,"The preliminary literature review is completed during the 2nd summer of the doctoral program as part of the EDD 591B course requirements.,",1.0,0.9855004909331471,0.99999999995,1.0,0.249999999375,0.734356061190833
"What happens if a student fails the oral defense of their dissertation?,","['a man in glasses sits in front of some wine glasses ', 'Man looking over paperwork while holding pen on desk.', 'The desk with a laptop has papers strewn all over it.']",There is no information provided in the context about what happens if a student fails the oral defense of their dissertation.,"If the oral defense is unsatisfactory, the committee may require revisions or schedule a second defense after the student is adequately prepared.,",1.0,0.0,0.0,0.0,0.0,0.2127243857110574
"What are the possible formats for a dissertation in the SJSU Ed.D. program?,","['a desk with a laptop and holding a disc', 'a woman writing something down on paper while the laptop sits on the table ', 'It is all  kinds of books on the book selves. ']","Based on the context provided, there is no information related to the possible formats for a dissertation in the SJSU Ed.D. program.","Students can choose from traditional formats (five chapters: introduction, literature review, methodology, results/analysis, conclusions/recommendations) or alternative formats like dissertations-in-practice, action research, and joint projects.,",0.0,0.0,0.0,0.0,0.0,0.2066914429507265
"How many units of dissertation credit are required in the final year?,","['a desk with a laptop and holding a disc', 'a long disk with several laptops and PC computers ', 'a person sitting on  desk operating a laptop next to two other ']",There is no mention of dissertation credit or any academic requirements in the context provided.,"Students must enroll in 12 units of dissertation credit during the final year, divided into 6 units for the fall and 6 units for the spring semesters.,",1.0,0.8888067081958614,0.0,0.0,0.0,0.20211969737739283
"What is the role of the Institutional Review Board (IRB) in the dissertation process?,","['Man looking over paperwork while holding pen on desk.', 'a woman writing something down on paper while the laptop sits on the table ', 'A remote and some booklets on a table.']",There is no information provided in the context about the role of the Institutional Review Board (IRB) in the dissertation process.,"Students must submit their research proposal to the IRB for approval before starting data collection. The IRB evaluates ethical considerations in the proposed research.,",1.0,0.0,0.0,0.0,0.0,0.2069749750893196
"Who can serve on a dissertation committee?,","['a couple of people sitting around a table ', 'Man looking over paperwork while holding pen on desk.', 'a person sitting on  desk operating a laptop next to two other ']",The person sitting on the desk operating a laptop next to two other people can serve on a dissertation committee.,"The dissertation committee comprises three members: two tenured/tenure-track faculty from SJSU (including the chair) and a third member with a terminal degree from the educational field.,",0.6666666666666666,0.9842753532890197,0.0,0.0,0.0,0.593860690389274
"What are the key purposes of the final oral defense?,","['A man standing at a podium at a USINDO luncheon ', 'a man in glasses sits in front of some wine glasses ', 'a couple of people sitting around a table ']","The key purposes of the final oral defense are to present and defend the research findings and conclusions to a panel of experts, as well as to demonstrate the candidate's knowledge and understanding of the subject matter.","The final oral defense evaluates the quality of the dissertation, the student’s presentation skills, and their ability to share research findings with the academic community.,",0.0,0.9905870109997216,0.0,0.0,0.0,0.6808959246214721
"What resources are available for dissertation formatting guidelines?,","['A remote and some booklets on a table.', 'a desk with a laptop and holding a disc', 'The desk with a laptop has papers strewn all over it.']",The context does not mention any specific resources available for dissertation formatting guidelines.,"Formatting guidelines are provided by the SJSU Graduate Studies division and reference APA style requirements.,",1.0,0.0,0.0,0.0,0.0,0.2056453900027101
"What is the timeline for selecting a dissertation chair and committee?,","['Man wearing slacks and tie sitting in chair.', 'Man looking over paperwork while holding pen on desk.', 'A chair by a window in a living room']",There is no information provided in the context about the timeline for selecting a dissertation chair and committee.,"The dissertation chair is identified in the spring of the 2nd year, with the full committee formed by the summer of the same year.",1.0,0.0,0.0,0.0,0.0,0.21633643757849325
"What experience does Charul have with LangChain?,","[""A black and brown dog wearing a chain around it's neck."", 'there is a goat that has a leash on ', 'Man wearing slacks and tie sitting in chair.']",There is no mention of Charul or LangChain in the context provided.,"During their internship at ThermoFisher Scientific, Charul built a Retrieval Augmented Generation (RAG) pipeline using LangChain, leveraging OpenAI APIs and ChromaDB vector store. Performance was evaluated using the RAGAS tool.,",1.0,0.9173940472627636,0.0,0.0,0.0,0.19718119367571707
"What is Charul's educational background?,","['A man sitting on a bed looking at a book ', 'A man in grey shirt sitting in chair on cellphone.', 'A nerdy young man in a baseball cap and leopard print bathrobe stands next to a bed. ']",There is no information provided in the context about Charul's educational background.,"Charul is pursuing an M.S. in Computer Science at San Jose State University with a GPA of 3.9/4 (Jan 2023 – Dec 2024) and holds a B.S. in Computer Science and Engineering from Mody University with a GPA of 4/4 (Jul 2014 – May 2018).,",1.0,0.0,0.0,0.0,0.0,0.20896742064531515
"What technologies does Charul have expertise in?,","['A man in grey shirt sitting in chair on cellphone.', 'a person sitting on  desk operating a laptop next to two other ', 'Man using his laptop computer in a dark room ']","Based on the context provided, it is not possible to determine what technologies Charul has expertise in as the context only mentions a man in a grey shirt sitting in a chair on a cellphone.","Charul has skills in technologies like Microservices, RabbitMQ, Kafka, REST APIs, Spring Boot, Flask, Redis, MongoDB, Spark, MapReduce, AWS, Docker, Kubernetes, and CI/CD pipelines.,",1.0,0.0,0.0,0.0,0.0,0.20666633653211983
"What project demonstrates Charul’s microservices expertise?,","['A man in grey shirt sitting in chair on cellphone.', 'A man wearing a gray vest and a red stripe tie.', 'a person sitting on  desk operating a laptop next to two other ']",There is no mention of Charul or any project related to microservices in the context provided.,"In the FitFlair Clothing App project, Charul designed a distributed microservices architecture with Node.js and Spring Boot, integrating an API Gateway and Kafka for async communication, improving product catalog API load times by 65%.,",1.0,0.9219505919582717,0.0,0.0,0.0,0.20526812339912376
"What was Charul's role at Société Générale?,","['epaulettes, the Grand Duke spoke four languages and read six. Never to\nwed, he represented his country at Portsmouth, managed three estates, and\ngenerally prized industry over nonsense. But before all of that, he had\nserved alongside the Count’s father as a devil-may-care cadet in the cavalry.\nThus had the Grand Duke become the Count’s watchful guardian. And\nwhen the Count’s parents succumbed to cholera within hours of each other\nin 1900, it was the Grand Duke who took the young Count aside and\nexplained that he must be strong for his sister’s sake; that adversity presents\nitself in many forms; and that if a man does not master his circumstances\nthen he is bound to be mastered by them.\nThe Count ran his hand across the desk’s dimpled surface.\nHow many of the Grand Duke’s words did those faint indentations\nreflect? Here over forty years had been written concise instructions to\ncaretakers; persuasive arguments to statesmen; exquisite counsel to friends.\nIn other words, it was a desk to be reckoned with.\nEmptying his glass, the Count pushed his chair back and sat on the floor.\nHe ran his hand behind the desk’s right front leg until he found the catch.\nWhen he pressed it, a seamless door opened to reveal a velvet-lined hollow\nthat, like the hollows in the other three legs, was stacked with pieces of\ngold.', 'corner of the bar, laid his head on his arms, and dreamed of the Sugar Plum\nFairy.\nTaking this as a signal, the Count prepared to push back his stool, but\nthe young Brit was refilling his glass.\n“The caviar was a stroke of genius,” he said. “But how did you manage\nit? You never left our sight.”\n“A magician never reveals his secrets.”\nThe Brit laughed. Then he studied the Count as if with renewed\ncuriosity.\n“Who are you?”\nThe Count shrugged.\n“I am someone you have met in a bar.”\n“No. That’s not quite it. I know a man of erudition when I meet one.\nAnd I heard how the bartender referred to you. Who are you, really?”\nThe Count offered a self-deprecating smile.\n“At one time, I was Count Alexander Ilyich Rostov—recipient of the\nOrder of Saint Andrew, member of the Jockey Club, Master of the\nHunt. . . .”\nThe young Brit held out his hand.\n“Charles Abernethy—presumptive heir to the Earl of Westmorland,\nfinancier’s apprentice, and bowman of the losing Cambridge crew at\nHenley in 1920.”\nThe two gentlemen shook hands and drank. And then the presumptive\nheir to the Earl of Westmorland studied the Count again. “This must have\nbeen quite a decade for you. . . .”\n“You could put it that way,” said the Count.\n“Did you try to leave after the Revolution?”\n“On the contrary, Charles; I came back because of it.”\nCharles looked at the Count in surprise.\n“You came back?”\n“I was in Paris when the Hermitage fell. I had left the country before the\nwar due to certain . . . circumstances.”\n“You weren’t an anarchist, were you?”\nThe Count laughed.\n“Hardly.”\n“Then what?”', '“Simple. Dine with me once a month in this very room. Speak with me\nin French and English. Share with me your impressions of Western\nsocieties. And in exchange . . .”\nGlebnikov let his sentence trail off, not to imply the paucity of what he\ncould do for the Count, but rather to suggest the abundance.\nBut the Count raised a hand to stay any talk of exchanges.\n“If you are a customer of the Boyarsky, Osip Ivanovich, then I am\nalready at your service.”']",The context does not mention Charul's role at Société Générale.,"At Société Générale, Charul developed a C++ Value-at-Risk simulation framework, created an asset register microservice with Python and C++, and deployed it to Kubernetes, enabling seamless communication with RabbitMQ.,",1.0,0.0,0.0,0.0,0.0,0.2018471215904571
"What academic projects has Charul worked on?,","['A boy sits on a bench in a park, working on homework.', 'A desk in a classroom with a blackboard behind it', 'a person sitting on  desk operating a laptop next to two other ']","Based on the context provided, we cannot determine what academic projects Charul has worked on as the context only mentions a boy sitting on a bench in a park working on homework, not Charul.","Projects include AI teaches AI (SaaS AI Companion using Next.js, Pinecone, Redis, and Stripe APIs), FitFlair Clothing App (distributed microservices with Kafka), and Event Scheduler (Flask app on AWS with Google Auth).,",0.6666666666666666,0.0,0.0,0.0,0.0,0.18844093789271055
"How has Charul optimized performance in projects?,","['Man using his laptop computer in a dark room ', 'A man typing on a laptop keyboard in a cluttered work environment.', 'a person sitting on  desk operating a laptop next to two other ']",Charul has optimized performance in projects by using his laptop computer in a dark room.,"In the FitFlair Clothing App, Charul optimized API load times by caching in Redis, reducing response time from 500ms to 150ms, a 65% improvement.,",0.0,0.9950171158034729,0.0,0.0,0.0,0.44934291740614185
"What experience does Charul have with OpenCV?,","['A black cat on a wooden table in front of a laptop. ', 'Cat in front of a laptop looking up from the screen', 'A black cat sitting on a table in front of a laptop. ']","Based on the context provided, there is no mention of Charul or OpenCV. Therefore, it is not possible to determine what experience Charul has with OpenCV.","At Valeo, Charul developed a validation microservice for detecting counterfeit 3D patterns using Python, C++, OpenCV, and Fourier Transformations, achieving 98.4% accuracy in production.,",1.0,0.0,0.0,0.0,0.0,0.19980427538355552
"What was the focus of Charul’s role at ThermoFisher?,","['A young man working at a range in a commercial kitchen.', 'Man looking over paperwork while holding pen on desk.', 'A thick man in a white suit and tie wearing a name badge']",The focus of Charul's role at ThermoFisher was looking over paperwork and holding a pen on the desk.,"Charul focused on building a RAG pipeline with LangChain and OpenAI APIs, and developed a distributed encryption service in C++ for the Digital PCR Absolute Q tool, ensuring data integrity and security.,",0.0,0.9649932141934879,0.0,0.0,0.0,0.20332746718991984
"What are Charul’s programming language proficiencies?,","['a person sitting on  desk operating a laptop next to two other ', 'A man typing on a laptop keyboard in a cluttered work environment.', 'a person sitting at a desk with a monitor and keyboard']","Based on the context provided, we cannot determine Charul's programming language proficiencies as there is no mention of Charul or their skills in the text.","Charul is proficient in C++, Python, Java, JavaScript, SQL, and HTML/CSS.,",1.0,0.0,0.0,0.0,0.0,0.2151353560383688
"What is the primary innovation of the Transformer model compared to previous architectures?,","['A electric trolly bus driving in the middle of a street with a building behind it.', 'A metro style train engine passing under a bridge.', 'An old fashioned train engine caked up to a caboose.']",The question is not relevant to the context provided.,"The Transformer eliminates the need for recurrence and convolution in sequence transduction models, relying solely on self-attention mechanisms to model global dependencies.,",0.0,0.0,0.0,0.0,0.0,0.1853533492286795
"How does multi-head attention improve the Transformer model's performance?,","['output values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\nwhere headi = Attention(QWQ\ni , KWK\ni , V WV\ni )\nWhere the projections are parameter matricesWQ\ni ∈ Rdmodel×dk , WK\ni ∈ Rdmodel×dk , WV\ni ∈ Rdmodel×dv\nand WO ∈ Rhdv×dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\n5', 'Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani∗\nGoogle Brain\navaswani@google.com\nNoam Shazeer∗\nGoogle Brain\nnoam@google.com\nNiki Parmar∗\nGoogle Research\nnikip@google.com\nJakob Uszkoreit∗\nGoogle Research\nusz@google.com\nLlion Jones∗\nGoogle Research\nllion@google.com\nAidan N. Gomez∗ †\nUniversity of Toronto\naidan@cs.toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023', 'Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10']","Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions, which improves the Transformer model's performance.","Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions, enabling richer feature extraction.,",0.6666666666666666,0.9576102689624095,0.9999999999,1.0,0.3333333322222222,0.7430878276960795
"What is the function of positional encodings in the Transformer?,","['A locomotive in the rear of a train pushing it. ', 'Yellow train engine hauling freight cars past a level crossing. ', 'Man standing in a black train with red train in front. ']","Positional encodings in the Transformer are used to provide information about the position of words in a sentence or sequence, allowing the model to understand the order of the input data.","Positional encodings inject information about the sequence order into the input embeddings since the Transformer lacks recurrence and convolution.,",0.0,0.9615756419392317,0.0,0.0,0.0,0.984252421441199
"Why is scaled dot-product attention used in the Transformer?,","['a green white engine is pulling some gray train cars', 'grainy image of two trains on tracks moving through a city', 'a digitally manipulated image of cars on a highway at night in the city']",Scaled dot-product attention is used in the Transformer to help the model focus on different parts of the input sequence during the encoding and decoding process.,"Scaled dot-product attention normalizes the dot products to prevent the softmax function from entering regions with extremely small gradients when the dimensionality of keys is large.,",0.0,0.9608845243171529,0.0,0.0,0.0,0.7265537760945489
"What are the two main sub-layers in the encoder of the Transformer?,","['three types of microwaves on top of each other.', 'A subway train that is passing over a train bridge.', 'A metro style train engine passing under a bridge.']","There is no information in the context provided about the encoder of the Transformer, so it is not possible to answer the question based on the context.","The encoder contains a multi-head self-attention mechanism and a position-wise fully connected feed-forward network, both with residual connections and layer normalization.,",1.0,0.0,0.0,0.0,0.0,0.20132414328995946
"How does the Transformer handle long-range dependencies compared to recurrent models?,","['a long disk with several laptops and PC computers ', 'Two long boats are sailing near a large bridge. ', 'A laptop computer sits near a roll of tape and a laptop computer.']",The question is not related to the context provided.,"The Transformer uses self-attention to directly connect all positions with a constant number of operations, significantly shortening the path for learning long-range dependencies.,",0.0,0.0,0.0,0.0,0.0,0.17881572402545293
"What is the role of the encoder-decoder attention layer in the Transformer?,","['A close-up of an electric train going by on the tracks.', 'A train traveling down the track, with power lines in the back.', 'a close up of a train with a view inside']",The question is not related to the context provided.,"Encoder-decoder attention enables the decoder to attend to all positions in the encoder's output sequence, facilitating input-output alignment.,",0.0,0.0,0.0,0.0,0.0,0.17555587742178275
"How does the Transformer achieve faster training compared to recurrent models?,","['A close-up of an electric train going by on the tracks.', 'Yellow train engine hauling freight cars past a level crossing. ', 'A high-speed passenger train is being loaded with luggage. ']","The context does not provide any information related to Transformers or recurrent models, so it is not possible to answer the question based on the context given.","By parallelizing computations across all sequence positions, the Transformer avoids the sequential nature of recurrent models, significantly reducing training time.,",0.6666666666666666,0.0,0.0,0.0,0.0,0.19935671093398916
"What dataset was used to evaluate the Transformer on English-to-German translation tasks?,","['A white and black train on rusty train tracks. ', 'A white train hauling cargo along a track.', 'Yellow train engine hauling freight cars past a level crossing. ']",The question is not related to the context provided.,"The WMT 2014 English-to-German dataset with about 4.5 million sentence pairs was used for evaluation.,",0.0,0.0,0.0,0.0,0.0,0.18381429785732178
"What BLEU score did the Transformer achieve on the English-to-German translation task?,","['Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\nTransformer (base model) 27.3 38.1 3.3 · 1018\nTransformer (big) 28.4 41.8 2.3 · 1019\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8', 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dv Pdrop ϵls\ntrain PPL BLEU params\nsteps (dev) (dev) ×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9', 'Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10']",The Transformer achieved a BLEU score of 28.4 on the English-to-German translation task.,"The Transformer (big) model achieved a BLEU score of 28.4, surpassing previous state-of-the-art results.,",1.0,0.9878118550298058,0.9999999999,1.0,0.0,0.539043224037959
"What regularization techniques were used in training the Transformer?,","['Yellow train engine hauling freight cars past a level crossing. ', 'A white and black train on rusty train tracks. ', 'A train traveling down the track, with power lines in the back.']",There is no mention of any regularization techniques used in training the Transformer in the given context.,"The Transformer used residual dropout, attention dropout, and label smoothing to prevent overfitting during training.,",0.5,0.0,0.0,0.0,0.0,0.20721324307116054
"What is the structure of the feed-forward network in the Transformer?,","['A train traveling down the track, with power lines in the back.', 'Yellow train engine hauling freight cars past a level crossing. ', 'a close up of a train with a view inside']",The question is not related to the context provided.,"Each feed-forward network consists of two linear transformations with a ReLU activation in between, applied independently to each position.,",0.0,0.0,0.0,0.0,0.0,0.1741097310116434
"What are the advantages of the sinusoidal positional encodings over learned positional embeddings?,","['a big teddy bear sitting next to some boxes', 'A teddy bear is sitting near to a toy with pegs.', 'A teddy bear sitting in front of toy wooden shapes.']",There is no information provided in the context about sinusoidal positional encodings or learned positional embeddings.,"Sinusoidal encodings allow the model to extrapolate to longer sequence lengths unseen during training, offering similar performance to learned embeddings.,",1.0,0.0,0.0,0.0,0.0,0.2116693178108759
"How does the Transformer handle auto-regression in the decoder?,","['A train traveling down the track, with power lines in the back.', 'Train cars sit on a track next to a roadway. ', 'Old rusted transit train cars sit on the tracks. ']",The question is not related to the context provided about a train traveling down the track.,"The Transformer masks future positions in the decoder’s self-attention mechanism to ensure predictions depend only on known outputs.,",0.5,0.0,0.0,0.0,0.0,0.17867436382121202
"What optimizer and learning rate schedule were used in the Transformer?,","['Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\nTransformer (base model) 27.3 38.1 3.3 · 1018\nTransformer (big) 28.4 41.8 2.3 · 1019\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8', 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dv Pdrop ϵls\ntrain PPL BLEU params\nsteps (dev) (dev) ×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9', 'Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over\nlanguage. IJCAI.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. 2021. Training veriﬁers to solve math word problems. arXiv preprint\narXiv:2110.14168.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. NAACL.\nHonghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. 2019. Neural\nlogic machines. ICLR.\nDheeru Dua, Sameer Singh, and Matt Gardner. 2020. Beneﬁts of intermediate annotations in reading\ncomprehension. ACL.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did\naristotle use a laptop? A question answering benchmark with implicit reasoning strategies. TACL.\nYuling Gu, Bhavana Dalvi Mishra, and Peter Clark. 2022. DREAM: Uncovering mental models\nbehind language models. NAACL.\nBraden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and Christopher\nRé. 2018. Training classiﬁers with natural language explanations. ACL.\nPeter Hase and Mohit Bansal. 2022. When can models learn from explanations? a formal framework\nfor understanding the roles of explanation data. ACL.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874.\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning\nto solve arithmetic word problems with verb categorization. EMNLP.\nZhanming Jie, Jierui Li, and Wei Lu. 2022. Learning to reason deductively: Math word problem\nsolving as complex relation extraction. arXiv preprint arXiv:2203.10316.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016.\nMAWPS: A math word problem repository. NAACL.\nAndrew K. Lampinen, Ishita Dasgupta, Stephanie C.Y . Chan, Kory Matthewson, Michael Henry\nTessler, Antonia Creswell, James L. McClelland, Jane X. Wang, and Felix Hill. 2022. Can language\nmodels learn from explanations in context? arXiv preprint arXiv:2204.02329.\nYihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang,\nand Ee-Peng Lim. 2021. MWPToolkit: An open-source framework for deep learning-based math\nword problem solvers. arXiv preprint arXiv:2109.00799.\nTeven Le Scao and Alexander Rush. 2021. How many data points is a prompt worth? NAACL.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efﬁcient\nprompt tuning. EMNLP.\nIddo Lev, Bill MacCartney, Christopher Manning, and Roger Levy. 2004. Solving logic puzzles:\nFrom robust processing to precise semantics. Proceedings of the 2nd Workshop on Text Meaning\nand Interpretation.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning: Optimizing continuous prompts for generation.\nACL.\n11']",The context does not provide information about the optimizer and learning rate schedule used in the Transformer.,"The Adam optimizer was used with a learning rate schedule that increases linearly for the first 4000 steps and decreases proportionally to the inverse square root of the step number.,",1.0,0.0,0.0,0.0,0.0,0.19806849468774665
"What is the main advantage of chain-of-thought prompting in large language models?,","['Chain-of-Thought Prompting Elicits Reasoning\nin Large Language Models\nJason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma\nBrian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou\nGoogle Research, Brain Team\n{jasonwei,dennyzhou}@google.com\nAbstract\nWe explore how generating a chain of thought—a series of intermediate reasoning\nsteps—signiﬁcantly improves the ability of large language models to perform\ncomplex reasoning. In particular, we show how such reasoning abilities emerge\nnaturally in sufﬁciently large language models via a simple method called chain-of-\nthought prompting, where a few chain of thought demonstrations are provided as\nexemplars in prompting.\nExperiments on three large language models show that chain-of-thought prompting\nimproves performance on a range of arithmetic, commonsense, and symbolic\nreasoning tasks. The empirical gains can be striking. For instance, prompting a\nPaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art\naccuracy on the GSM8K benchmark of math word problems, surpassing even\nﬁnetuned GPT-3 with a veriﬁer.\nA: The cafeteria had 23 apples originally. They used \n20 to make lunch. So they had 23 - 20 = 3. They \nbought 6 more apples, so they have 3 + 6 = 9. The \nanswer is 9.\nChain-of-Thought Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of \ntennis balls. Each can has 3 tennis balls. How many \ntennis balls does he have now? \nA: The answer is 11. \nQ: The cafeteria had 23 apples. If they used 20 to \nmake lunch and bought 6 more, how many apples \ndo they have?\nA: The answer is 27.\nStandard Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of \ntennis balls. Each can has 3 tennis balls. How many \ntennis balls does he have now? \nA: Roger started with 5 balls. 2 cans of 3 tennis balls \neach is 6 tennis balls. 5 + 6 = 11. The answer is 11. \nQ: The cafeteria had 23 apples. If they used 20 to \nmake lunch and bought 6 more, how many apples \ndo they have?\nModel Input\nModel Output Model Output\nModel Input\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic,\ncommonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\narXiv:2201.11903v6  [cs.CL]  10 Jan 2023', 'experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\nreasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\nchain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\nall experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\nmodel. No language models were ﬁnetuned in the process of writing this paper.\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\n(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ﬂat scaling curve, chain-\nof-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\nappears to expand the set of tasks that large language models can perform successfully—in other\nwords, our work underscores that standard prompting only provides a lower bound on the capabilities\nof large language models. This observation likely raises more questions than it answers—for instance,\nhow much more can we expect reasoning ability to improve with a further increase in model scale?\nWhat other prompting methods might expand the range of tasks that language models can solve?\nAs for limitations, we ﬁrst qualify that although chain of thought emulates the thought processes of\nhuman reasoners, this does not answer whether the neural network is actually “reasoning,” which\nwe leave as an open question. Second, although the cost of manually augmenting exemplars with\nchains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\nﬁnetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\ngeneralization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\nand incorrect answers; improving factual generations of language models is an open direction for\nfuture work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022,inter alia). Finally,\nthe emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\nreal-world applications; further research could explore how to induce reasoning in smaller models.\n7 Related Work\nThis work is inspired by many research areas, which we detail in an extended related work section\n(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\nThe ﬁrst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\npioneer the idea of using natural language rationales to solve math word problems through a series\nof intermediate steps. Their work is a remarkable contrast to the literature using formal languages\nto reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\net al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ﬁnetune a pretrained\nlanguage model rather than training a model from scratch. In the domain of program synthesis,\nNye et al. (2021) leverage language models to predict the ﬁnal outputs of Python programs via\nﬁrst line-to-line predicting the intermediate computational results, and show that their step-by-step\nprediction method performs better than directly predicting the ﬁnal outputs.\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the\npopularization of few-shot prompting as given by Brown et al. (2020), several general approaches\nhave improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\n2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\net al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\noutputs of language models with a chain of thought.\n8 Conclusions\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\ning reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\nreasoning, we ﬁnd that chain-of-thought reasoning is an emergent property of model scale that allows\nsufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves.\nBroadening the range of reasoning tasks that language models can perform will hopefully inspire\nfurther work on language-based approaches to reasoning.\n9', 'C Extended Related Work\nChain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-\ning, natural language explanations, program synthesis/execution, numeric and logical reasoning, and\nintermediate language steps.\nC.1 Prompting\nThe recent success of large-scale language models has led to growing interest in improving their\ncapability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a\nsurvey). This paper falls in the category of general prompting approaches, whereby input prompts are\noptimized to allow a single large language model to better perform a variety of tasks (Li and Liang,\n2021; Lester et al., 2021; Reif et al., 2022, inter alia).\nOne recent line of work aims to improve the ability of language models to perform a task by providing\ninstructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh\net al., 2022; Wang et al., 2022b). This line of work is related because it also augments input–output\npairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically\nprepended to the inputs), chain-of-thought prompting augments the outputs of language models.\nAnother related direction is sequentially combining the outputs of language models; human–computer\ninteraction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of\nlanguage models improves task outcomes in a 20-person user study.\nC.2 Natural language explanations\nAnother closely related direction uses natural language explanations (NLEs), often with the goal of\nimproving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi´c, 2021, inter alia). That\nline of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,\n2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ﬁnal\nprediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,\nthe chain of thought processing considered in this paper occurs before the ﬁnal answer. And while\nNLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of\nchain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple\nsteps—interpretability is just a side effect. Marasovi ´c et al. (2022) show that prompt-based ﬁnetuning\nwith NLE improves NLI and classiﬁcation performance, though they largely focus on evaluating\nexplanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,\nand symbolic tasks that require multi-hop reasoning.\nC.3 Program synthesis and execution\nUsing intermediate reasoning steps has a long history in program synthesis and execution (Zaremba\nand Sutskever, 2014, inter alia). Recent work along in this direction has included a number of\narchitectural innovations (Cai et al., 2017; Dong et al., 2019; Yan et al., 2020), as well as the use of\nlarge language models (Chen et al., 2021; Austin et al., 2021). The program execution work closest to\nours is perhaps Nye et al. (2021), which show that large language models can perform up to 10-digit\naddition, evaluate polynomials, and execute python programs. Whereas generating a program and\nthen executing it can be viewed as a type of reasoning, our work generalizes such domain-speciﬁc\nprimitives to natural language, which is open-domain and relevant to any text-to-text NLP task in\nprinciple.\nC.4 Numeric and logical reasoning\nNumeric and logical reasoning has been a long-studied task in machine learning and natural language\nprocessing (Lev et al., 2004, inter alia). Recent work has also aimed to inject numeric reasoning\nabilities in language models in various ways, such as augmenting BERT with a predeﬁned set of\nexecutable operations (Andor et al., 2019), including a graph neural network (Ran et al., 2019), and\nusing specialized training procedures (Pi˛ ekos et al., 2021). Another line of work aims to enable\nlanguage models to perform logical or formal reasoning, often by verablizing the rules in natural\nlanguage formal rules using language (Clark et al., 2020; Saeed et al., 2021; Liang et al., 2021).\n24']","The main advantage of chain-of-thought prompting in large language models is that it significantly improves the ability of the models to perform complex reasoning tasks, such as arithmetic, commonsense, and symbolic reasoning.","Chain-of-thought prompting enables language models to decompose complex reasoning tasks into intermediate steps, improving their performance on arithmetic, commonsense, and symbolic reasoning tasks.,",,0.9910111332355016,0.9999999999666667,1.0,0.0,0.839855672818453
"How does chain-of-thought prompting compare to standard prompting on the GSM8K benchmark?,","['0\n20\n40\n60\nGSM8K\nsolve rate (%)\nLaMDA GPT PaLM\nStandard prompting\nChain-of-thought prompting\nPrior supervised best\n0\n20\n40\n60\n80\nSV AMP\nsolve rate (%)\n0.4 8 137\n0\n25\n50\n75\n100\nMAWPS\nsolve rate (%)\n0.4 7 175 8 62 540\nModel scale (# parameters in billions)\nFigure 4: Chain-of-thought prompting enables\nlarge language models to solve challenging math\nproblems. Notably, chain-of-thought reasoning\nis an emergent ability of increasing model scale.\nPrior best numbers are from Cobbe et al. (2021)\nfor GSM8K, Jie et al. (2022) for SV AMP, and Lan\net al. (2021) for MAWPS.\nSecond, chain-of-thought prompting has larger\nperformance gains for more-complicated prob-\nlems. For instance, for GSM8K (the dataset\nwith the lowest baseline performance), perfor-\nmance more than doubled for the largest GPT\nand PaLM models. On the other hand, for Sin-\ngleOp, the easiest subset of MAWPS which only\nrequires a single step to solve, performance im-\nprovements were either negative or very small\n(see Appendix Table 3).\nThird, chain-of-thought prompting via GPT-3\n175B and PaLM 540B compares favorably to\nprior state of the art, which typically ﬁnetunes a\ntask-speciﬁc model on a labeled training dataset.\nFigure 4 shows how PaLM 540B uses chain-of-\nthought prompting to achieve new state of the art\non GSM8K, SV AMP, and MAWPS (though note\nthat standard prompting already passed the prior\nbest for SV AMP). On the other two datasets,\nAQuA and ASDiv, PaLM with chain-of-thought\nprompting reaches within 2% of the state of the\nart (Appendix Table 2).\nTo better understand why chain-of-thought\nprompting works, we manually examined model-\ngenerated chains of thought by LaMDA 137B\nfor GSM8K. Of 50 random examples where the\nmodel returned the correct ﬁnal answer, all of\nthe generated chains of thought were also log-\nically and mathematically correct except two\nthat coincidentally arrived at the correct answer\n(see Appendix D.1, and Table 8 for examples\nof correct model-generated chains of thought).\nWe also randomly examined 50 random sam-\nples for which the model gave the wrong answer.\nThe summary of this analysis is that 46% of the\nchains of thought were almost correct, barring\nminor mistakes (calculator error, symbol map-\nping error, or one reasoning step missing), and that the other 54% of the chains of thought had major\nerrors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into\nwhy scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors\nmade by PaLM 62B and whether those errors were ﬁxed by scaling to PaLM 540B. The summary\nis that scaling PaLM to 540B ﬁxes a large portion of one-step missing and semantic understanding\nerrors in the 62B model (see Appendix A.1).\n3.3 Ablation Study\nThe observed beneﬁts of using chain-of-thought prompting raises the natural question of whether the\nsame performance improvements can be conferred via other types of prompting. Figure 5 shows an\nablation study with three variations of chain of thought described below.\nEquation only. One reason for why chain-of-thought prompting might help is that it produces the\nmathematical equation to be evaluated, and so we test a variation where the model is prompted\nto output only a mathematical equation before giving the answer. Figure 5 shows that equation\nonly prompting does not help much for GSM8K, which implies that the semantics of the questions\nin GSM8K are too challenging to directly translate into an equation without the natural language\nreasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we ﬁnd\nthat equation only prompting does improve performance, since the equation can be easily derived\nfrom the question (see Appendix Table 6).\n5', 'Table 2: Standard prompting versus chain of thought prompting on ﬁve arithmetic reasoning bench-\nmarks. Note that chain of thought prompting is an emergent ability of model scale—it does not\npositively impact performance until used with a model of sufﬁcient scale.\nGSM8K SV AMP ASDiv AQuA MAWPS\nModel standard CoT standard CoT standard CoT standard CoT standard CoT\nUL2 20B 4.1 4.4 10.1 12.5 16.0 16.9 20.5 23.6 16.6 19.1\nLaMDA 420M 2.6 0.4 2.5 1.6 3.2 0.8 23.5 8.3 3.2 0.9\n2B 3.6 1.9 3.3 2.4 4.1 3.8 22.9 17.7 3.9 3.1\n8B 3.2 1.6 4.3 3.4 5.9 5.0 22.8 18.6 5.3 4.8\n68B 5.7 8.2 13.6 18.8 21.8 23.1 22.3 20.2 21.6 30.6\n137B 6.5 14.3 29.5 37.5 40.1 46.6 25.5 20.6 43.2 57.9\nGPT 350M 2.2 0.5 1.4 0.8 2.1 0.8 18.1 8.7 2.4 1.1\n1.3B 2.4 0.5 1.5 1.7 2.6 1.4 12.6 4.3 3.1 1.7\n6.7B 4.0 2.4 6.1 3.1 8.6 3.6 15.4 13.4 8.8 3.5\n175B 15.6 46.9 65.7 68.9 70.3 71.3 24.8 35.8 72.7 87.1\nCodex - 19.7 63.1 69.9 76.4 74.0 80.4 29.5 45.3 78.7 92.6\nPaLM 8B 4.9 4.1 15.1 16.8 23.7 25.2 19.3 21.7 26.2 30.5\n62B 9.6 29.9 48.2 46.7 58.7 61.9 25.6 22.4 61.8 80.3\n540B 17.9 56.9 69.4 79.0 72.1 73.9 25.2 35.8 79.2 93.3\nTable 3: Standard prompting versus chain of thought prompting on the four subsets of the MAWPS\nbenchmark. The point of stratifying the MAWPS benchmark is to show that performance gains are\nminimal on easy one-step or two-step problems where large language models already achieve high\nperformance (e.g., SingleOp, SingleEq, and AddSub).\nSingleOp SingleEq AddSub MultiArith\nModel standard CoT standard CoT standard CoT standard CoT\nUL2 20B 24.9 27.2 18.0 20.2 18.5 18.2 5.0 10.7\nLaMDA 420M 2.8 1.0 2.4 0.4 1.9 0.7 5.8 1.5\n2B 4.6 4.1 2.4 3.3 2.7 3.2 5.8 1.8\n8B 8.0 7.0 4.5 4.4 3.4 5.2 5.2 2.4\n68B 36.5 40.8 23.9 26.0 17.3 23.2 8.7 32.4\n137B 73.2 76.2 48.8 58.7 43.0 51.9 7.6 44.9\nGPT 350M 3.2 1.8 2.0 0.2 2.0 1.5 2.3 0.8\n1.3B 5.3 3.0 2.4 1.6 2.3 1.5 2.2 0.5\n6.7B 13.5 3.9 8.7 4.9 8.6 2.5 4.5 2.8\n175B 90.9 88.8 82.7 86.6 83.3 81.3 33.8 91.7\nCodex - 93.1 91.8 86.8 93.1 90.9 89.1 44.0 96.2\nPaLM 8B 41.8 46.6 29.5 28.2 29.4 31.4 4.2 15.8\n62B 87.9 85.6 77.2 83.5 74.7 78.2 7.3 73.7\n540B 94.1 94.1 86.5 92.3 93.9 91.9 42.2 94.7\n21', 'Table 4: Standard prompting versus chain of thought prompting on ﬁve commonsense reasoning\nbenchmarks. Chain of thought prompting is an emergent ability of model scale—it does not positively\nimpact performance until used with a model of sufﬁcient scale.\nCSQA StrategyQA Date Sports SayCan\nModel standard CoT standard CoT standard CoT standard CoT standard CoT\nUL2 20B 34.2 51.4 59.0 53.3 13.5 14.0 57.9 65.3 20.0 41.7\nLaMDA 420M 20.1 19.2 46.4 24.9 1.9 1.6 50.0 49.7 7.5 7.5\n2B 20.2 19.6 52.6 45.2 8.0 6.8 49.3 57.5 8.3 8.3\n8B 19.0 20.3 54.1 46.8 9.5 5.4 50.0 52.1 28.3 33.3\n68B 37.0 44.1 59.6 62.2 15.5 18.6 55.2 77.5 35.0 42.5\n137B 53.6 57.9 62.4 65.4 21.5 26.8 59.5 85.8 43.3 46.6\nGPT 350M 14.7 15.2 20.6 0.9 4.3 0.9 33.8 41.6 12.5 0.8\n1.3B 12.0 19.2 45.8 35.7 4.0 1.4 0.0 26.9 20.8 9.2\n6.7B 19.0 24.0 53.6 50.0 8.9 4.9 0.0 4.4 17.5 35.0\n175B 79.5 73.5 65.9 65.4 43.8 52.1 69.6 82.4 81.7 87.5\nCodex - 82.3 77.9 67.1 73.2 49.0 64.8 71.7 98.5 85.8 88.3\nPaLM 8B 19.8 24.9 55.6 53.5 12.9 13.1 55.1 75.2 34.2 40.0\n62B 65.4 68.1 58.4 63.4 29.8 44.7 72.1 93.6 65.8 70.0\n540B 78.1 79.9 68.6 77.8 49.0 65.3 80.5 95.4 80.8 91.7\nTable 5: Standard prompting versus chain of thought prompting enables length generalization to\nlonger inference examples on two symbolic manipulation tasks.\nLast Letter Concatenation Coin Flip (state tracking)\n2 OOD: 3 OOD: 4 2 OOD: 3 OOD: 4\nModel standard CoT standard CoT standard CoT standard CoT standard CoT standard CoT\nUL2 20B 0.6 18.8 0.0 0.2 0.0 0.0 70.4 67.1 51.6 52.2 48.7 50.4\nLaMDA 420M 0.3 1.6 0.0 0.0 0.0 0.0 52.9 49.6 50.0 50.5 49.5 49.1\n2B 2.3 6.0 0.0 0.0 0.0 0.0 54.9 55.3 47.4 48.7 49.8 50.2\n8B 1.5 11.5 0.0 0.0 0.0 0.0 52.9 55.5 48.2 49.6 51.2 50.6\n68B 4.4 52.0 0.0 0.8 0.0 2.5 56.2 83.2 50.4 69.1 50.9 59.6\n137B 5.8 77.5 0.0 34.4 0.0 13.5 49.0 99.6 50.7 91.0 49.1 74.5\nPaLM 8B 2.6 18.8 0.0 0.0 0.0 0.2 60.0 74.4 47.3 57.1 50.9 51.8\n62B 6.8 85.0 0.0 59.6 0.0 13.4 91.4 96.8 43.9 91.0 38.3 72.4\n540B 7.6 99.4 0.2 94.8 0.0 63.0 98.1 100.0 49.3 98.6 54.8 90.2\n22']",Chain-of-thought prompting outperforms standard prompting on the GSM8K benchmark.,"Chain-of-thought prompting significantly outperforms standard prompting, achieving state-of-the-art results with PaLM 540B on the GSM8K benchmark for math word problems.,",1.0,0.9933203867701897,0.9999999999666667,1.0,0.0,0.7370949740899185
"What are the three primary reasoning tasks improved by chain-of-thought prompting?,","['experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\nreasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\nchain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\nall experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\nmodel. No language models were ﬁnetuned in the process of writing this paper.\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\n(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ﬂat scaling curve, chain-\nof-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\nappears to expand the set of tasks that large language models can perform successfully—in other\nwords, our work underscores that standard prompting only provides a lower bound on the capabilities\nof large language models. This observation likely raises more questions than it answers—for instance,\nhow much more can we expect reasoning ability to improve with a further increase in model scale?\nWhat other prompting methods might expand the range of tasks that language models can solve?\nAs for limitations, we ﬁrst qualify that although chain of thought emulates the thought processes of\nhuman reasoners, this does not answer whether the neural network is actually “reasoning,” which\nwe leave as an open question. Second, although the cost of manually augmenting exemplars with\nchains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\nﬁnetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\ngeneralization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\nand incorrect answers; improving factual generations of language models is an open direction for\nfuture work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022,inter alia). Finally,\nthe emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\nreal-world applications; further research could explore how to induce reasoning in smaller models.\n7 Related Work\nThis work is inspired by many research areas, which we detail in an extended related work section\n(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\nThe ﬁrst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\npioneer the idea of using natural language rationales to solve math word problems through a series\nof intermediate steps. Their work is a remarkable contrast to the literature using formal languages\nto reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\net al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ﬁnetune a pretrained\nlanguage model rather than training a model from scratch. In the domain of program synthesis,\nNye et al. (2021) leverage language models to predict the ﬁnal outputs of Python programs via\nﬁrst line-to-line predicting the intermediate computational results, and show that their step-by-step\nprediction method performs better than directly predicting the ﬁnal outputs.\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the\npopularization of few-shot prompting as given by Brown et al. (2020), several general approaches\nhave improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\n2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\net al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\noutputs of language models with a chain of thought.\n8 Conclusions\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\ning reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\nreasoning, we ﬁnd that chain-of-thought reasoning is an emergent property of model scale that allows\nsufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves.\nBroadening the range of reasoning tasks that language models can perform will hopefully inspire\nfurther work on language-based approaches to reasoning.\n9', 'Chain-of-Thought Prompting Elicits Reasoning\nin Large Language Models\nJason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma\nBrian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou\nGoogle Research, Brain Team\n{jasonwei,dennyzhou}@google.com\nAbstract\nWe explore how generating a chain of thought—a series of intermediate reasoning\nsteps—signiﬁcantly improves the ability of large language models to perform\ncomplex reasoning. In particular, we show how such reasoning abilities emerge\nnaturally in sufﬁciently large language models via a simple method called chain-of-\nthought prompting, where a few chain of thought demonstrations are provided as\nexemplars in prompting.\nExperiments on three large language models show that chain-of-thought prompting\nimproves performance on a range of arithmetic, commonsense, and symbolic\nreasoning tasks. The empirical gains can be striking. For instance, prompting a\nPaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art\naccuracy on the GSM8K benchmark of math word problems, surpassing even\nﬁnetuned GPT-3 with a veriﬁer.\nA: The cafeteria had 23 apples originally. They used \n20 to make lunch. So they had 23 - 20 = 3. They \nbought 6 more apples, so they have 3 + 6 = 9. The \nanswer is 9.\nChain-of-Thought Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of \ntennis balls. Each can has 3 tennis balls. How many \ntennis balls does he have now? \nA: The answer is 11. \nQ: The cafeteria had 23 apples. If they used 20 to \nmake lunch and bought 6 more, how many apples \ndo they have?\nA: The answer is 27.\nStandard Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of \ntennis balls. Each can has 3 tennis balls. How many \ntennis balls does he have now? \nA: Roger started with 5 balls. 2 cans of 3 tennis balls \neach is 6 tennis balls. 5 + 6 = 11. The answer is 11. \nQ: The cafeteria had 23 apples. If they used 20 to \nmake lunch and bought 6 more, how many apples \ndo they have?\nModel Input\nModel Output Model Output\nModel Input\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic,\ncommonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\narXiv:2201.11903v6  [cs.CL]  10 Jan 2023', 'C Extended Related Work\nChain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-\ning, natural language explanations, program synthesis/execution, numeric and logical reasoning, and\nintermediate language steps.\nC.1 Prompting\nThe recent success of large-scale language models has led to growing interest in improving their\ncapability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a\nsurvey). This paper falls in the category of general prompting approaches, whereby input prompts are\noptimized to allow a single large language model to better perform a variety of tasks (Li and Liang,\n2021; Lester et al., 2021; Reif et al., 2022, inter alia).\nOne recent line of work aims to improve the ability of language models to perform a task by providing\ninstructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh\net al., 2022; Wang et al., 2022b). This line of work is related because it also augments input–output\npairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically\nprepended to the inputs), chain-of-thought prompting augments the outputs of language models.\nAnother related direction is sequentially combining the outputs of language models; human–computer\ninteraction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of\nlanguage models improves task outcomes in a 20-person user study.\nC.2 Natural language explanations\nAnother closely related direction uses natural language explanations (NLEs), often with the goal of\nimproving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi´c, 2021, inter alia). That\nline of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,\n2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ﬁnal\nprediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,\nthe chain of thought processing considered in this paper occurs before the ﬁnal answer. And while\nNLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of\nchain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple\nsteps—interpretability is just a side effect. Marasovi ´c et al. (2022) show that prompt-based ﬁnetuning\nwith NLE improves NLI and classiﬁcation performance, though they largely focus on evaluating\nexplanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,\nand symbolic tasks that require multi-hop reasoning.\nC.3 Program synthesis and execution\nUsing intermediate reasoning steps has a long history in program synthesis and execution (Zaremba\nand Sutskever, 2014, inter alia). Recent work along in this direction has included a number of\narchitectural innovations (Cai et al., 2017; Dong et al., 2019; Yan et al., 2020), as well as the use of\nlarge language models (Chen et al., 2021; Austin et al., 2021). The program execution work closest to\nours is perhaps Nye et al. (2021), which show that large language models can perform up to 10-digit\naddition, evaluate polynomials, and execute python programs. Whereas generating a program and\nthen executing it can be viewed as a type of reasoning, our work generalizes such domain-speciﬁc\nprimitives to natural language, which is open-domain and relevant to any text-to-text NLP task in\nprinciple.\nC.4 Numeric and logical reasoning\nNumeric and logical reasoning has been a long-studied task in machine learning and natural language\nprocessing (Lev et al., 2004, inter alia). Recent work has also aimed to inject numeric reasoning\nabilities in language models in various ways, such as augmenting BERT with a predeﬁned set of\nexecutable operations (Andor et al., 2019), including a graph neural network (Ran et al., 2019), and\nusing specialized training procedures (Pi˛ ekos et al., 2021). Another line of work aims to enable\nlanguage models to perform logical or formal reasoning, often by verablizing the rules in natural\nlanguage formal rules using language (Clark et al., 2020; Saeed et al., 2021; Liang et al., 2021).\n24']","The three primary reasoning tasks improved by chain-of-thought prompting are arithmetic, commonsense, and symbolic reasoning.","Chain-of-thought prompting improves performance in arithmetic reasoning, commonsense reasoning, and symbolic reasoning.,",0.0,0.9852203873003443,0.9999999999666667,1.0,0.7499999981250001,0.9890958121313858
"How does chain-of-thought prompting facilitate interpretability in language models?,","[""A black and brown dog wearing a chain around it's neck."", 'Two small dogs on leashes walking on a brick pathway.', 'A dog is sitting in a chair at a table.']",The question is not related to the context provided.,"It provides a step-by-step reasoning process that allows users to debug the model’s reasoning path and understand how an answer was derived.,",0.0,0.0,0.0,0.0,0.0,0.18393132025493364
"What scaling behavior is associated with chain-of-thought prompting?,","[""A black and brown dog wearing a chain around it's neck."", 'a miniature horse walking near a restaurants outside deck ', 'Two small dogs on leashes walking on a brick pathway.']",The question is not relevant to the context provided.,"The benefits of chain-of-thought prompting emerge only in sufficiently large models (approximately 100B parameters or more), with smaller models showing little to no performance improvement.,",0.0,0.0,0.0,0.0,0.0,0.18371016590635647
"What datasets were used to evaluate arithmetic reasoning performance?,","['A boy sits on a bench in a park, working on homework.', 'a number of people sitting on benches ', 'some kids playing with some large checkerboards painted on the ground ']",There is no mention of datasets being used to evaluate arithmetic reasoning performance in the context provided.,"The datasets include GSM8K, SVAMP, ASDiv, AQuA, and MAWPS.,",0.5,0.0,0.0,0.0,0.0,0.19630758812673393
"Why is natural language reasoning beneficial in chain-of-thought prompting?,","['experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\nreasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\nchain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\nall experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\nmodel. No language models were ﬁnetuned in the process of writing this paper.\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\n(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ﬂat scaling curve, chain-\nof-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\nappears to expand the set of tasks that large language models can perform successfully—in other\nwords, our work underscores that standard prompting only provides a lower bound on the capabilities\nof large language models. This observation likely raises more questions than it answers—for instance,\nhow much more can we expect reasoning ability to improve with a further increase in model scale?\nWhat other prompting methods might expand the range of tasks that language models can solve?\nAs for limitations, we ﬁrst qualify that although chain of thought emulates the thought processes of\nhuman reasoners, this does not answer whether the neural network is actually “reasoning,” which\nwe leave as an open question. Second, although the cost of manually augmenting exemplars with\nchains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\nﬁnetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\ngeneralization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\nand incorrect answers; improving factual generations of language models is an open direction for\nfuture work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022,inter alia). Finally,\nthe emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\nreal-world applications; further research could explore how to induce reasoning in smaller models.\n7 Related Work\nThis work is inspired by many research areas, which we detail in an extended related work section\n(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\nThe ﬁrst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\npioneer the idea of using natural language rationales to solve math word problems through a series\nof intermediate steps. Their work is a remarkable contrast to the literature using formal languages\nto reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\net al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ﬁnetune a pretrained\nlanguage model rather than training a model from scratch. In the domain of program synthesis,\nNye et al. (2021) leverage language models to predict the ﬁnal outputs of Python programs via\nﬁrst line-to-line predicting the intermediate computational results, and show that their step-by-step\nprediction method performs better than directly predicting the ﬁnal outputs.\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the\npopularization of few-shot prompting as given by Brown et al. (2020), several general approaches\nhave improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\n2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\net al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\noutputs of language models with a chain of thought.\n8 Conclusions\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\ning reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\nreasoning, we ﬁnd that chain-of-thought reasoning is an emergent property of model scale that allows\nsufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves.\nBroadening the range of reasoning tasks that language models can perform will hopefully inspire\nfurther work on language-based approaches to reasoning.\n9', 'C Extended Related Work\nChain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-\ning, natural language explanations, program synthesis/execution, numeric and logical reasoning, and\nintermediate language steps.\nC.1 Prompting\nThe recent success of large-scale language models has led to growing interest in improving their\ncapability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a\nsurvey). This paper falls in the category of general prompting approaches, whereby input prompts are\noptimized to allow a single large language model to better perform a variety of tasks (Li and Liang,\n2021; Lester et al., 2021; Reif et al., 2022, inter alia).\nOne recent line of work aims to improve the ability of language models to perform a task by providing\ninstructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh\net al., 2022; Wang et al., 2022b). This line of work is related because it also augments input–output\npairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically\nprepended to the inputs), chain-of-thought prompting augments the outputs of language models.\nAnother related direction is sequentially combining the outputs of language models; human–computer\ninteraction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of\nlanguage models improves task outcomes in a 20-person user study.\nC.2 Natural language explanations\nAnother closely related direction uses natural language explanations (NLEs), often with the goal of\nimproving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi´c, 2021, inter alia). That\nline of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,\n2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ﬁnal\nprediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,\nthe chain of thought processing considered in this paper occurs before the ﬁnal answer. And while\nNLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of\nchain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple\nsteps—interpretability is just a side effect. Marasovi ´c et al. (2022) show that prompt-based ﬁnetuning\nwith NLE improves NLI and classiﬁcation performance, though they largely focus on evaluating\nexplanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,\nand symbolic tasks that require multi-hop reasoning.\nC.3 Program synthesis and execution\nUsing intermediate reasoning steps has a long history in program synthesis and execution (Zaremba\nand Sutskever, 2014, inter alia). Recent work along in this direction has included a number of\narchitectural innovations (Cai et al., 2017; Dong et al., 2019; Yan et al., 2020), as well as the use of\nlarge language models (Chen et al., 2021; Austin et al., 2021). The program execution work closest to\nours is perhaps Nye et al. (2021), which show that large language models can perform up to 10-digit\naddition, evaluate polynomials, and execute python programs. Whereas generating a program and\nthen executing it can be viewed as a type of reasoning, our work generalizes such domain-speciﬁc\nprimitives to natural language, which is open-domain and relevant to any text-to-text NLP task in\nprinciple.\nC.4 Numeric and logical reasoning\nNumeric and logical reasoning has been a long-studied task in machine learning and natural language\nprocessing (Lev et al., 2004, inter alia). Recent work has also aimed to inject numeric reasoning\nabilities in language models in various ways, such as augmenting BERT with a predeﬁned set of\nexecutable operations (Andor et al., 2019), including a graph neural network (Ran et al., 2019), and\nusing specialized training procedures (Pi˛ ekos et al., 2021). Another line of work aims to enable\nlanguage models to perform logical or formal reasoning, often by verablizing the rules in natural\nlanguage formal rules using language (Clark et al., 2020; Saeed et al., 2021; Liang et al., 2021).\n24', 'Chain-of-Thought Prompting Elicits Reasoning\nin Large Language Models\nJason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma\nBrian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou\nGoogle Research, Brain Team\n{jasonwei,dennyzhou}@google.com\nAbstract\nWe explore how generating a chain of thought—a series of intermediate reasoning\nsteps—signiﬁcantly improves the ability of large language models to perform\ncomplex reasoning. In particular, we show how such reasoning abilities emerge\nnaturally in sufﬁciently large language models via a simple method called chain-of-\nthought prompting, where a few chain of thought demonstrations are provided as\nexemplars in prompting.\nExperiments on three large language models show that chain-of-thought prompting\nimproves performance on a range of arithmetic, commonsense, and symbolic\nreasoning tasks. The empirical gains can be striking. For instance, prompting a\nPaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art\naccuracy on the GSM8K benchmark of math word problems, surpassing even\nﬁnetuned GPT-3 with a veriﬁer.\nA: The cafeteria had 23 apples originally. They used \n20 to make lunch. So they had 23 - 20 = 3. They \nbought 6 more apples, so they have 3 + 6 = 9. The \nanswer is 9.\nChain-of-Thought Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of \ntennis balls. Each can has 3 tennis balls. How many \ntennis balls does he have now? \nA: The answer is 11. \nQ: The cafeteria had 23 apples. If they used 20 to \nmake lunch and bought 6 more, how many apples \ndo they have?\nA: The answer is 27.\nStandard Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of \ntennis balls. Each can has 3 tennis balls. How many \ntennis balls does he have now? \nA: Roger started with 5 balls. 2 cans of 3 tennis balls \neach is 6 tennis balls. 5 + 6 = 11. The answer is 11. \nQ: The cafeteria had 23 apples. If they used 20 to \nmake lunch and bought 6 more, how many apples \ndo they have?\nModel Input\nModel Output Model Output\nModel Input\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic,\ncommonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\narXiv:2201.11903v6  [cs.CL]  10 Jan 2023']","Natural language reasoning is beneficial in chain-of-thought prompting because it allows large language models to perform complex reasoning tasks by breaking them down into a series of intermediate steps. This helps the models to understand and process the reasoning process more effectively, leading to improved performance on tasks such as arithmetic, commonsense, and symbolic reasoning.","Natural language reasoning steps help translate complex problem semantics into manageable components, which is especially useful for challenging tasks like those in GSM8K.,",0.16666666666666666,0.9852129769627229,0.9999999999666667,1.0,0.0,0.7623663441112174
"What are the limitations of chain-of-thought prompting noted in the paper?,","['Chain-of-Thought Prompting Elicits Reasoning\nin Large Language Models\nJason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma\nBrian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou\nGoogle Research, Brain Team\n{jasonwei,dennyzhou}@google.com\nAbstract\nWe explore how generating a chain of thought—a series of intermediate reasoning\nsteps—signiﬁcantly improves the ability of large language models to perform\ncomplex reasoning. In particular, we show how such reasoning abilities emerge\nnaturally in sufﬁciently large language models via a simple method called chain-of-\nthought prompting, where a few chain of thought demonstrations are provided as\nexemplars in prompting.\nExperiments on three large language models show that chain-of-thought prompting\nimproves performance on a range of arithmetic, commonsense, and symbolic\nreasoning tasks. The empirical gains can be striking. For instance, prompting a\nPaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art\naccuracy on the GSM8K benchmark of math word problems, surpassing even\nﬁnetuned GPT-3 with a veriﬁer.\nA: The cafeteria had 23 apples originally. They used \n20 to make lunch. So they had 23 - 20 = 3. They \nbought 6 more apples, so they have 3 + 6 = 9. The \nanswer is 9.\nChain-of-Thought Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of \ntennis balls. Each can has 3 tennis balls. How many \ntennis balls does he have now? \nA: The answer is 11. \nQ: The cafeteria had 23 apples. If they used 20 to \nmake lunch and bought 6 more, how many apples \ndo they have?\nA: The answer is 27.\nStandard Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of \ntennis balls. Each can has 3 tennis balls. How many \ntennis balls does he have now? \nA: Roger started with 5 balls. 2 cans of 3 tennis balls \neach is 6 tennis balls. 5 + 6 = 11. The answer is 11. \nQ: The cafeteria had 23 apples. If they used 20 to \nmake lunch and bought 6 more, how many apples \ndo they have?\nModel Input\nModel Output Model Output\nModel Input\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic,\ncommonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\narXiv:2201.11903v6  [cs.CL]  10 Jan 2023', 'C Extended Related Work\nChain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-\ning, natural language explanations, program synthesis/execution, numeric and logical reasoning, and\nintermediate language steps.\nC.1 Prompting\nThe recent success of large-scale language models has led to growing interest in improving their\ncapability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a\nsurvey). This paper falls in the category of general prompting approaches, whereby input prompts are\noptimized to allow a single large language model to better perform a variety of tasks (Li and Liang,\n2021; Lester et al., 2021; Reif et al., 2022, inter alia).\nOne recent line of work aims to improve the ability of language models to perform a task by providing\ninstructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh\net al., 2022; Wang et al., 2022b). This line of work is related because it also augments input–output\npairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically\nprepended to the inputs), chain-of-thought prompting augments the outputs of language models.\nAnother related direction is sequentially combining the outputs of language models; human–computer\ninteraction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of\nlanguage models improves task outcomes in a 20-person user study.\nC.2 Natural language explanations\nAnother closely related direction uses natural language explanations (NLEs), often with the goal of\nimproving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi´c, 2021, inter alia). That\nline of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,\n2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ﬁnal\nprediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,\nthe chain of thought processing considered in this paper occurs before the ﬁnal answer. And while\nNLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of\nchain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple\nsteps—interpretability is just a side effect. Marasovi ´c et al. (2022) show that prompt-based ﬁnetuning\nwith NLE improves NLI and classiﬁcation performance, though they largely focus on evaluating\nexplanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,\nand symbolic tasks that require multi-hop reasoning.\nC.3 Program synthesis and execution\nUsing intermediate reasoning steps has a long history in program synthesis and execution (Zaremba\nand Sutskever, 2014, inter alia). Recent work along in this direction has included a number of\narchitectural innovations (Cai et al., 2017; Dong et al., 2019; Yan et al., 2020), as well as the use of\nlarge language models (Chen et al., 2021; Austin et al., 2021). The program execution work closest to\nours is perhaps Nye et al. (2021), which show that large language models can perform up to 10-digit\naddition, evaluate polynomials, and execute python programs. Whereas generating a program and\nthen executing it can be viewed as a type of reasoning, our work generalizes such domain-speciﬁc\nprimitives to natural language, which is open-domain and relevant to any text-to-text NLP task in\nprinciple.\nC.4 Numeric and logical reasoning\nNumeric and logical reasoning has been a long-studied task in machine learning and natural language\nprocessing (Lev et al., 2004, inter alia). Recent work has also aimed to inject numeric reasoning\nabilities in language models in various ways, such as augmenting BERT with a predeﬁned set of\nexecutable operations (Andor et al., 2019), including a graph neural network (Ran et al., 2019), and\nusing specialized training procedures (Pi˛ ekos et al., 2021). Another line of work aims to enable\nlanguage models to perform logical or formal reasoning, often by verablizing the rules in natural\nlanguage formal rules using language (Clark et al., 2020; Saeed et al., 2021; Liang et al., 2021).\n24', 'experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\nreasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\nchain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\nall experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\nmodel. No language models were ﬁnetuned in the process of writing this paper.\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\n(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ﬂat scaling curve, chain-\nof-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\nappears to expand the set of tasks that large language models can perform successfully—in other\nwords, our work underscores that standard prompting only provides a lower bound on the capabilities\nof large language models. This observation likely raises more questions than it answers—for instance,\nhow much more can we expect reasoning ability to improve with a further increase in model scale?\nWhat other prompting methods might expand the range of tasks that language models can solve?\nAs for limitations, we ﬁrst qualify that although chain of thought emulates the thought processes of\nhuman reasoners, this does not answer whether the neural network is actually “reasoning,” which\nwe leave as an open question. Second, although the cost of manually augmenting exemplars with\nchains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\nﬁnetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\ngeneralization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\nand incorrect answers; improving factual generations of language models is an open direction for\nfuture work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022,inter alia). Finally,\nthe emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\nreal-world applications; further research could explore how to induce reasoning in smaller models.\n7 Related Work\nThis work is inspired by many research areas, which we detail in an extended related work section\n(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\nThe ﬁrst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\npioneer the idea of using natural language rationales to solve math word problems through a series\nof intermediate steps. Their work is a remarkable contrast to the literature using formal languages\nto reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\net al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ﬁnetune a pretrained\nlanguage model rather than training a model from scratch. In the domain of program synthesis,\nNye et al. (2021) leverage language models to predict the ﬁnal outputs of Python programs via\nﬁrst line-to-line predicting the intermediate computational results, and show that their step-by-step\nprediction method performs better than directly predicting the ﬁnal outputs.\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the\npopularization of few-shot prompting as given by Brown et al. (2020), several general approaches\nhave improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\n2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\net al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\noutputs of language models with a chain of thought.\n8 Conclusions\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\ning reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\nreasoning, we ﬁnd that chain-of-thought reasoning is an emergent property of model scale that allows\nsufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves.\nBroadening the range of reasoning tasks that language models can perform will hopefully inspire\nfurther work on language-based approaches to reasoning.\n9']","The limitations of chain-of-thought prompting noted in the paper are:
1. It is not clear whether the neural network is actually ""reasoning.""
2. The cost of manually augmenting exemplars with chains of thought may be prohibitive for finetuning.
3. There is no guarantee of correct reasoning paths, which can lead to both correct and incorrect answers.
4. The emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in real-world applications.","Chain-of-thought reasoning requires large model scales, is not always factually accurate, and may incur high annotation costs for generating reasoning steps.,",1.0,0.9904536776033833,0.9999999999666667,,0.0,0.7275710221597879
"What are the observed performance trends for symbolic reasoning tasks?,","['A boy sits on a bench in a park, working on homework.', 'a rock like table with a notebook and pen on it', 'a person sitting on  desk operating a laptop next to two other ']",There is no information provided in the context about observed performance trends for symbolic reasoning tasks.,"Chain-of-thought prompting enhances in-domain and out-of-domain generalization in symbolic tasks, such as letter concatenation and coin flips, particularly with larger models like PaLM 540B.,",1.0,0.0,0.0,0.0,0.0,0.2020524452111342
"What methods were tested to isolate the benefits of chain-of-thought reasoning?,","['a man in a white lab coat holding a hacksaw', 'a number of people sitting on benches ', 'a couple of people sitting around a table ']",There is no mention of any methods being tested to isolate the benefits of chain-of-thought reasoning in the given context.,"Ablation studies included variations like equation-only prompts, variable computation-only prompts, and reasoning steps added after the answer, all of which showed reduced effectiveness compared to chain-of-thought.,",1.0,0.0,0.0,0.0,0.0,0.2086402815476138
"What prompts were used in commonsense reasoning benchmarks?,","['source (examples in this dataset already included reasoning steps like a chain of thought). 2 Fig-\nure 6 shows that these prompts performed comparably with our manually written exemplars, also\nsubstantially outperforming standard prompting.\nIn addition to robustness to annotators, independently-written chains of thought, different exemplars,\nand various language models, we also ﬁnd that chain-of-thought prompting for arithmetic reasoning\nis robust to different exemplar orders and varying numbers of exemplars (see Appendix A.2).\n4 Commonsense Reasoning\nAlthough chain of thought is particularly suitable for math word problems, the language-based nature\nof chain of thought actually makes it applicable to a broad class of commonsense reasoning problems,\nwhich involve reasoning about physical and human interactions under the presumption of general\nbackground knowledge. Commonsense reasoning is key for interacting with the world and is still\nbeyond the reach of current natural language understanding systems (Talmor et al., 2021).\nBenchmarks. We consider ﬁve datasets covering a diverse range of commonsense reasoning types.\nThe popular CSQA (Talmor et al., 2019) asks commonsense questions about the world involving\ncomplex semantics that often require prior knowledge. StrategyQA (Geva et al., 2021) requires\nmodels to infer a multi-hop strategy to answer questions. We choose two specialized evaluation sets\nfrom the BIG-bench effort (BIG-bench collaboration, 2021): Date Understanding, which involves\ninferring a date from a given context, andSports Understanding, which involves determining whether\na sentence relating to sports is plausible or implausible. Finally, the SayCan dataset (Ahn et al.,\n2022) involves mapping a natural language instruction to a sequence of robot actions from a discrete\nset. Figure 3 shows examples with chain of thought annotations for all datasets.\nPrompts. We follow the same experimental setup as the prior section. For CSQA and StrategyQA,\nwe randomly selected examples from the training set and manually composed chains of thought for\nthem to use as few-shot exemplars. The two BIG-bench tasks do not have training sets, so we selected\nthe ﬁrst ten examples as exemplars in the evaluation set as few-shot exemplars and report numbers on\nthe rest of the evaluation set. For SayCan, we use six examples from the training set used in Ahn et al.\n(2022) and also manually composed chains of thought.\nResults. Figure 7 highlights these results for PaLM (full results for LaMDA, GPT-3, and different\nmodel scales are shown in Table 4). For all tasks, scaling up model size improved the performance\nof standard prompting; chain-of-thought prompting led to further gains, with improvements appear-\ning to be largest for PaLM 540B. With chain-of-thought prompting, PaLM 540B achieved strong\nperformance relative to baselines, outperforming the prior state of the art on StrategyQA (75.6% vs\n69.4%) and outperforming an unaided sports enthusiast on sports understanding (95.4% vs 84%).\nThese results demonstrate that chain-of-thought prompting can also improve performance on tasks\nrequiring a range of commonsense reasoning abilities (though note that gain was minimal on CSQA).\n8 62 540\n20\n40\n60\n80\n100Solve rate (%)\nCSQA\n8 62 540\n50\n60\n70\n80\n90\nStrategyQA\nStandard prompting\nChain of thought\nPrior supervised best\nHuman\n8 62 540\n0\n20\n40\n60\n80\nModel scale (# parameters in billions)\nDate\n8 62 540\n40\n60\n80\n100\nSports\n8 62 540\n20\n40\n60\n80\n100\nSayCan\nFigure 7: Chain-of-thought prompting also improves the commonsense reasoning abilities of\nlanguage models. The language model shown here is PaLM. Prior best numbers are from the\nleaderboards of CSQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021) (single-model only,\nas of May 5, 2022). Additional results using various sizes of LaMDA, GPT-3, and PaLM are shown\nin Table 4.\n2We sample examples ≤60 tokens to ﬁt into our input context window, and also limit the examples to ≤2\nsteps to solve for a fair comparison with the eight exemplars that we composed.\n7', 'Task NumerSense QASC\nPrompt Generate some numerical facts about objects. Examples: Generate some knowledge about the input. Examples:\nInput:penguins have <mask> wings. Input:What type of water formation is formed by clouds?\nKnowledge:Birds have two wings. Penguin is a kind of bird.Knowledge:Clouds are made of water vapor.\n... ...\nInput:a typical human being has <mask> limbs. Input:The process by which genes are passed is\nKnowledge:Human has two arms and two legs. Knowledge:Genes are passed from parent to offspring.\nInput:{question} Input:{question}\nKnowledge: Knowledge:\nTable 2: Prompts for knowledge generation for two of our tasks, NumerSense and QASC. The prompt consists of\nan instruction, ﬁve demonstrations of question-knowledge pairs, and a new question placeholder. For full prompts\non all the tasks we evaluate on, see Appendix A.2.\nchallenges posed by the task (e.g. numerical com-\nmonsense, scientiﬁc commonsense). We pair each\nquestion with a knowledge statement that turns the\ncommonsense problem posed by the question into\nan explicit reasoning procedure, without directly\nanswering the question. For example, the knowl-\nedge statement Birds have two wings. Penguin is a\nkind of bird. is helpful for the question Penguins\nhave <mask> wings, because it turns the problem\ninto deductive reasoning. Meanwhile, Penguins\nhave two wings. would be a poor knowledge state-\nment to demonstrate according to our guideline.\nWhen generating knowledge for a new question\nq, we plug the question into the placeholder, and\nrepeatedly sample generated continuations of this\nprompt to obtain a set of knowledge statements\nKq = {k1,k2,...,k M }. For full prompts on all\nthe tasks we evaluate on, see Appendix A.2.\n2.2 Knowledge Integration via Prompting\nIn the knowledge integration step, we use a lan-\nguage model – called the inference model – to\nmake predictions with each generated knowledge\nstatement, then select the highest-conﬁdence pre-\ndiction. Speciﬁcally, we use each knowledge state-\nment to prompt the model, forming M knowledge-\naugmented questions:\nq0 = q,q1 = [k1||q],...,q M = [kM ||q],\nwhere [·||·] denotes text concatenation.\nWe compute an aggregated score for each answer\nchoice ausing the augmented question that best\nsupports it under the inference model:\npI(a|q,Kq) ∝ max\n0≤m≤M\npI(a|qm). (1)\nIntuitively, this favors knowledge statements that\nstrongly support one of the choices.\nThe predicted answer is then,\nˆa= arg max\na∈Aq\nmax\n0≤m≤M\npI(a|qm),\nwhich is the choice that gets most support from one\nof the knowledge statements. This prediction uses\na single knowledge statement, which we refer to as\nthe selected knowledge:\nˆk= kˆm where ˆm= arg max\n0≤m≤M\nmax\na∈Aq\npI(a|qm).\nThe inference model may be any existing lan-\nguage model taken off-the-shelf (i.e. zero-shot) or\nﬁnetuned on the task. We do not do any further\nﬁnetuning with knowledge prompting.\n3 Experimental Setup\nHere, we describe the implementation details of\nour method and how they are adapted to each task.\nFor knowledge generation, we use GPT-3\n(Brown et al., 2020) as the underlying language\nmodel, where our few-shot prompting method is\nmost effective. We generate M = 20knowledge\nstatements for each question with nucleus sampling\np= 0.5 (Holtzman et al., 2019), and discard repe-\ntitions and empty strings. Generation is terminated\nwhen it exceeds 64 tokens or hits the \\n token.1\nFor inference, we use off-the-shelf T5 (Raffel\net al., 2019) and GPT-3, as well as ﬁnetuned models\nthat are state-of-the-art on each dataset, including\nUniﬁedQA (UQA) (Khashabi et al., 2020) and Uni-\ncorn (Lourie et al., 2021). See details in the task\nsetup below.\n3.1 Datasets and Task Setup\nWe evaluate our method on four commonsense rea-\nsoning datasets which cover a variety of challenges\nand problem formats.\n1An exception is with the CSQA2 dataset, where for the\nbest results we choose M = 5and allow for up to 128 tokens\nin each generation.', 'Dataset Question / Knowledge Prediction Score Reasoning\nNumerSense clams have evolved to have [M] shells. no 0.37 | 0.18 Commonsense\nClams have a bivalve shell. two 0.89 Paraphrasing\nNumerSense an easel can have [M] or four legs. two 0.45 | 0.45 Commonsense\nA tripod is a kind of easel. three 0.46 Induction\nCSQA Where does a heifer’s master live? slaughter house 0.89 | 0.01 Commonsense\nThe master of a heifer is a farmer. farm house 0.92 Deduction\nCSQA Aside from water and nourishment what does your\ndog need?\nwalked 0.55 | 0.04 Commonsense\nDogs need attention and affection. lots of attention 0.91 Elimination\nCSQA I did not need a servant. I was not a what? in charge 0.47 | 0.32 Commonsense\nPeople who have servants are rich. rich person 0.99 Abduction\nCSQA2 Part of golf is trying to get a higher point total than\nothers.\nyes 1.00 | 0.00 Commonsense\nThe player with the lowest score wins. no 1.00 Negation\nCSQA2 Eighth plus eight is smaller than ﬁfteen. yes 0.97 | 0.03 Commonsense\nEighth plus eight is sixteen, which is larger than\nﬁfteen.\nno 1.00 Numerical\nQASC [M] is used for transportation. plastic 0.41 | 0.12 Commonsense\nBicycles are used for transportation. boats 0.74 Analogy\nTable 5: More examples where prompting with generated knowledge reduces the reasoning type and rectiﬁes the\nprediction. The ﬁrst row of each section is the original question and the inference results associated with it; the\nsecond row is a model-generated knowledge statement that prompts the inference model. We show correct answers\nin green, incorrect answers in red, and their corresponding scores assigned by the inference model.\nrect answer, while with knowledge prompting, the\ncorrect answer is assigned a much higher score.\nPrompting with generated knowledge can trans-\nform commonsense reasoning into explicit reason-\ning procedures such as paraphrasing, induction,\ndeduction, analogy, abductive reasoning, logical\nelimination, negation, and numerical reasoning.\n5 Related Work\nKnowledge can be elicited from pretrained lan-\nguage models. Numerous works have shown that\npretrained language models implicitly contain a\nlarge amount of knowledge that can be queried\nvia conditional generation (Davison et al., 2019;\nPetroni et al., 2019; Jiang et al., 2020). Conse-\nquently, these models can directly perform infer-\nence on tasks like commonsense reasoning (Trinh\nand Le, 2018; Yang et al., 2020), text classiﬁca-\ntion (Shin et al., 2020; Puri and Catanzaro, 2019),\nand natural language inference (Shin et al., 2020;\nSchick and Schütze, 2021). Inspired by these obser-\nvations, we elicit question-related knowledge in an\nexplicit form from language models and use them\nto guide the inference.\nLeveraging external knowledge for common-\nsense reasoning. Some work uses external com-\nmonsense knowledge bases to make improvements\non various NLP tasks, including commonsense rea-\nsoning. One approach is to inject commonsense\nknowledge into language models, either by pretrain-\ning on knowledge bases (Ma et al., 2021; Chang\net al., 2020; Mitra et al., 2019; Zhong et al., 2019)\nor ﬁnetuning the model so that it can reason with\nadditional retrieved knowledge (Chang et al., 2020;\nMitra et al., 2019; Bian et al., 2021). Another di-\nrection is to ground the question into a knowledge\ngraph and do inference with graph-based reasoning\n(Lin et al., 2019; Lv et al., 2020; Yasunaga et al.,\n2021).\nA common prerequisite of these methods is a\nhigh-quality, high-coverage, in-domain common-\nsense knowledge base (Ma et al., 2019). Some\ncommonsense reasoning datasets are derived from\nexisting knowledge bases; for example, Common-\nsenseQA (Talmor et al., 2019) is derived from\nConceptNet (Speer et al., 2017), and Social IQA\n(Sap et al., 2019b) is derived from ATOMIC (Sap\net al., 2019a). For such datasets, it is natural to\nelicit related knowledge from the underlying knowl-\nedge base that derived them, and typically this\nwould demonstrate considerable gains (Mitra et al.,\n2019; Chang et al., 2020). However, if there is\na domain mismatch between the dataset and the\nknowledge base, such gains tend to diminish (Mi-']",Chain-of-thought prompts were used in commonsense reasoning benchmarks.,"Prompts included few-shot exemplars with chain-of-thought annotations for datasets like CSQA, StrategyQA, and BIG-bench subsets.,",1.0,0.8727507343600057,0.9999999999666667,,0.3999999992,0.4727413439224129
"How does chain-of-thought prompting perform on tasks requiring background knowledge?,","['C Extended Related Work\nChain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-\ning, natural language explanations, program synthesis/execution, numeric and logical reasoning, and\nintermediate language steps.\nC.1 Prompting\nThe recent success of large-scale language models has led to growing interest in improving their\ncapability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a\nsurvey). This paper falls in the category of general prompting approaches, whereby input prompts are\noptimized to allow a single large language model to better perform a variety of tasks (Li and Liang,\n2021; Lester et al., 2021; Reif et al., 2022, inter alia).\nOne recent line of work aims to improve the ability of language models to perform a task by providing\ninstructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh\net al., 2022; Wang et al., 2022b). This line of work is related because it also augments input–output\npairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically\nprepended to the inputs), chain-of-thought prompting augments the outputs of language models.\nAnother related direction is sequentially combining the outputs of language models; human–computer\ninteraction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of\nlanguage models improves task outcomes in a 20-person user study.\nC.2 Natural language explanations\nAnother closely related direction uses natural language explanations (NLEs), often with the goal of\nimproving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi´c, 2021, inter alia). That\nline of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,\n2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ﬁnal\nprediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,\nthe chain of thought processing considered in this paper occurs before the ﬁnal answer. And while\nNLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of\nchain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple\nsteps—interpretability is just a side effect. Marasovi ´c et al. (2022) show that prompt-based ﬁnetuning\nwith NLE improves NLI and classiﬁcation performance, though they largely focus on evaluating\nexplanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,\nand symbolic tasks that require multi-hop reasoning.\nC.3 Program synthesis and execution\nUsing intermediate reasoning steps has a long history in program synthesis and execution (Zaremba\nand Sutskever, 2014, inter alia). Recent work along in this direction has included a number of\narchitectural innovations (Cai et al., 2017; Dong et al., 2019; Yan et al., 2020), as well as the use of\nlarge language models (Chen et al., 2021; Austin et al., 2021). The program execution work closest to\nours is perhaps Nye et al. (2021), which show that large language models can perform up to 10-digit\naddition, evaluate polynomials, and execute python programs. Whereas generating a program and\nthen executing it can be viewed as a type of reasoning, our work generalizes such domain-speciﬁc\nprimitives to natural language, which is open-domain and relevant to any text-to-text NLP task in\nprinciple.\nC.4 Numeric and logical reasoning\nNumeric and logical reasoning has been a long-studied task in machine learning and natural language\nprocessing (Lev et al., 2004, inter alia). Recent work has also aimed to inject numeric reasoning\nabilities in language models in various ways, such as augmenting BERT with a predeﬁned set of\nexecutable operations (Andor et al., 2019), including a graph neural network (Ran et al., 2019), and\nusing specialized training procedures (Pi˛ ekos et al., 2021). Another line of work aims to enable\nlanguage models to perform logical or formal reasoning, often by verablizing the rules in natural\nlanguage formal rules using language (Clark et al., 2020; Saeed et al., 2021; Liang et al., 2021).\n24', 'Chain-of-Thought Prompting Elicits Reasoning\nin Large Language Models\nJason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma\nBrian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou\nGoogle Research, Brain Team\n{jasonwei,dennyzhou}@google.com\nAbstract\nWe explore how generating a chain of thought—a series of intermediate reasoning\nsteps—signiﬁcantly improves the ability of large language models to perform\ncomplex reasoning. In particular, we show how such reasoning abilities emerge\nnaturally in sufﬁciently large language models via a simple method called chain-of-\nthought prompting, where a few chain of thought demonstrations are provided as\nexemplars in prompting.\nExperiments on three large language models show that chain-of-thought prompting\nimproves performance on a range of arithmetic, commonsense, and symbolic\nreasoning tasks. The empirical gains can be striking. For instance, prompting a\nPaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art\naccuracy on the GSM8K benchmark of math word problems, surpassing even\nﬁnetuned GPT-3 with a veriﬁer.\nA: The cafeteria had 23 apples originally. They used \n20 to make lunch. So they had 23 - 20 = 3. They \nbought 6 more apples, so they have 3 + 6 = 9. The \nanswer is 9.\nChain-of-Thought Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of \ntennis balls. Each can has 3 tennis balls. How many \ntennis balls does he have now? \nA: The answer is 11. \nQ: The cafeteria had 23 apples. If they used 20 to \nmake lunch and bought 6 more, how many apples \ndo they have?\nA: The answer is 27.\nStandard Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of \ntennis balls. Each can has 3 tennis balls. How many \ntennis balls does he have now? \nA: Roger started with 5 balls. 2 cans of 3 tennis balls \neach is 6 tennis balls. 5 + 6 = 11. The answer is 11. \nQ: The cafeteria had 23 apples. If they used 20 to \nmake lunch and bought 6 more, how many apples \ndo they have?\nModel Input\nModel Output Model Output\nModel Input\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic,\ncommonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\narXiv:2201.11903v6  [cs.CL]  10 Jan 2023', 'source (examples in this dataset already included reasoning steps like a chain of thought). 2 Fig-\nure 6 shows that these prompts performed comparably with our manually written exemplars, also\nsubstantially outperforming standard prompting.\nIn addition to robustness to annotators, independently-written chains of thought, different exemplars,\nand various language models, we also ﬁnd that chain-of-thought prompting for arithmetic reasoning\nis robust to different exemplar orders and varying numbers of exemplars (see Appendix A.2).\n4 Commonsense Reasoning\nAlthough chain of thought is particularly suitable for math word problems, the language-based nature\nof chain of thought actually makes it applicable to a broad class of commonsense reasoning problems,\nwhich involve reasoning about physical and human interactions under the presumption of general\nbackground knowledge. Commonsense reasoning is key for interacting with the world and is still\nbeyond the reach of current natural language understanding systems (Talmor et al., 2021).\nBenchmarks. We consider ﬁve datasets covering a diverse range of commonsense reasoning types.\nThe popular CSQA (Talmor et al., 2019) asks commonsense questions about the world involving\ncomplex semantics that often require prior knowledge. StrategyQA (Geva et al., 2021) requires\nmodels to infer a multi-hop strategy to answer questions. We choose two specialized evaluation sets\nfrom the BIG-bench effort (BIG-bench collaboration, 2021): Date Understanding, which involves\ninferring a date from a given context, andSports Understanding, which involves determining whether\na sentence relating to sports is plausible or implausible. Finally, the SayCan dataset (Ahn et al.,\n2022) involves mapping a natural language instruction to a sequence of robot actions from a discrete\nset. Figure 3 shows examples with chain of thought annotations for all datasets.\nPrompts. We follow the same experimental setup as the prior section. For CSQA and StrategyQA,\nwe randomly selected examples from the training set and manually composed chains of thought for\nthem to use as few-shot exemplars. The two BIG-bench tasks do not have training sets, so we selected\nthe ﬁrst ten examples as exemplars in the evaluation set as few-shot exemplars and report numbers on\nthe rest of the evaluation set. For SayCan, we use six examples from the training set used in Ahn et al.\n(2022) and also manually composed chains of thought.\nResults. Figure 7 highlights these results for PaLM (full results for LaMDA, GPT-3, and different\nmodel scales are shown in Table 4). For all tasks, scaling up model size improved the performance\nof standard prompting; chain-of-thought prompting led to further gains, with improvements appear-\ning to be largest for PaLM 540B. With chain-of-thought prompting, PaLM 540B achieved strong\nperformance relative to baselines, outperforming the prior state of the art on StrategyQA (75.6% vs\n69.4%) and outperforming an unaided sports enthusiast on sports understanding (95.4% vs 84%).\nThese results demonstrate that chain-of-thought prompting can also improve performance on tasks\nrequiring a range of commonsense reasoning abilities (though note that gain was minimal on CSQA).\n8 62 540\n20\n40\n60\n80\n100Solve rate (%)\nCSQA\n8 62 540\n50\n60\n70\n80\n90\nStrategyQA\nStandard prompting\nChain of thought\nPrior supervised best\nHuman\n8 62 540\n0\n20\n40\n60\n80\nModel scale (# parameters in billions)\nDate\n8 62 540\n40\n60\n80\n100\nSports\n8 62 540\n20\n40\n60\n80\n100\nSayCan\nFigure 7: Chain-of-thought prompting also improves the commonsense reasoning abilities of\nlanguage models. The language model shown here is PaLM. Prior best numbers are from the\nleaderboards of CSQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021) (single-model only,\nas of May 5, 2022). Additional results using various sizes of LaMDA, GPT-3, and PaLM are shown\nin Table 4.\n2We sample examples ≤60 tokens to ﬁt into our input context window, and also limit the examples to ≤2\nsteps to solve for a fair comparison with the eight exemplars that we composed.\n7']","Chain-of-thought prompting performs well on tasks requiring background knowledge, as shown in the context where it improved performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The results demonstrate that chain-of-thought prompting can also improve performance on tasks requiring a range of commonsense reasoning abilities.","It improves performance on tasks involving general background knowledge, such as StrategyQA and Sports Understanding, though gains on some datasets like CSQA were minimal.,",0.8,0.9268445857495152,0.9999999999666667,1.0,0.9999999966666667,0.39382818761014843
"What prior work inspired the use of chain-of-thought prompting?,","['C Extended Related Work\nChain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-\ning, natural language explanations, program synthesis/execution, numeric and logical reasoning, and\nintermediate language steps.\nC.1 Prompting\nThe recent success of large-scale language models has led to growing interest in improving their\ncapability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a\nsurvey). This paper falls in the category of general prompting approaches, whereby input prompts are\noptimized to allow a single large language model to better perform a variety of tasks (Li and Liang,\n2021; Lester et al., 2021; Reif et al., 2022, inter alia).\nOne recent line of work aims to improve the ability of language models to perform a task by providing\ninstructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh\net al., 2022; Wang et al., 2022b). This line of work is related because it also augments input–output\npairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically\nprepended to the inputs), chain-of-thought prompting augments the outputs of language models.\nAnother related direction is sequentially combining the outputs of language models; human–computer\ninteraction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of\nlanguage models improves task outcomes in a 20-person user study.\nC.2 Natural language explanations\nAnother closely related direction uses natural language explanations (NLEs), often with the goal of\nimproving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi´c, 2021, inter alia). That\nline of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,\n2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ﬁnal\nprediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,\nthe chain of thought processing considered in this paper occurs before the ﬁnal answer. And while\nNLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of\nchain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple\nsteps—interpretability is just a side effect. Marasovi ´c et al. (2022) show that prompt-based ﬁnetuning\nwith NLE improves NLI and classiﬁcation performance, though they largely focus on evaluating\nexplanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,\nand symbolic tasks that require multi-hop reasoning.\nC.3 Program synthesis and execution\nUsing intermediate reasoning steps has a long history in program synthesis and execution (Zaremba\nand Sutskever, 2014, inter alia). Recent work along in this direction has included a number of\narchitectural innovations (Cai et al., 2017; Dong et al., 2019; Yan et al., 2020), as well as the use of\nlarge language models (Chen et al., 2021; Austin et al., 2021). The program execution work closest to\nours is perhaps Nye et al. (2021), which show that large language models can perform up to 10-digit\naddition, evaluate polynomials, and execute python programs. Whereas generating a program and\nthen executing it can be viewed as a type of reasoning, our work generalizes such domain-speciﬁc\nprimitives to natural language, which is open-domain and relevant to any text-to-text NLP task in\nprinciple.\nC.4 Numeric and logical reasoning\nNumeric and logical reasoning has been a long-studied task in machine learning and natural language\nprocessing (Lev et al., 2004, inter alia). Recent work has also aimed to inject numeric reasoning\nabilities in language models in various ways, such as augmenting BERT with a predeﬁned set of\nexecutable operations (Andor et al., 2019), including a graph neural network (Ran et al., 2019), and\nusing specialized training procedures (Pi˛ ekos et al., 2021). Another line of work aims to enable\nlanguage models to perform logical or formal reasoning, often by verablizing the rules in natural\nlanguage formal rules using language (Clark et al., 2020; Saeed et al., 2021; Liang et al., 2021).\n24', 'Chain-of-Thought Prompting Elicits Reasoning\nin Large Language Models\nJason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma\nBrian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou\nGoogle Research, Brain Team\n{jasonwei,dennyzhou}@google.com\nAbstract\nWe explore how generating a chain of thought—a series of intermediate reasoning\nsteps—signiﬁcantly improves the ability of large language models to perform\ncomplex reasoning. In particular, we show how such reasoning abilities emerge\nnaturally in sufﬁciently large language models via a simple method called chain-of-\nthought prompting, where a few chain of thought demonstrations are provided as\nexemplars in prompting.\nExperiments on three large language models show that chain-of-thought prompting\nimproves performance on a range of arithmetic, commonsense, and symbolic\nreasoning tasks. The empirical gains can be striking. For instance, prompting a\nPaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art\naccuracy on the GSM8K benchmark of math word problems, surpassing even\nﬁnetuned GPT-3 with a veriﬁer.\nA: The cafeteria had 23 apples originally. They used \n20 to make lunch. So they had 23 - 20 = 3. They \nbought 6 more apples, so they have 3 + 6 = 9. The \nanswer is 9.\nChain-of-Thought Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of \ntennis balls. Each can has 3 tennis balls. How many \ntennis balls does he have now? \nA: The answer is 11. \nQ: The cafeteria had 23 apples. If they used 20 to \nmake lunch and bought 6 more, how many apples \ndo they have?\nA: The answer is 27.\nStandard Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of \ntennis balls. Each can has 3 tennis balls. How many \ntennis balls does he have now? \nA: Roger started with 5 balls. 2 cans of 3 tennis balls \neach is 6 tennis balls. 5 + 6 = 11. The answer is 11. \nQ: The cafeteria had 23 apples. If they used 20 to \nmake lunch and bought 6 more, how many apples \ndo they have?\nModel Input\nModel Output Model Output\nModel Input\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic,\ncommonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\narXiv:2201.11903v6  [cs.CL]  10 Jan 2023', 'experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\nreasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\nchain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\nall experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\nmodel. No language models were ﬁnetuned in the process of writing this paper.\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\n(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ﬂat scaling curve, chain-\nof-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\nappears to expand the set of tasks that large language models can perform successfully—in other\nwords, our work underscores that standard prompting only provides a lower bound on the capabilities\nof large language models. This observation likely raises more questions than it answers—for instance,\nhow much more can we expect reasoning ability to improve with a further increase in model scale?\nWhat other prompting methods might expand the range of tasks that language models can solve?\nAs for limitations, we ﬁrst qualify that although chain of thought emulates the thought processes of\nhuman reasoners, this does not answer whether the neural network is actually “reasoning,” which\nwe leave as an open question. Second, although the cost of manually augmenting exemplars with\nchains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\nﬁnetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\ngeneralization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\nand incorrect answers; improving factual generations of language models is an open direction for\nfuture work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022,inter alia). Finally,\nthe emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\nreal-world applications; further research could explore how to induce reasoning in smaller models.\n7 Related Work\nThis work is inspired by many research areas, which we detail in an extended related work section\n(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\nThe ﬁrst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\npioneer the idea of using natural language rationales to solve math word problems through a series\nof intermediate steps. Their work is a remarkable contrast to the literature using formal languages\nto reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\net al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ﬁnetune a pretrained\nlanguage model rather than training a model from scratch. In the domain of program synthesis,\nNye et al. (2021) leverage language models to predict the ﬁnal outputs of Python programs via\nﬁrst line-to-line predicting the intermediate computational results, and show that their step-by-step\nprediction method performs better than directly predicting the ﬁnal outputs.\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the\npopularization of few-shot prompting as given by Brown et al. (2020), several general approaches\nhave improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\n2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\net al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\noutputs of language models with a chain of thought.\n8 Conclusions\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\ning reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\nreasoning, we ﬁnd that chain-of-thought reasoning is an emergent property of model scale that allows\nsufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves.\nBroadening the range of reasoning tasks that language models can perform will hopefully inspire\nfurther work on language-based approaches to reasoning.\n9']","Prior work on prompting, natural language explanations, program synthesis/execution, numeric and logical reasoning, and intermediate language steps inspired the use of chain-of-thought prompting.","Prior work includes rationale-augmented training for math problems and few-shot prompting techniques for question-answering tasks.,",0.16666666666666666,0.9737323447427156,0.9999999999666667,1.0,0.0,0.22127451933424613
"How does chain-of-thought prompting address variable computation needs?,","['Chain-of-Thought Prompting Elicits Reasoning\nin Large Language Models\nJason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma\nBrian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou\nGoogle Research, Brain Team\n{jasonwei,dennyzhou}@google.com\nAbstract\nWe explore how generating a chain of thought—a series of intermediate reasoning\nsteps—signiﬁcantly improves the ability of large language models to perform\ncomplex reasoning. In particular, we show how such reasoning abilities emerge\nnaturally in sufﬁciently large language models via a simple method called chain-of-\nthought prompting, where a few chain of thought demonstrations are provided as\nexemplars in prompting.\nExperiments on three large language models show that chain-of-thought prompting\nimproves performance on a range of arithmetic, commonsense, and symbolic\nreasoning tasks. The empirical gains can be striking. For instance, prompting a\nPaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art\naccuracy on the GSM8K benchmark of math word problems, surpassing even\nﬁnetuned GPT-3 with a veriﬁer.\nA: The cafeteria had 23 apples originally. They used \n20 to make lunch. So they had 23 - 20 = 3. They \nbought 6 more apples, so they have 3 + 6 = 9. The \nanswer is 9.\nChain-of-Thought Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of \ntennis balls. Each can has 3 tennis balls. How many \ntennis balls does he have now? \nA: The answer is 11. \nQ: The cafeteria had 23 apples. If they used 20 to \nmake lunch and bought 6 more, how many apples \ndo they have?\nA: The answer is 27.\nStandard Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of \ntennis balls. Each can has 3 tennis balls. How many \ntennis balls does he have now? \nA: Roger started with 5 balls. 2 cans of 3 tennis balls \neach is 6 tennis balls. 5 + 6 = 11. The answer is 11. \nQ: The cafeteria had 23 apples. If they used 20 to \nmake lunch and bought 6 more, how many apples \ndo they have?\nModel Input\nModel Output Model Output\nModel Input\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic,\ncommonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\narXiv:2201.11903v6  [cs.CL]  10 Jan 2023', 'language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\nprovided in the exemplars for few-shot prompting.\nFigure 1 shows an example of a model producing a chain of thought to solve a math word problem\nthat it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\nand can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\nmimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations\ntypically come after the ﬁnal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al.,\n2022, inter alia)).\nChain-of-thought prompting has several attractive properties as an approach for facilitating reasoning\nin language models.\n1. First, chain of thought, in principle, allows models to decompose multi-step problems into\nintermediate steps, which means that additional computation can be allocated to problems\nthat require more reasoning steps.\n2. Second, a chain of thought provides an interpretable window into the behavior of the model,\nsuggesting how it might have arrived at a particular answer and providing opportunities\nto debug where the reasoning path went wrong (although fully characterizing a model’s\ncomputations that support an answer remains an open question).\n3. Third, chain-of-thought reasoning can be used for tasks such as math word problems,\ncommonsense reasoning, and symbolic manipulation, and is potentially applicable (at least\nin principle) to any task that humans can solve via language.\n4. Finally, chain-of-thought reasoning can be readily elicited in sufﬁciently large off-the-shelf\nlanguage models simply by including examples of chain of thought sequences into the\nexemplars of few-shot prompting.\nIn empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic\nreasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5).\n3 Arithmetic Reasoning\nWe begin by considering math word problems of the form in Figure 1, which measure the arithmetic\nreasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where\nlanguage models often struggle (Hendrycks et al., 2021; Patel et al., 2021,inter alia). Strikingly, chain-\nof-thought prompting when used with the 540B parameter language model performs comparably with\ntask-speciﬁc ﬁnetuned models on several tasks, even achieving new state of the art on the challenging\nGSM8K benchmark (Cobbe et al., 2021).\n3.1 Experimental Setup\nWe explore chain-of-thought prompting for various language models on multiple benchmarks.\nBenchmarks. We consider the following ﬁve math word problem benchmarks: (1) the GSM8K\nbenchmark of math word problems (Cobbe et al., 2021), (2) the SV AMPdataset of math word\nproblems with varying structures (Patel et al., 2021), (3) the ASDiv dataset of diverse math word\nproblems (Miao et al., 2020), (4) the AQuA dataset of algebraic word problems, and (5) the MA WPS\nbenchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\nStandard prompting. For the baseline, we consider standard few-shot prompting, popularized by\nBrown et al. (2020), in which a language model is given in-context exemplars of input–output pairs\nbefore outputting a prediction for a test-time example. Exemplars are formatted as questions and\nanswers. The model gives the answer directly, as shown in Figure 1 (left).\nChain-of-thought prompting. Our proposed approach is to augment each exemplar in few-shot\nprompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most\nof the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars\nwith chains of thought for prompting—Figure 1 (right) shows one chain of thought exemplar, and the\nfull set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo\nprompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether\nchain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\n3', '0\n20\n40\n60\nGSM8K\nsolve rate (%)\nLaMDA GPT PaLM\nStandard prompting\nChain-of-thought prompting\nPrior supervised best\n0\n20\n40\n60\n80\nSV AMP\nsolve rate (%)\n0.4 8 137\n0\n25\n50\n75\n100\nMAWPS\nsolve rate (%)\n0.4 7 175 8 62 540\nModel scale (# parameters in billions)\nFigure 4: Chain-of-thought prompting enables\nlarge language models to solve challenging math\nproblems. Notably, chain-of-thought reasoning\nis an emergent ability of increasing model scale.\nPrior best numbers are from Cobbe et al. (2021)\nfor GSM8K, Jie et al. (2022) for SV AMP, and Lan\net al. (2021) for MAWPS.\nSecond, chain-of-thought prompting has larger\nperformance gains for more-complicated prob-\nlems. For instance, for GSM8K (the dataset\nwith the lowest baseline performance), perfor-\nmance more than doubled for the largest GPT\nand PaLM models. On the other hand, for Sin-\ngleOp, the easiest subset of MAWPS which only\nrequires a single step to solve, performance im-\nprovements were either negative or very small\n(see Appendix Table 3).\nThird, chain-of-thought prompting via GPT-3\n175B and PaLM 540B compares favorably to\nprior state of the art, which typically ﬁnetunes a\ntask-speciﬁc model on a labeled training dataset.\nFigure 4 shows how PaLM 540B uses chain-of-\nthought prompting to achieve new state of the art\non GSM8K, SV AMP, and MAWPS (though note\nthat standard prompting already passed the prior\nbest for SV AMP). On the other two datasets,\nAQuA and ASDiv, PaLM with chain-of-thought\nprompting reaches within 2% of the state of the\nart (Appendix Table 2).\nTo better understand why chain-of-thought\nprompting works, we manually examined model-\ngenerated chains of thought by LaMDA 137B\nfor GSM8K. Of 50 random examples where the\nmodel returned the correct ﬁnal answer, all of\nthe generated chains of thought were also log-\nically and mathematically correct except two\nthat coincidentally arrived at the correct answer\n(see Appendix D.1, and Table 8 for examples\nof correct model-generated chains of thought).\nWe also randomly examined 50 random sam-\nples for which the model gave the wrong answer.\nThe summary of this analysis is that 46% of the\nchains of thought were almost correct, barring\nminor mistakes (calculator error, symbol map-\nping error, or one reasoning step missing), and that the other 54% of the chains of thought had major\nerrors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into\nwhy scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors\nmade by PaLM 62B and whether those errors were ﬁxed by scaling to PaLM 540B. The summary\nis that scaling PaLM to 540B ﬁxes a large portion of one-step missing and semantic understanding\nerrors in the 62B model (see Appendix A.1).\n3.3 Ablation Study\nThe observed beneﬁts of using chain-of-thought prompting raises the natural question of whether the\nsame performance improvements can be conferred via other types of prompting. Figure 5 shows an\nablation study with three variations of chain of thought described below.\nEquation only. One reason for why chain-of-thought prompting might help is that it produces the\nmathematical equation to be evaluated, and so we test a variation where the model is prompted\nto output only a mathematical equation before giving the answer. Figure 5 shows that equation\nonly prompting does not help much for GSM8K, which implies that the semantics of the questions\nin GSM8K are too challenging to directly translate into an equation without the natural language\nreasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we ﬁnd\nthat equation only prompting does improve performance, since the equation can be easily derived\nfrom the question (see Appendix Table 6).\n5']","Chain-of-thought prompting addresses variable computation needs by allowing models to decompose multi-step problems into intermediate steps, allocating additional computation to problems that require more reasoning steps. This approach provides an interpretable window into the behavior of the model, suggesting how it arrived at a particular answer and offering opportunities to debug where the reasoning path went wrong. Additionally, chain-of-thought reasoning can be used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, potentially applicable to any task that humans can solve via language.","It allows models to allocate additional computation to harder problems by breaking them into intermediate reasoning steps, thus improving accuracy.,",0.9,0.9046899507746676,0.9999999999666667,1.0,0.0,0.4476089199825422
"What key observation is made about emergent abilities in large language models?,","['a big bird flies through the air ', 'A big elephant and a little elephant walking ', 'a large kite that is being flown in the sky']",No key observation about emergent abilities in large language models is made in the context provided.,"Chain-of-thought prompting reveals emergent reasoning abilities in large models, expanding the range of tasks they can perform and demonstrating flat scaling curves with standard prompting.,",0.5,0.0,0.0,0.0,0.0,0.20786853846089992
"What is the main purpose of the Generated Knowledge Prompting (GKP) method?,","['Generated Knowledge Prompting for Commonsense Reasoning\nJiacheng Liu♥ Alisa Liu♥ Ximing Lu♥♠ Sean Welleck♥♠\nPeter West♥♠ Ronan Le Bras♠ Yejin Choi♥♠ Hannaneh Hajishirzi♥♠\n♥Paul G. Allen School of Computer Science & Engineering, University of Washington\n♠Allen Institute for Artiﬁcial Intelligence\nliujc@cs.washington.edu\nAbstract\nIt remains an open question whether incorpo-\nrating external knowledge beneﬁts common-\nsense reasoning while maintaining the ﬂexi-\nbility of pretrained sequence models. To in-\nvestigate this question, we develop generated\nknowledge prompting, which consists of gen-\nerating knowledge from a language model,\nthen providing the knowledge as additional in-\nput when answering a question. Our method\ndoes not require task-speciﬁc supervision for\nknowledge integration, or access to a struc-\ntured knowledge base, yet it improves perfor-\nmance of large-scale, state-of-the-art models\non four commonsense reasoning tasks, achiev-\ning state-of-the-art results on numerical com-\nmonsense (NumerSense), general common-\nsense (CommonsenseQA 2.0), and scientiﬁc\ncommonsense (QASC) benchmarks. Gener-\nated knowledge prompting highlights large-\nscale language models as ﬂexible sources of\nexternal knowledge for improving common-\nsense reasoning. Our code is available at\ngithub.com/liujch1998/GKP\n1 Introduction\nIt remains an open research question whether exter-\nnal knowledge is needed for commonsense reason-\ning. On one hand, a substantial body of prior work\nhas reported that integrating external knowledge\ncan help improve task performance (Mitra et al.,\n2019; Bian et al., 2021, inter alia), especially if the\nknowledge is high quality (e.g. hand-crafted by ex-\nperts). On the other hand, recent leaderboards are\noften dominated by large-scale pretrained models\nthat are ﬁne-tuned on a target benchmark (Khashabi\net al., 2020; Lourie et al., 2021), suggesting that\nthe beneﬁts of external knowledge may wash away\nas the underlying models increase in size and are\npretrained on ever larger amounts of raw text.\nEven if external knowledge is found to be ef-\nfective on a particular task, ﬂexibility remains a\nfundamental hurdle to integrating external knowl-\nFigure 1: Generated knowledge prompting involves\n(i) using few-shot demonstrations to generate question-\nrelated knowledge statements from a language model;\n(ii) using a second language model to make predic-\ntions with each knowledge statement, then selecting the\nhighest-conﬁdence prediction.\nedge, as many benchmarks currently lack appropri-\nate knowledge bases with sufﬁcient coverage. Fur-\nthermore, prior methods often require task-speciﬁc,\ncustom supervision for knowledge integration (Mi-\ntra et al., 2019; Chang et al., 2020), introducing a\nburden for rapidly adapting new pretrained models\nto a wide variety of tasks.\nIn this paper, we investigate whether external\nknowledge can be helpful for commonsense rea-\nsoning, even on top of the largest state-of-the-art\npretrained models (e.g. T5-11b (Raffel et al., 2019)\nand its variants), with a focus on four recent com-\nmonsense benchmarks. To facilitate easier adap-\ntation with any zero-shot or ﬁnetuned models, we\npropose an approach that does not require access\nto a structured knowledge base or joint ﬁnetuning\nfor knowledge integration.\nThe key insight behind our method, Generated\nKnowledge Prompting (sketched in Figure 1), is\nthat we can generate useful knowledge from a lan-\nguage model, then provide the knowledge as an in-\nput prompt that is concatenated with a question. To\narXiv:2110.08387v3  [cs.CL]  28 Sep 2022', 'A Appendix\nA.1 Comparison with Prior Methods\nTable 6 summarizes the comparison between our\ngenerated knowledge prompting method and prior\nmethods that add generated text to an inference\nmodel for commonsense reasoning tasks. Our\nmethod is unique because it uses few-shot demon-\nstrations to prompt for knowledge generation, and\ncan apply to ﬁnetuned inference models without\njoint ﬁnetuning with knowledge.\nA.2 Prompts for Knowledge Generation\nTable 7 through 10 shows the full prompts for\nknowledge generation that we use for each eval-\nuated task: NumerSense, CSQA, CSQA2, and\nQASC.\nA.3 Human Evaluation Guidelines\nTable 11 and 12 shows the detailed guidelines we\nuse for human evaluation of generated knowledge.\nB Checklist\nB.1 Limitations and Risks\nLimitations. Our method is tested on a represen-\ntative selection of commonsense reasoning tasks\nand datasets. Applying this method to other tasks\nmay require people with moderate expertise to craft\na task-speciﬁc prompt to feed into the method.\nRisks. It is possible that our proposed method\nmay lower the performance of commonsense rea-\nsoning systems, if not implemented properly or\nusing badly-designed prompts. Such risk can be\nmitigated by following the prompt design guide-\nlines in this paper (§2.1).\nB.2 Computation\nWe do not train any new model in this paper. Infer-\nence is conducted on Quadro RTX 8000 GPUs and\ncosts about 200 GPU hours in total. Knowledge\ngeneration is done with the OpenAI GPT-3 API,\nwith an approximate cost of $500.\nOur method is implemented with PyTorch and\nthe Huggingface Transformers library.\nMethod Knowledge Generator Inference Model\nCAGE (Rajani et al., 2019) task-ﬁnetuned joint-ﬁnetuned\nLatcinnik and Berant (2020) task-ﬁnetuned joint-ﬁnetuned\nDynaGen (Bosselut et al., 2021) task-ﬁnetuned joint-ﬁnetuned\nSelf-talk (Shwartz et al., 2020) template-prompted 0-shot\nContrastive expl. (Paranjape et al., 2021) template-prompted 0-shot & joint-ﬁnetuned\nGenerated knowledge prompting (ours) demonstrations-prompted 0-shot & task-ﬁnetuned\nTable 6: Comparison of methods that add generated text to an inference model. Knowledge Generator: task-\nﬁnetuned – a model ﬁnetuned to generate task-speciﬁc knowledge; template-prompted – an off-the-shelf LM from\nwhich knowledge statements are elicited via templates;demonstration-prompted – an off-the-shelf LM from which\nknowledge statements are elicited via few-shot demonstrations (§2.1). Inference Model: 0-shot – an off-the-shelf\nLM that is set up to make predictions; task-ﬁnetuned – a model ﬁnetuned with task training data (and without\nseeing extra knowledge); joint-ﬁnetuned – a model ﬁnetuned with task training data and generated knowledge.', 'Task NumerSense QASC\nPrompt Generate some numerical facts about objects. Examples: Generate some knowledge about the input. Examples:\nInput:penguins have <mask> wings. Input:What type of water formation is formed by clouds?\nKnowledge:Birds have two wings. Penguin is a kind of bird.Knowledge:Clouds are made of water vapor.\n... ...\nInput:a typical human being has <mask> limbs. Input:The process by which genes are passed is\nKnowledge:Human has two arms and two legs. Knowledge:Genes are passed from parent to offspring.\nInput:{question} Input:{question}\nKnowledge: Knowledge:\nTable 2: Prompts for knowledge generation for two of our tasks, NumerSense and QASC. The prompt consists of\nan instruction, ﬁve demonstrations of question-knowledge pairs, and a new question placeholder. For full prompts\non all the tasks we evaluate on, see Appendix A.2.\nchallenges posed by the task (e.g. numerical com-\nmonsense, scientiﬁc commonsense). We pair each\nquestion with a knowledge statement that turns the\ncommonsense problem posed by the question into\nan explicit reasoning procedure, without directly\nanswering the question. For example, the knowl-\nedge statement Birds have two wings. Penguin is a\nkind of bird. is helpful for the question Penguins\nhave <mask> wings, because it turns the problem\ninto deductive reasoning. Meanwhile, Penguins\nhave two wings. would be a poor knowledge state-\nment to demonstrate according to our guideline.\nWhen generating knowledge for a new question\nq, we plug the question into the placeholder, and\nrepeatedly sample generated continuations of this\nprompt to obtain a set of knowledge statements\nKq = {k1,k2,...,k M }. For full prompts on all\nthe tasks we evaluate on, see Appendix A.2.\n2.2 Knowledge Integration via Prompting\nIn the knowledge integration step, we use a lan-\nguage model – called the inference model – to\nmake predictions with each generated knowledge\nstatement, then select the highest-conﬁdence pre-\ndiction. Speciﬁcally, we use each knowledge state-\nment to prompt the model, forming M knowledge-\naugmented questions:\nq0 = q,q1 = [k1||q],...,q M = [kM ||q],\nwhere [·||·] denotes text concatenation.\nWe compute an aggregated score for each answer\nchoice ausing the augmented question that best\nsupports it under the inference model:\npI(a|q,Kq) ∝ max\n0≤m≤M\npI(a|qm). (1)\nIntuitively, this favors knowledge statements that\nstrongly support one of the choices.\nThe predicted answer is then,\nˆa= arg max\na∈Aq\nmax\n0≤m≤M\npI(a|qm),\nwhich is the choice that gets most support from one\nof the knowledge statements. This prediction uses\na single knowledge statement, which we refer to as\nthe selected knowledge:\nˆk= kˆm where ˆm= arg max\n0≤m≤M\nmax\na∈Aq\npI(a|qm).\nThe inference model may be any existing lan-\nguage model taken off-the-shelf (i.e. zero-shot) or\nﬁnetuned on the task. We do not do any further\nﬁnetuning with knowledge prompting.\n3 Experimental Setup\nHere, we describe the implementation details of\nour method and how they are adapted to each task.\nFor knowledge generation, we use GPT-3\n(Brown et al., 2020) as the underlying language\nmodel, where our few-shot prompting method is\nmost effective. We generate M = 20knowledge\nstatements for each question with nucleus sampling\np= 0.5 (Holtzman et al., 2019), and discard repe-\ntitions and empty strings. Generation is terminated\nwhen it exceeds 64 tokens or hits the \\n token.1\nFor inference, we use off-the-shelf T5 (Raffel\net al., 2019) and GPT-3, as well as ﬁnetuned models\nthat are state-of-the-art on each dataset, including\nUniﬁedQA (UQA) (Khashabi et al., 2020) and Uni-\ncorn (Lourie et al., 2021). See details in the task\nsetup below.\n3.1 Datasets and Task Setup\nWe evaluate our method on four commonsense rea-\nsoning datasets which cover a variety of challenges\nand problem formats.\n1An exception is with the CSQA2 dataset, where for the\nbest results we choose M = 5and allow for up to 128 tokens\nin each generation.']",The main purpose of the Generated Knowledge Prompting (GKP) method is to investigate whether external knowledge can benefit commonsense reasoning while maintaining the flexibility of pretrained sequence models.,"The purpose of GKP is to improve commonsense reasoning by generating and integrating external knowledge from language models, enhancing the performance of state-of-the-art models on commonsense reasoning tasks.,",0.6666666666666666,0.9859638231410149,0.9999999999666667,1.0,0.0,0.5647727584343969
"What are the steps involved in the Generated Knowledge Prompting method?,","['a person sitting on  desk operating a laptop next to two other ', 'a person sitting at a desk with a monitor and keyboard', 'A computer with an image of lighting on the screen. ']",There is no information provided in the context about the steps involved in the Generated Knowledge Prompting method.,"GKP involves two main steps: (1) generating knowledge statements from a language model using a few-shot prompting technique and (2) integrating these statements into the inference process by concatenating them with the input question.,",1.0,0.0,0.0,0.0,0.0,0.21014965013525158
"How does GKP perform compared to retrieval-based methods?,","['Figure 2: Performance with different number of gen-\nerated knowledge statements per question (QASC dev\nset, T5-11b inference model).\nIntegration method QASC-dev\nours 58.32\nMixture-of-Experts 56.26\nProduct-of-Experts 55.94\nTable 4: Performance with different knowledge integra-\ntion methods (QASC dev set, T5-11b inference model).\nedge from models.\nOur knowledge is comparable with retrieval-\nbased knowledge. On NumerSense, the re-\ntrieved knowledge only improves inference per-\nformance by 0.18% on test-core and 1.02% on\ntest-all, while our method further outperforms it\nby 8.83% and 7.37%, respectively. This shows\nthat knowledge retrieved from a loosely-related\nknowledge base can be far less useful than our\ngenerated knowledge. On CSQA2, although we\nare not able to beat the web-retrieved knowledge,\nour method still bridges the performance gap with-\nout referring to Google search. For QASC, the\n“retrieved” knowledge is actually gold knowledge\nfrom a knowledge base that was used to construct\nthe dataset. As a result, our generated knowledge\nfalls signiﬁcantly short of the retrieved knowledge.\nIn summary, our generated knowledge is roughly\ncomparable with retrieved knowledge in terms of\ndownstream performance, and is most valuable\nwhen there is no appropriate in-domain knowledge\nbase to retrieve from.\n4.3 Analysis\nBetter performance with more knowledge.\nWe analyze the impact of the number of generated\nknowledge statements, M, and show the results\nin Figure 2. Generally, the performance increases\nwith the quantity of knowledge statements. It satu-\nrates at M = 20and begins to decline when more\nknowledge statements are introduced, which may\nbe because more noisy knowledge is generated.\nFigure 3: Improvement on top of different sizes of in-\nference model (Numersense dev set).\nFigure 4: Improvement by different sizes of knowledge\ngeneration model (Numersense dev set, T5-11b infer-\nence model).\nThe knowledge integration method. In addi-\ntion to the knowledge integration method described\nin §2.2, we experiment with two alternatives:\nMixture-of-Experts (MoE) and Product-of-Experts\n(PoE) (Hinton, 2002). These make the following\nmodiﬁcations to Equation 1, respectively:\nMoE: pI(a|q,Kq) ∝\n∑\n0≤m≤M\npI(a|qm), (2)\nPoE: pI(a|q,Kq) ∝\n∏\n0≤m≤M\npI(a|qm). (3)\nThe results in Table 4 indicate that our knowledge\nintegration method – i.e. adaptively choosing the\nbest knowledge to rely on – is best among the three.\nLightweight inference models and ampliﬁca-\ntion. We found that the size of inference model\naffects the magnitude of improvement. Figure 3\nshows the NumerSense performance gain on top\nof different sizes of inference model. As we use\nsmaller inference models, the performance gain in-\ncreases drastically. In particular, with our method\nthe smallest T5 model is as powerful as the T5-3b\nbaseline, and T5-large outperforms the GPT-3 base-\nline. This indicates that model-generated knowl-\nedge can enable high performing, yet lightweight,\ninference models. Furthermore, the improvement\ndoes not diminish as the inference model becomes', 'Figure 5: Human evaluation of generated knowledge. Left: Percentage of good knowledge statements along each\naxis. Right: Agreement between human and machine on helpfulness of selected knowledge.\nas big as the knowledge generation model, as the\ninference by GPT-3 can beneﬁt by 9.0% from the\nknowledge elicited from itself. This indicates that\nour method can somewhatamplify the useful knowl-\nedge already possessed by the model, leading to\nbetter predictions.\nThe size of knowledge generation model. Fig-\nure 4 shows the NumerSense performance gain\nwhen using different sizes of GPT-3 as the knowl-\nedge generation model. On top of the T5-11b in-\nference model, The 6.7B knowledge model gives\na 5.0% improvement, narrower than the 10.5% im-\nprovement given by the 175B knowledge model.\nThe 1.3B and 0.4B knowledge models do not give\na signiﬁcant improvement. Therefore, we do not\nnecessarily need the largest version of GPT-3 as the\nknowledge source, though we do need the model to\nbe relatively large in order to generate useful and\nreliable knowledge.\n4.4 Human Evaluation\nWe conduct a human evaluation on NumerSense\nand QASC to study the quality of generated knowl-\nedge and the interpretability of its impact on task\nperformance.\nEvaluation. We report the quality of knowledge\nstatements along four axes: (1) Grammaticality:\nwhether it is grammatical; (2) Relevance: whether\nit is relevant to the topic or concepts mentioned on\nthe question; (3) Factuality: whether it is (mostly)\nfactually correct; and (4) Helpfulness: whether it\nhelps answering the question in an either direct or\nindirect way, and may fall into one of the three cat-\negories: helpful (i.e. supports the correct answer),\nharmful (i.e. negates the correct answer or supports\nan incorrect answer), or neutral (neither helpful nor\nharmful). These metrics are adapted from Shwartz\net al. (2020) and are deﬁned in Appendix A.3.\nFrom each dataset, we sample up to 50 selected\nknowledge (§2.2) that change the correctness of\nT5-11b’s prediction (i.e. rectiﬁes model prediction\nfrom wrong to right, or misleads model prediction\nfrom right to wrong). The knowledge are labeled\nby two NLP experts and a moderate level of agree-\nment was reached (Fleiss Kappa κ= 0.57 (Landis\nand Koch, 1977)). To ensure objectivity, it is not\nrevealed to the annotators whether the knowledge\nrectiﬁes or misleads the model prediction.\nResults. Figure 5 summarizes the results. The\nvast majority of selected knowledge are grammati-\ncal and relevant to the question, and 83% of them\nare factually correct. 72% are seen as being helpful\nfor answering the question according the human\nevaluators, whereas 13% are harmful. Out of the\nknowledge statements that rectify the model pre-\ndictions, 93% are labeled as helpful by the human\nevaluators; in contrast, when the knowledge state-\nment misleads the model, only 21% are labeled\nas helpful, and 39% harmful. Of the knowledge\ndeemed helpful by human and rectiﬁes model pre-\ndiction, 95% are factual, while of those deemed\nharmful by human and misleads model prediction,\n86% are non-factual, suggesting that improving\nknowledge factuality is a promising path towards\nmore helpful knowledge. We also analyzed the non-\nselected knowledge and found that these statements\nhave slightly lower factuality and helpfulness than\nthe selected knowledge.\n4.5 Qualitative Examples\nTable 5 shows a few examples where the gener-\nated knowledge rectiﬁes model prediction. Due to\nspace constraints we only show the selected knowl-\nedge (§2.2) for each question. In all examples,\nthe model without prompted knowledge assigns a\nhigher score to an incorrect answer than the cor-', 'A B 1 B2 C D 1 D2\nDataset NumerSense CSQA CSQA CSQA2 QASC QASC\nInference Model T5-11b T5-11b UQA-11b-ft Unicorn-ft T5-11b UQA-11b-ft\ndev testcore testall dev dev dev test dev test dev test\nKnowledge Gen.\n(∅) Vanilla baseline 67.5 70.23 64.05 39.89 85.18 69.9 70.2† 48.16 44.89 81.75 76.74\n(R) Random sentences68.5 – – 21.79 85.42 70.37 – 49.35 – 82.18 –\n(C) Context sentences70.5 – – 42.51 85.34 70.92 – 55.83 – 82.61 –\n(T) Template-based – – – 45.37 – – – – – – –\n(IR) Retrieval-based – 70.41 65.10∗∗ – – 74.0 73.3†† 76.89 – 90.06 –\n(A) Answers 73.0 – – 51.84 84.93 69.22 – 52.48 – 81.53 –\n(K) Ours 78.0 79.24 72.47 47.26 85.34 72.37 73.03 58.32 55.00 84.02 80.33\nprev. SOTA (no IR) – 72.61 66.18∗ – 79.1 (test)# 69.9 70.2† – – 81.75 76.74‡\nFew-shot GPT-3 Infer.60.5 – – – 71.58 53.80 – – – 66.09 –\nTable 3: Experimental results of applying different knowledge generation methods on various tasks and inference\nmodels. T5-11b is the zero-shot inference model, whereas other inference models are ﬁnetuned based on T5-11b.\nWe bold the best and underline the second best numbers. Previous SOTA and retrieval-based methods are also\nbased on the inference model in their corresponding column: * T5-11b 1.1 +digits (Submission by ISI Waltham);\n** T5-11b + IR (Yan, 2021); # UQA-11b-ft (Khashabi et al., 2020) (SOTA of single-model methods without\nreferencing ConceptNet); †Unicorn-ft (Talmor et al., 2021);††Unicorn-ft + Google snippets (Talmor et al., 2021);\n‡UQA-11b-ft (Khashabi et al., 2020).\ndatasets we evaluate on, and works well under both\nzero-shot and ﬁnetuned settings. In particular, our\nknowledge generation outperforms naive baselines\nas well as template-based knowledge generation,\nand is on-par with retrieval-based systems.\n4.1 Overall Performance\nTable 3 shows the results on zero-shot and ﬁnetuned\nmodels following our task setups.\nNew state-of-the-art. We apply our method on\ntop of the same inference model used in the previ-\nous state-of-the-art. On NumerSense, we achieve a\n6% (66.18 →72.47) improvement over the previ-\nous best method based on the zero-shot T5 model.\nThe previous state-of-the-art among non-retrieval\nmethods on CSQA2 is based on the ﬁnetuned Uni-\ncorn model, upon which we improve by 2% (70.2\n→73.03). For QASC, the previous best is based\non the ﬁnetuned UniﬁedQA model, upon which we\nimprove by 3% (76.74 →80.33).\nZero-shot settings. Columns A, B1, and D1\nin Table 3 show that our method substantially\nimproves zero-shot inference models, by 7% to\n10% across NumerSense (64.05 →72.47), CSQA\n(39.89 →47.26), and QASC (44.89 →55.00).\nFinetuned settings. Columns B2, C, and D2 in\nTable 3 indicate that our method consistently im-\nproves upon the vanilla baseline set by ﬁnetuned\ninference models (though by smaller margins than\nin the zero-shot settings).\n4.2 Knowledge Generation Methods\nTable 3 reports the performance with different\nknowledge generation baselines. Generally, ran-\ndom sentences barely help and even hurt the in-\nference model, whereas context sentences of the\nquestion provide some gain. In contrast, knowl-\nedge generated by our method consistently leads\nto substantial performance improvements, which\nimplies that our knowledge is of high quality.\nKnowledge is an essential factor. The few-shot\nGPT-3 model is poorly calibrated to directly answer\ncommonsense questions, underperforming our best\nmodels by 14% to 20% across all tasks. Even\nwhen we use answers generated by few-shot GPT-3\nto prompt the SOTA inference models, this still\nsigniﬁcantly falls behind our method on almost\nall the tasks and models we consider (with one\nexception – CSQA with T5 inference). Through the\nmedium of knowledge, our method can effectively\nleverage useful information possessed by GPT-3\nto help improve even the SOTA models on various\ncommonsense reasoning tasks.\nOur knowledge outperform template generated\nknowledge. We compare our knowledge gener-\nation method with the template-based self-talk on\nthe CSQA dev set. (CSQA is the only task we\nexperiment with that has self-talk templates avail-\nable.) Our method leads to a larger improvement\nover the T5-11b baseline than self-talk (by 1.89%),\nshowing that it is better at eliciting helpful knowl-']","Our method outperforms retrieval-based methods in terms of downstream performance, showing that knowledge retrieved from a loosely-related knowledge base can be far less useful than our generated knowledge.","GKP performs comparably to retrieval-based methods, offering significant improvements when a structured knowledge base is unavailable.,",0.5,0.9077088220188446,0.8333333332916666,1.0,0.0,0.22141927983955995
"What datasets were used to evaluate GKP?,","['A group of people in a field with various kites in the sky.', 'many kites with different shapes all laying on the ground', 'People flying kites over a lake in a park ']",The context does not mention anything about evaluating GKP or using any specific datasets.,"GKP was evaluated on four commonsense reasoning datasets: NumerSense, CommonsenseQA, CommonsenseQA 2.0, and QASC.,",1.0,0.0,0.0,0.0,0.0,0.20847057559592022
"What advantages does GKP provide for zero-shot models?,","['a giraffe eating food from a food dispenser ', 'a pair of giraffe standing in a big open area', 'a giraffe staring right into the camera ']",The question is not related to the context provided.,"GKP improves zero-shot model performance by converting implicit commonsense reasoning into explicit reasoning procedures through generated knowledge.,",0.0,0.0,0.0,0.0,0.0,0.18213178869115645
"How does the size of the inference model affect GKP performance?,","['Figure 2: Performance with different number of gen-\nerated knowledge statements per question (QASC dev\nset, T5-11b inference model).\nIntegration method QASC-dev\nours 58.32\nMixture-of-Experts 56.26\nProduct-of-Experts 55.94\nTable 4: Performance with different knowledge integra-\ntion methods (QASC dev set, T5-11b inference model).\nedge from models.\nOur knowledge is comparable with retrieval-\nbased knowledge. On NumerSense, the re-\ntrieved knowledge only improves inference per-\nformance by 0.18% on test-core and 1.02% on\ntest-all, while our method further outperforms it\nby 8.83% and 7.37%, respectively. This shows\nthat knowledge retrieved from a loosely-related\nknowledge base can be far less useful than our\ngenerated knowledge. On CSQA2, although we\nare not able to beat the web-retrieved knowledge,\nour method still bridges the performance gap with-\nout referring to Google search. For QASC, the\n“retrieved” knowledge is actually gold knowledge\nfrom a knowledge base that was used to construct\nthe dataset. As a result, our generated knowledge\nfalls signiﬁcantly short of the retrieved knowledge.\nIn summary, our generated knowledge is roughly\ncomparable with retrieved knowledge in terms of\ndownstream performance, and is most valuable\nwhen there is no appropriate in-domain knowledge\nbase to retrieve from.\n4.3 Analysis\nBetter performance with more knowledge.\nWe analyze the impact of the number of generated\nknowledge statements, M, and show the results\nin Figure 2. Generally, the performance increases\nwith the quantity of knowledge statements. It satu-\nrates at M = 20and begins to decline when more\nknowledge statements are introduced, which may\nbe because more noisy knowledge is generated.\nFigure 3: Improvement on top of different sizes of in-\nference model (Numersense dev set).\nFigure 4: Improvement by different sizes of knowledge\ngeneration model (Numersense dev set, T5-11b infer-\nence model).\nThe knowledge integration method. In addi-\ntion to the knowledge integration method described\nin §2.2, we experiment with two alternatives:\nMixture-of-Experts (MoE) and Product-of-Experts\n(PoE) (Hinton, 2002). These make the following\nmodiﬁcations to Equation 1, respectively:\nMoE: pI(a|q,Kq) ∝\n∑\n0≤m≤M\npI(a|qm), (2)\nPoE: pI(a|q,Kq) ∝\n∏\n0≤m≤M\npI(a|qm). (3)\nThe results in Table 4 indicate that our knowledge\nintegration method – i.e. adaptively choosing the\nbest knowledge to rely on – is best among the three.\nLightweight inference models and ampliﬁca-\ntion. We found that the size of inference model\naffects the magnitude of improvement. Figure 3\nshows the NumerSense performance gain on top\nof different sizes of inference model. As we use\nsmaller inference models, the performance gain in-\ncreases drastically. In particular, with our method\nthe smallest T5 model is as powerful as the T5-3b\nbaseline, and T5-large outperforms the GPT-3 base-\nline. This indicates that model-generated knowl-\nedge can enable high performing, yet lightweight,\ninference models. Furthermore, the improvement\ndoes not diminish as the inference model becomes', 'Figure 5: Human evaluation of generated knowledge. Left: Percentage of good knowledge statements along each\naxis. Right: Agreement between human and machine on helpfulness of selected knowledge.\nas big as the knowledge generation model, as the\ninference by GPT-3 can beneﬁt by 9.0% from the\nknowledge elicited from itself. This indicates that\nour method can somewhatamplify the useful knowl-\nedge already possessed by the model, leading to\nbetter predictions.\nThe size of knowledge generation model. Fig-\nure 4 shows the NumerSense performance gain\nwhen using different sizes of GPT-3 as the knowl-\nedge generation model. On top of the T5-11b in-\nference model, The 6.7B knowledge model gives\na 5.0% improvement, narrower than the 10.5% im-\nprovement given by the 175B knowledge model.\nThe 1.3B and 0.4B knowledge models do not give\na signiﬁcant improvement. Therefore, we do not\nnecessarily need the largest version of GPT-3 as the\nknowledge source, though we do need the model to\nbe relatively large in order to generate useful and\nreliable knowledge.\n4.4 Human Evaluation\nWe conduct a human evaluation on NumerSense\nand QASC to study the quality of generated knowl-\nedge and the interpretability of its impact on task\nperformance.\nEvaluation. We report the quality of knowledge\nstatements along four axes: (1) Grammaticality:\nwhether it is grammatical; (2) Relevance: whether\nit is relevant to the topic or concepts mentioned on\nthe question; (3) Factuality: whether it is (mostly)\nfactually correct; and (4) Helpfulness: whether it\nhelps answering the question in an either direct or\nindirect way, and may fall into one of the three cat-\negories: helpful (i.e. supports the correct answer),\nharmful (i.e. negates the correct answer or supports\nan incorrect answer), or neutral (neither helpful nor\nharmful). These metrics are adapted from Shwartz\net al. (2020) and are deﬁned in Appendix A.3.\nFrom each dataset, we sample up to 50 selected\nknowledge (§2.2) that change the correctness of\nT5-11b’s prediction (i.e. rectiﬁes model prediction\nfrom wrong to right, or misleads model prediction\nfrom right to wrong). The knowledge are labeled\nby two NLP experts and a moderate level of agree-\nment was reached (Fleiss Kappa κ= 0.57 (Landis\nand Koch, 1977)). To ensure objectivity, it is not\nrevealed to the annotators whether the knowledge\nrectiﬁes or misleads the model prediction.\nResults. Figure 5 summarizes the results. The\nvast majority of selected knowledge are grammati-\ncal and relevant to the question, and 83% of them\nare factually correct. 72% are seen as being helpful\nfor answering the question according the human\nevaluators, whereas 13% are harmful. Out of the\nknowledge statements that rectify the model pre-\ndictions, 93% are labeled as helpful by the human\nevaluators; in contrast, when the knowledge state-\nment misleads the model, only 21% are labeled\nas helpful, and 39% harmful. Of the knowledge\ndeemed helpful by human and rectiﬁes model pre-\ndiction, 95% are factual, while of those deemed\nharmful by human and misleads model prediction,\n86% are non-factual, suggesting that improving\nknowledge factuality is a promising path towards\nmore helpful knowledge. We also analyzed the non-\nselected knowledge and found that these statements\nhave slightly lower factuality and helpfulness than\nthe selected knowledge.\n4.5 Qualitative Examples\nTable 5 shows a few examples where the gener-\nated knowledge rectiﬁes model prediction. Due to\nspace constraints we only show the selected knowl-\nedge (§2.2) for each question. In all examples,\nthe model without prompted knowledge assigns a\nhigher score to an incorrect answer than the cor-', 'A B 1 B2 C D 1 D2\nDataset NumerSense CSQA CSQA CSQA2 QASC QASC\nInference Model T5-11b T5-11b UQA-11b-ft Unicorn-ft T5-11b UQA-11b-ft\ndev testcore testall dev dev dev test dev test dev test\nKnowledge Gen.\n(∅) Vanilla baseline 67.5 70.23 64.05 39.89 85.18 69.9 70.2† 48.16 44.89 81.75 76.74\n(R) Random sentences68.5 – – 21.79 85.42 70.37 – 49.35 – 82.18 –\n(C) Context sentences70.5 – – 42.51 85.34 70.92 – 55.83 – 82.61 –\n(T) Template-based – – – 45.37 – – – – – – –\n(IR) Retrieval-based – 70.41 65.10∗∗ – – 74.0 73.3†† 76.89 – 90.06 –\n(A) Answers 73.0 – – 51.84 84.93 69.22 – 52.48 – 81.53 –\n(K) Ours 78.0 79.24 72.47 47.26 85.34 72.37 73.03 58.32 55.00 84.02 80.33\nprev. SOTA (no IR) – 72.61 66.18∗ – 79.1 (test)# 69.9 70.2† – – 81.75 76.74‡\nFew-shot GPT-3 Infer.60.5 – – – 71.58 53.80 – – – 66.09 –\nTable 3: Experimental results of applying different knowledge generation methods on various tasks and inference\nmodels. T5-11b is the zero-shot inference model, whereas other inference models are ﬁnetuned based on T5-11b.\nWe bold the best and underline the second best numbers. Previous SOTA and retrieval-based methods are also\nbased on the inference model in their corresponding column: * T5-11b 1.1 +digits (Submission by ISI Waltham);\n** T5-11b + IR (Yan, 2021); # UQA-11b-ft (Khashabi et al., 2020) (SOTA of single-model methods without\nreferencing ConceptNet); †Unicorn-ft (Talmor et al., 2021);††Unicorn-ft + Google snippets (Talmor et al., 2021);\n‡UQA-11b-ft (Khashabi et al., 2020).\ndatasets we evaluate on, and works well under both\nzero-shot and ﬁnetuned settings. In particular, our\nknowledge generation outperforms naive baselines\nas well as template-based knowledge generation,\nand is on-par with retrieval-based systems.\n4.1 Overall Performance\nTable 3 shows the results on zero-shot and ﬁnetuned\nmodels following our task setups.\nNew state-of-the-art. We apply our method on\ntop of the same inference model used in the previ-\nous state-of-the-art. On NumerSense, we achieve a\n6% (66.18 →72.47) improvement over the previ-\nous best method based on the zero-shot T5 model.\nThe previous state-of-the-art among non-retrieval\nmethods on CSQA2 is based on the ﬁnetuned Uni-\ncorn model, upon which we improve by 2% (70.2\n→73.03). For QASC, the previous best is based\non the ﬁnetuned UniﬁedQA model, upon which we\nimprove by 3% (76.74 →80.33).\nZero-shot settings. Columns A, B1, and D1\nin Table 3 show that our method substantially\nimproves zero-shot inference models, by 7% to\n10% across NumerSense (64.05 →72.47), CSQA\n(39.89 →47.26), and QASC (44.89 →55.00).\nFinetuned settings. Columns B2, C, and D2 in\nTable 3 indicate that our method consistently im-\nproves upon the vanilla baseline set by ﬁnetuned\ninference models (though by smaller margins than\nin the zero-shot settings).\n4.2 Knowledge Generation Methods\nTable 3 reports the performance with different\nknowledge generation baselines. Generally, ran-\ndom sentences barely help and even hurt the in-\nference model, whereas context sentences of the\nquestion provide some gain. In contrast, knowl-\nedge generated by our method consistently leads\nto substantial performance improvements, which\nimplies that our knowledge is of high quality.\nKnowledge is an essential factor. The few-shot\nGPT-3 model is poorly calibrated to directly answer\ncommonsense questions, underperforming our best\nmodels by 14% to 20% across all tasks. Even\nwhen we use answers generated by few-shot GPT-3\nto prompt the SOTA inference models, this still\nsigniﬁcantly falls behind our method on almost\nall the tasks and models we consider (with one\nexception – CSQA with T5 inference). Through the\nmedium of knowledge, our method can effectively\nleverage useful information possessed by GPT-3\nto help improve even the SOTA models on various\ncommonsense reasoning tasks.\nOur knowledge outperform template generated\nknowledge. We compare our knowledge gener-\nation method with the template-based self-talk on\nthe CSQA dev set. (CSQA is the only task we\nexperiment with that has self-talk templates avail-\nable.) Our method leads to a larger improvement\nover the T5-11b baseline than self-talk (by 1.89%),\nshowing that it is better at eliciting helpful knowl-']","The size of the inference model affects the magnitude of improvement. As shown in Figure 3, smaller inference models lead to a drastic increase in performance gain. For example, with the method used, the smallest T5 model performs as well as the T5-3b baseline, and T5-large outperforms the GPT-3 baseline. This indicates that model-generated knowledge can enable high-performing, lightweight inference models. Additionally, the improvement does not decrease as the inference model becomes larger.","The performance gain from GKP is more pronounced for smaller inference models, but even large models benefit, demonstrating that GKP effectively amplifies knowledge utility.,",1.0,0.9395268924612669,0.9999999999666667,1.0,0.0,0.6321366301462178
"What role do few-shot demonstrations play in GKP?,","['A presenter projected on a large screen at a conference.', 'A group of people looking at some kind of show or exhibit ', 'A group of people collect around a giraffe head. ']","Few-shot demonstrations play a role in providing quick and concise examples or explanations to the audience, similar to how a presenter may use visual aids or demonstrations to convey information at a conference.","Few-shot demonstrations guide the language model to generate relevant and helpful knowledge statements for answering questions.,",0.3333333333333333,0.9234580584416965,0.0,0.0,0.0,0.2149208688334252
"How does GKP handle factuality in generated knowledge?,","['Figure 5: Human evaluation of generated knowledge. Left: Percentage of good knowledge statements along each\naxis. Right: Agreement between human and machine on helpfulness of selected knowledge.\nas big as the knowledge generation model, as the\ninference by GPT-3 can beneﬁt by 9.0% from the\nknowledge elicited from itself. This indicates that\nour method can somewhatamplify the useful knowl-\nedge already possessed by the model, leading to\nbetter predictions.\nThe size of knowledge generation model. Fig-\nure 4 shows the NumerSense performance gain\nwhen using different sizes of GPT-3 as the knowl-\nedge generation model. On top of the T5-11b in-\nference model, The 6.7B knowledge model gives\na 5.0% improvement, narrower than the 10.5% im-\nprovement given by the 175B knowledge model.\nThe 1.3B and 0.4B knowledge models do not give\na signiﬁcant improvement. Therefore, we do not\nnecessarily need the largest version of GPT-3 as the\nknowledge source, though we do need the model to\nbe relatively large in order to generate useful and\nreliable knowledge.\n4.4 Human Evaluation\nWe conduct a human evaluation on NumerSense\nand QASC to study the quality of generated knowl-\nedge and the interpretability of its impact on task\nperformance.\nEvaluation. We report the quality of knowledge\nstatements along four axes: (1) Grammaticality:\nwhether it is grammatical; (2) Relevance: whether\nit is relevant to the topic or concepts mentioned on\nthe question; (3) Factuality: whether it is (mostly)\nfactually correct; and (4) Helpfulness: whether it\nhelps answering the question in an either direct or\nindirect way, and may fall into one of the three cat-\negories: helpful (i.e. supports the correct answer),\nharmful (i.e. negates the correct answer or supports\nan incorrect answer), or neutral (neither helpful nor\nharmful). These metrics are adapted from Shwartz\net al. (2020) and are deﬁned in Appendix A.3.\nFrom each dataset, we sample up to 50 selected\nknowledge (§2.2) that change the correctness of\nT5-11b’s prediction (i.e. rectiﬁes model prediction\nfrom wrong to right, or misleads model prediction\nfrom right to wrong). The knowledge are labeled\nby two NLP experts and a moderate level of agree-\nment was reached (Fleiss Kappa κ= 0.57 (Landis\nand Koch, 1977)). To ensure objectivity, it is not\nrevealed to the annotators whether the knowledge\nrectiﬁes or misleads the model prediction.\nResults. Figure 5 summarizes the results. The\nvast majority of selected knowledge are grammati-\ncal and relevant to the question, and 83% of them\nare factually correct. 72% are seen as being helpful\nfor answering the question according the human\nevaluators, whereas 13% are harmful. Out of the\nknowledge statements that rectify the model pre-\ndictions, 93% are labeled as helpful by the human\nevaluators; in contrast, when the knowledge state-\nment misleads the model, only 21% are labeled\nas helpful, and 39% harmful. Of the knowledge\ndeemed helpful by human and rectiﬁes model pre-\ndiction, 95% are factual, while of those deemed\nharmful by human and misleads model prediction,\n86% are non-factual, suggesting that improving\nknowledge factuality is a promising path towards\nmore helpful knowledge. We also analyzed the non-\nselected knowledge and found that these statements\nhave slightly lower factuality and helpfulness than\nthe selected knowledge.\n4.5 Qualitative Examples\nTable 5 shows a few examples where the gener-\nated knowledge rectiﬁes model prediction. Due to\nspace constraints we only show the selected knowl-\nedge (§2.2) for each question. In all examples,\nthe model without prompted knowledge assigns a\nhigher score to an incorrect answer than the cor-', 'Task NumerSense QASC\nPrompt Generate some numerical facts about objects. Examples: Generate some knowledge about the input. Examples:\nInput:penguins have <mask> wings. Input:What type of water formation is formed by clouds?\nKnowledge:Birds have two wings. Penguin is a kind of bird.Knowledge:Clouds are made of water vapor.\n... ...\nInput:a typical human being has <mask> limbs. Input:The process by which genes are passed is\nKnowledge:Human has two arms and two legs. Knowledge:Genes are passed from parent to offspring.\nInput:{question} Input:{question}\nKnowledge: Knowledge:\nTable 2: Prompts for knowledge generation for two of our tasks, NumerSense and QASC. The prompt consists of\nan instruction, ﬁve demonstrations of question-knowledge pairs, and a new question placeholder. For full prompts\non all the tasks we evaluate on, see Appendix A.2.\nchallenges posed by the task (e.g. numerical com-\nmonsense, scientiﬁc commonsense). We pair each\nquestion with a knowledge statement that turns the\ncommonsense problem posed by the question into\nan explicit reasoning procedure, without directly\nanswering the question. For example, the knowl-\nedge statement Birds have two wings. Penguin is a\nkind of bird. is helpful for the question Penguins\nhave <mask> wings, because it turns the problem\ninto deductive reasoning. Meanwhile, Penguins\nhave two wings. would be a poor knowledge state-\nment to demonstrate according to our guideline.\nWhen generating knowledge for a new question\nq, we plug the question into the placeholder, and\nrepeatedly sample generated continuations of this\nprompt to obtain a set of knowledge statements\nKq = {k1,k2,...,k M }. For full prompts on all\nthe tasks we evaluate on, see Appendix A.2.\n2.2 Knowledge Integration via Prompting\nIn the knowledge integration step, we use a lan-\nguage model – called the inference model – to\nmake predictions with each generated knowledge\nstatement, then select the highest-conﬁdence pre-\ndiction. Speciﬁcally, we use each knowledge state-\nment to prompt the model, forming M knowledge-\naugmented questions:\nq0 = q,q1 = [k1||q],...,q M = [kM ||q],\nwhere [·||·] denotes text concatenation.\nWe compute an aggregated score for each answer\nchoice ausing the augmented question that best\nsupports it under the inference model:\npI(a|q,Kq) ∝ max\n0≤m≤M\npI(a|qm). (1)\nIntuitively, this favors knowledge statements that\nstrongly support one of the choices.\nThe predicted answer is then,\nˆa= arg max\na∈Aq\nmax\n0≤m≤M\npI(a|qm),\nwhich is the choice that gets most support from one\nof the knowledge statements. This prediction uses\na single knowledge statement, which we refer to as\nthe selected knowledge:\nˆk= kˆm where ˆm= arg max\n0≤m≤M\nmax\na∈Aq\npI(a|qm).\nThe inference model may be any existing lan-\nguage model taken off-the-shelf (i.e. zero-shot) or\nﬁnetuned on the task. We do not do any further\nﬁnetuning with knowledge prompting.\n3 Experimental Setup\nHere, we describe the implementation details of\nour method and how they are adapted to each task.\nFor knowledge generation, we use GPT-3\n(Brown et al., 2020) as the underlying language\nmodel, where our few-shot prompting method is\nmost effective. We generate M = 20knowledge\nstatements for each question with nucleus sampling\np= 0.5 (Holtzman et al., 2019), and discard repe-\ntitions and empty strings. Generation is terminated\nwhen it exceeds 64 tokens or hits the \\n token.1\nFor inference, we use off-the-shelf T5 (Raffel\net al., 2019) and GPT-3, as well as ﬁnetuned models\nthat are state-of-the-art on each dataset, including\nUniﬁedQA (UQA) (Khashabi et al., 2020) and Uni-\ncorn (Lourie et al., 2021). See details in the task\nsetup below.\n3.1 Datasets and Task Setup\nWe evaluate our method on four commonsense rea-\nsoning datasets which cover a variety of challenges\nand problem formats.\n1An exception is with the CSQA2 dataset, where for the\nbest results we choose M = 5and allow for up to 128 tokens\nin each generation.', 'Generated Knowledge Prompting for Commonsense Reasoning\nJiacheng Liu♥ Alisa Liu♥ Ximing Lu♥♠ Sean Welleck♥♠\nPeter West♥♠ Ronan Le Bras♠ Yejin Choi♥♠ Hannaneh Hajishirzi♥♠\n♥Paul G. Allen School of Computer Science & Engineering, University of Washington\n♠Allen Institute for Artiﬁcial Intelligence\nliujc@cs.washington.edu\nAbstract\nIt remains an open question whether incorpo-\nrating external knowledge beneﬁts common-\nsense reasoning while maintaining the ﬂexi-\nbility of pretrained sequence models. To in-\nvestigate this question, we develop generated\nknowledge prompting, which consists of gen-\nerating knowledge from a language model,\nthen providing the knowledge as additional in-\nput when answering a question. Our method\ndoes not require task-speciﬁc supervision for\nknowledge integration, or access to a struc-\ntured knowledge base, yet it improves perfor-\nmance of large-scale, state-of-the-art models\non four commonsense reasoning tasks, achiev-\ning state-of-the-art results on numerical com-\nmonsense (NumerSense), general common-\nsense (CommonsenseQA 2.0), and scientiﬁc\ncommonsense (QASC) benchmarks. Gener-\nated knowledge prompting highlights large-\nscale language models as ﬂexible sources of\nexternal knowledge for improving common-\nsense reasoning. Our code is available at\ngithub.com/liujch1998/GKP\n1 Introduction\nIt remains an open research question whether exter-\nnal knowledge is needed for commonsense reason-\ning. On one hand, a substantial body of prior work\nhas reported that integrating external knowledge\ncan help improve task performance (Mitra et al.,\n2019; Bian et al., 2021, inter alia), especially if the\nknowledge is high quality (e.g. hand-crafted by ex-\nperts). On the other hand, recent leaderboards are\noften dominated by large-scale pretrained models\nthat are ﬁne-tuned on a target benchmark (Khashabi\net al., 2020; Lourie et al., 2021), suggesting that\nthe beneﬁts of external knowledge may wash away\nas the underlying models increase in size and are\npretrained on ever larger amounts of raw text.\nEven if external knowledge is found to be ef-\nfective on a particular task, ﬂexibility remains a\nfundamental hurdle to integrating external knowl-\nFigure 1: Generated knowledge prompting involves\n(i) using few-shot demonstrations to generate question-\nrelated knowledge statements from a language model;\n(ii) using a second language model to make predic-\ntions with each knowledge statement, then selecting the\nhighest-conﬁdence prediction.\nedge, as many benchmarks currently lack appropri-\nate knowledge bases with sufﬁcient coverage. Fur-\nthermore, prior methods often require task-speciﬁc,\ncustom supervision for knowledge integration (Mi-\ntra et al., 2019; Chang et al., 2020), introducing a\nburden for rapidly adapting new pretrained models\nto a wide variety of tasks.\nIn this paper, we investigate whether external\nknowledge can be helpful for commonsense rea-\nsoning, even on top of the largest state-of-the-art\npretrained models (e.g. T5-11b (Raffel et al., 2019)\nand its variants), with a focus on four recent com-\nmonsense benchmarks. To facilitate easier adap-\ntation with any zero-shot or ﬁnetuned models, we\npropose an approach that does not require access\nto a structured knowledge base or joint ﬁnetuning\nfor knowledge integration.\nThe key insight behind our method, Generated\nKnowledge Prompting (sketched in Figure 1), is\nthat we can generate useful knowledge from a lan-\nguage model, then provide the knowledge as an in-\nput prompt that is concatenated with a question. To\narXiv:2110.08387v3  [cs.CL]  28 Sep 2022']","The human evaluation of generated knowledge in the context focuses on four axes: Grammaticality, Relevance, Factuality, and Helpfulness. The knowledge statements are evaluated based on these criteria. The majority of selected knowledge statements are grammatical, relevant, and factually correct. 72% of them are considered helpful for answering the question, while 13% are harmful. When the knowledge rectifies the model predictions, 93% are labeled as helpful, but when it misleads the model, only 21% are helpful and 39% are harmful. The helpful knowledge statements are mostly factual, while the harmful ones are non-factual.","Human evaluation indicates that improving the factuality of generated knowledge can further enhance its helpfulness and reduce the potential for harmful reasoning.,",1.0,0.8372216208505603,0.9999999999666667,1.0,0.0,0.22447454660294475
"What are the key factors influencing the performance of GKP?,","['Figure 2: Performance with different number of gen-\nerated knowledge statements per question (QASC dev\nset, T5-11b inference model).\nIntegration method QASC-dev\nours 58.32\nMixture-of-Experts 56.26\nProduct-of-Experts 55.94\nTable 4: Performance with different knowledge integra-\ntion methods (QASC dev set, T5-11b inference model).\nedge from models.\nOur knowledge is comparable with retrieval-\nbased knowledge. On NumerSense, the re-\ntrieved knowledge only improves inference per-\nformance by 0.18% on test-core and 1.02% on\ntest-all, while our method further outperforms it\nby 8.83% and 7.37%, respectively. This shows\nthat knowledge retrieved from a loosely-related\nknowledge base can be far less useful than our\ngenerated knowledge. On CSQA2, although we\nare not able to beat the web-retrieved knowledge,\nour method still bridges the performance gap with-\nout referring to Google search. For QASC, the\n“retrieved” knowledge is actually gold knowledge\nfrom a knowledge base that was used to construct\nthe dataset. As a result, our generated knowledge\nfalls signiﬁcantly short of the retrieved knowledge.\nIn summary, our generated knowledge is roughly\ncomparable with retrieved knowledge in terms of\ndownstream performance, and is most valuable\nwhen there is no appropriate in-domain knowledge\nbase to retrieve from.\n4.3 Analysis\nBetter performance with more knowledge.\nWe analyze the impact of the number of generated\nknowledge statements, M, and show the results\nin Figure 2. Generally, the performance increases\nwith the quantity of knowledge statements. It satu-\nrates at M = 20and begins to decline when more\nknowledge statements are introduced, which may\nbe because more noisy knowledge is generated.\nFigure 3: Improvement on top of different sizes of in-\nference model (Numersense dev set).\nFigure 4: Improvement by different sizes of knowledge\ngeneration model (Numersense dev set, T5-11b infer-\nence model).\nThe knowledge integration method. In addi-\ntion to the knowledge integration method described\nin §2.2, we experiment with two alternatives:\nMixture-of-Experts (MoE) and Product-of-Experts\n(PoE) (Hinton, 2002). These make the following\nmodiﬁcations to Equation 1, respectively:\nMoE: pI(a|q,Kq) ∝\n∑\n0≤m≤M\npI(a|qm), (2)\nPoE: pI(a|q,Kq) ∝\n∏\n0≤m≤M\npI(a|qm). (3)\nThe results in Table 4 indicate that our knowledge\nintegration method – i.e. adaptively choosing the\nbest knowledge to rely on – is best among the three.\nLightweight inference models and ampliﬁca-\ntion. We found that the size of inference model\naffects the magnitude of improvement. Figure 3\nshows the NumerSense performance gain on top\nof different sizes of inference model. As we use\nsmaller inference models, the performance gain in-\ncreases drastically. In particular, with our method\nthe smallest T5 model is as powerful as the T5-3b\nbaseline, and T5-large outperforms the GPT-3 base-\nline. This indicates that model-generated knowl-\nedge can enable high performing, yet lightweight,\ninference models. Furthermore, the improvement\ndoes not diminish as the inference model becomes', 'Figure 5: Human evaluation of generated knowledge. Left: Percentage of good knowledge statements along each\naxis. Right: Agreement between human and machine on helpfulness of selected knowledge.\nas big as the knowledge generation model, as the\ninference by GPT-3 can beneﬁt by 9.0% from the\nknowledge elicited from itself. This indicates that\nour method can somewhatamplify the useful knowl-\nedge already possessed by the model, leading to\nbetter predictions.\nThe size of knowledge generation model. Fig-\nure 4 shows the NumerSense performance gain\nwhen using different sizes of GPT-3 as the knowl-\nedge generation model. On top of the T5-11b in-\nference model, The 6.7B knowledge model gives\na 5.0% improvement, narrower than the 10.5% im-\nprovement given by the 175B knowledge model.\nThe 1.3B and 0.4B knowledge models do not give\na signiﬁcant improvement. Therefore, we do not\nnecessarily need the largest version of GPT-3 as the\nknowledge source, though we do need the model to\nbe relatively large in order to generate useful and\nreliable knowledge.\n4.4 Human Evaluation\nWe conduct a human evaluation on NumerSense\nand QASC to study the quality of generated knowl-\nedge and the interpretability of its impact on task\nperformance.\nEvaluation. We report the quality of knowledge\nstatements along four axes: (1) Grammaticality:\nwhether it is grammatical; (2) Relevance: whether\nit is relevant to the topic or concepts mentioned on\nthe question; (3) Factuality: whether it is (mostly)\nfactually correct; and (4) Helpfulness: whether it\nhelps answering the question in an either direct or\nindirect way, and may fall into one of the three cat-\negories: helpful (i.e. supports the correct answer),\nharmful (i.e. negates the correct answer or supports\nan incorrect answer), or neutral (neither helpful nor\nharmful). These metrics are adapted from Shwartz\net al. (2020) and are deﬁned in Appendix A.3.\nFrom each dataset, we sample up to 50 selected\nknowledge (§2.2) that change the correctness of\nT5-11b’s prediction (i.e. rectiﬁes model prediction\nfrom wrong to right, or misleads model prediction\nfrom right to wrong). The knowledge are labeled\nby two NLP experts and a moderate level of agree-\nment was reached (Fleiss Kappa κ= 0.57 (Landis\nand Koch, 1977)). To ensure objectivity, it is not\nrevealed to the annotators whether the knowledge\nrectiﬁes or misleads the model prediction.\nResults. Figure 5 summarizes the results. The\nvast majority of selected knowledge are grammati-\ncal and relevant to the question, and 83% of them\nare factually correct. 72% are seen as being helpful\nfor answering the question according the human\nevaluators, whereas 13% are harmful. Out of the\nknowledge statements that rectify the model pre-\ndictions, 93% are labeled as helpful by the human\nevaluators; in contrast, when the knowledge state-\nment misleads the model, only 21% are labeled\nas helpful, and 39% harmful. Of the knowledge\ndeemed helpful by human and rectiﬁes model pre-\ndiction, 95% are factual, while of those deemed\nharmful by human and misleads model prediction,\n86% are non-factual, suggesting that improving\nknowledge factuality is a promising path towards\nmore helpful knowledge. We also analyzed the non-\nselected knowledge and found that these statements\nhave slightly lower factuality and helpfulness than\nthe selected knowledge.\n4.5 Qualitative Examples\nTable 5 shows a few examples where the gener-\nated knowledge rectiﬁes model prediction. Due to\nspace constraints we only show the selected knowl-\nedge (§2.2) for each question. In all examples,\nthe model without prompted knowledge assigns a\nhigher score to an incorrect answer than the cor-', '1 Introduction\nMath Word Problems (GSM8K)\n0\n20\n40\n60\n80\n100\n33\n55\n18\n57\nSolve rate (%)\nFinetuned GPT-3 175B\nPrior best\nPaLM 540B: standard prompting\nPaLM 540B: chain-of-thought prompting\nFigure 2: PaLM 540B uses chain-of-\nthought prompting to achieve new state-\nof-the-art performance on the GSM8K\nbenchmark of math word problems.\nFinetuned GPT-3 and prior best are from\nCobbe et al. (2021).\nThe NLP landscape has recently been revolutionized by\nlanguage models (Peters et al., 2018; Devlin et al., 2019;\nBrown et al., 2020, inter alia). Scaling up the size of lan-\nguage models has been shown to confer a range of beneﬁts,\nsuch as improved performance and sample efﬁciency (Ka-\nplan et al., 2020; Brown et al., 2020, inter alia). However,\nscaling up model size alone has not proved sufﬁcient for\nachieving high performance on challenging tasks such as\narithmetic, commonsense, and symbolic reasoning (Rae\net al., 2021).\nThis work explores how the reasoning ability of large\nlanguage models can be unlocked by a simple method\nmotivated by two ideas. First, techniques for arithmetic\nreasoning can beneﬁt from generating natural language\nrationales that lead to the ﬁnal answer. Prior work has\ngiven models the ability to generate natural language inter-\nmediate steps by training from scratch (Ling et al., 2017)\nor ﬁnetuning a pretrained model (Cobbe et al., 2021), in\naddition to neuro-symbolic methods that use formal lan-\nguages instead of natural language (Roy and Roth, 2015;\nChiang and Chen, 2019; Amini et al., 2019; Chen et al.,\n2019). Second, large language models offer the exciting\nprospect of in-context few-shot learning via prompting. That is, instead of ﬁnetuning a separate\nlanguage model checkpoint for each new task, one can simply “prompt” the model with a few\ninput–output exemplars demonstrating the task. Remarkably, this has been successful for a range of\nsimple question-answering tasks (Brown et al., 2020).\nBoth of the above ideas, however, have key limitations. For rationale-augmented training and\nﬁnetuning methods, it is costly to create a large set of high quality rationales, which is much more\ncomplicated than simple input–output pairs used in normal machine learning. For the traditional few-\nshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning\nabilities, and often does not improve substantially with increasing language model scale (Rae et al.,\n2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations.\nSpeciﬁcally, we explore the ability of language models to perform few-shot prompting for reasoning\ntasks, given a prompt that consists of triples: ⟨input, chain of thought, output⟩. A chain of thought is\na series of intermediate natural language reasoning steps that lead to the ﬁnal output, and we refer to\nthis approach as chain-of-thought prompting. An example prompt is shown in Figure 1.\nWe present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks,\nshowing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking\ndegree. Figure 2 illustrates one such result—on the GSM8K benchmark of math word problems\n(Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting\nby a large margin and achieves new state-of-the-art performance. A prompting only approach is\nimportant because it does not require a large training dataset and because a single model checkpoint\ncan perform many tasks without loss of generality. This work underscores how large language models\ncan learn via a few examples with natural language data about the task (c.f. automatically learning\nthe patterns underlying inputs and outputs via a large training dataset).\n2 Chain-of-Thought Prompting\nConsider one’s own thought process when solving a complicated reasoning task such as a multi-step\nmath word problem. It is typical to decompose the problem into intermediate steps and solve each\nbefore giving the ﬁnal answer: “After Jane gives 2 ﬂowers to her mom she has 10 ... then after she\ngives 3 to her dad she will have 7 ... so the answer is 7. ”The goal of this paper is to endow language\nmodels with the ability to generate a similar chain of thought—a coherent series of intermediate\nreasoning steps that lead to the ﬁnal answer for a problem. We will show that sufﬁciently large\n2']","The key factors influencing the performance of GKP are the number of generated knowledge statements, the knowledge integration method, the size of the inference model, and the size of the knowledge generation model.","GKP's performance depends on the quality of the generated knowledge, the quantity of knowledge statements, and the integration strategy during inference.,",1.0,0.9920711704461693,0.9999999999,1.0,0.0,0.6671318524094837
"Can GKP integrate with fine-tuned models?,","['a close up of a cat on a desk near a keyboard', 'Cat in front of a laptop looking up from the screen', 'A cat looking like it is using a laptop ']","Based on the context provided, there is no information related to GKP or fine-tuned models, so it is not possible to answer the question.","Yes, GKP can enhance the performance of fine-tuned models without requiring additional fine-tuning for knowledge integration.,",1.0,0.0,0.0,0.0,0.0,0.21246145697321345
"What is the main difference between GKP and template-based methods like self-talk?,","['A man in a gray shirt holds his phone as he sits by a man in a white t-shirt.', 'a woman writing something down on paper while the laptop sits on the table ', 'a person sitting on a side walk talking on a cell phone ']","The main difference between GKP and template-based methods like self-talk is that GKP involves a man in a gray shirt holding his phone while sitting next to a man in a white t-shirt, whereas template-based methods like self-talk involve a person sitting on a sidewalk talking on a cell phone.","Unlike template-based methods, GKP uses flexible few-shot prompting to generate diverse knowledge statements, making it easier to transfer across tasks.,",0.5,0.9919673627870137,0.0,0.0,0.0,0.21571170443505236
"How does GKP influence reasoning types in commonsense tasks?,","['some kids playing with some large checkerboards painted on the ground ', 'Young children playing game on wooden deck surface.', 'Someone in orange pants stands on a wooden bench.']",There is no mention of GKP or reasoning types in the context provided.,"GKP facilitates reasoning types like deduction, induction, negation, and numerical reasoning by providing explicit knowledge relevant to the task.,",1.0,0.8950451524048809,0.0,0.0,0.0,0.2121665536093214
"What is the impact of knowledge quantity on GKP performance?,","['Figure 2: Performance with different number of gen-\nerated knowledge statements per question (QASC dev\nset, T5-11b inference model).\nIntegration method QASC-dev\nours 58.32\nMixture-of-Experts 56.26\nProduct-of-Experts 55.94\nTable 4: Performance with different knowledge integra-\ntion methods (QASC dev set, T5-11b inference model).\nedge from models.\nOur knowledge is comparable with retrieval-\nbased knowledge. On NumerSense, the re-\ntrieved knowledge only improves inference per-\nformance by 0.18% on test-core and 1.02% on\ntest-all, while our method further outperforms it\nby 8.83% and 7.37%, respectively. This shows\nthat knowledge retrieved from a loosely-related\nknowledge base can be far less useful than our\ngenerated knowledge. On CSQA2, although we\nare not able to beat the web-retrieved knowledge,\nour method still bridges the performance gap with-\nout referring to Google search. For QASC, the\n“retrieved” knowledge is actually gold knowledge\nfrom a knowledge base that was used to construct\nthe dataset. As a result, our generated knowledge\nfalls signiﬁcantly short of the retrieved knowledge.\nIn summary, our generated knowledge is roughly\ncomparable with retrieved knowledge in terms of\ndownstream performance, and is most valuable\nwhen there is no appropriate in-domain knowledge\nbase to retrieve from.\n4.3 Analysis\nBetter performance with more knowledge.\nWe analyze the impact of the number of generated\nknowledge statements, M, and show the results\nin Figure 2. Generally, the performance increases\nwith the quantity of knowledge statements. It satu-\nrates at M = 20and begins to decline when more\nknowledge statements are introduced, which may\nbe because more noisy knowledge is generated.\nFigure 3: Improvement on top of different sizes of in-\nference model (Numersense dev set).\nFigure 4: Improvement by different sizes of knowledge\ngeneration model (Numersense dev set, T5-11b infer-\nence model).\nThe knowledge integration method. In addi-\ntion to the knowledge integration method described\nin §2.2, we experiment with two alternatives:\nMixture-of-Experts (MoE) and Product-of-Experts\n(PoE) (Hinton, 2002). These make the following\nmodiﬁcations to Equation 1, respectively:\nMoE: pI(a|q,Kq) ∝\n∑\n0≤m≤M\npI(a|qm), (2)\nPoE: pI(a|q,Kq) ∝\n∏\n0≤m≤M\npI(a|qm). (3)\nThe results in Table 4 indicate that our knowledge\nintegration method – i.e. adaptively choosing the\nbest knowledge to rely on – is best among the three.\nLightweight inference models and ampliﬁca-\ntion. We found that the size of inference model\naffects the magnitude of improvement. Figure 3\nshows the NumerSense performance gain on top\nof different sizes of inference model. As we use\nsmaller inference models, the performance gain in-\ncreases drastically. In particular, with our method\nthe smallest T5 model is as powerful as the T5-3b\nbaseline, and T5-large outperforms the GPT-3 base-\nline. This indicates that model-generated knowl-\nedge can enable high performing, yet lightweight,\ninference models. Furthermore, the improvement\ndoes not diminish as the inference model becomes', 'Figure 5: Human evaluation of generated knowledge. Left: Percentage of good knowledge statements along each\naxis. Right: Agreement between human and machine on helpfulness of selected knowledge.\nas big as the knowledge generation model, as the\ninference by GPT-3 can beneﬁt by 9.0% from the\nknowledge elicited from itself. This indicates that\nour method can somewhatamplify the useful knowl-\nedge already possessed by the model, leading to\nbetter predictions.\nThe size of knowledge generation model. Fig-\nure 4 shows the NumerSense performance gain\nwhen using different sizes of GPT-3 as the knowl-\nedge generation model. On top of the T5-11b in-\nference model, The 6.7B knowledge model gives\na 5.0% improvement, narrower than the 10.5% im-\nprovement given by the 175B knowledge model.\nThe 1.3B and 0.4B knowledge models do not give\na signiﬁcant improvement. Therefore, we do not\nnecessarily need the largest version of GPT-3 as the\nknowledge source, though we do need the model to\nbe relatively large in order to generate useful and\nreliable knowledge.\n4.4 Human Evaluation\nWe conduct a human evaluation on NumerSense\nand QASC to study the quality of generated knowl-\nedge and the interpretability of its impact on task\nperformance.\nEvaluation. We report the quality of knowledge\nstatements along four axes: (1) Grammaticality:\nwhether it is grammatical; (2) Relevance: whether\nit is relevant to the topic or concepts mentioned on\nthe question; (3) Factuality: whether it is (mostly)\nfactually correct; and (4) Helpfulness: whether it\nhelps answering the question in an either direct or\nindirect way, and may fall into one of the three cat-\negories: helpful (i.e. supports the correct answer),\nharmful (i.e. negates the correct answer or supports\nan incorrect answer), or neutral (neither helpful nor\nharmful). These metrics are adapted from Shwartz\net al. (2020) and are deﬁned in Appendix A.3.\nFrom each dataset, we sample up to 50 selected\nknowledge (§2.2) that change the correctness of\nT5-11b’s prediction (i.e. rectiﬁes model prediction\nfrom wrong to right, or misleads model prediction\nfrom right to wrong). The knowledge are labeled\nby two NLP experts and a moderate level of agree-\nment was reached (Fleiss Kappa κ= 0.57 (Landis\nand Koch, 1977)). To ensure objectivity, it is not\nrevealed to the annotators whether the knowledge\nrectiﬁes or misleads the model prediction.\nResults. Figure 5 summarizes the results. The\nvast majority of selected knowledge are grammati-\ncal and relevant to the question, and 83% of them\nare factually correct. 72% are seen as being helpful\nfor answering the question according the human\nevaluators, whereas 13% are harmful. Out of the\nknowledge statements that rectify the model pre-\ndictions, 93% are labeled as helpful by the human\nevaluators; in contrast, when the knowledge state-\nment misleads the model, only 21% are labeled\nas helpful, and 39% harmful. Of the knowledge\ndeemed helpful by human and rectiﬁes model pre-\ndiction, 95% are factual, while of those deemed\nharmful by human and misleads model prediction,\n86% are non-factual, suggesting that improving\nknowledge factuality is a promising path towards\nmore helpful knowledge. We also analyzed the non-\nselected knowledge and found that these statements\nhave slightly lower factuality and helpfulness than\nthe selected knowledge.\n4.5 Qualitative Examples\nTable 5 shows a few examples where the gener-\nated knowledge rectiﬁes model prediction. Due to\nspace constraints we only show the selected knowl-\nedge (§2.2) for each question. In all examples,\nthe model without prompted knowledge assigns a\nhigher score to an incorrect answer than the cor-', 'Task Prompt\nCSQA2 Generate some knowledge about the input. Examples:\nInput: Greece is larger than mexico.\nKnowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375\nsq km, making Mexico 1,389% larger than Greece.\nInput: Glasses always fog up.\nKnowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath,\nand ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid,\nforming a ﬁlm that you see as fog. Your lenses will be relatively cool compared to your breath,\nespecially when the outside air is cold.\nInput: A ﬁsh is capable of thinking.\nKnowledge: Fish are more intelligent than they appear. In many areas, such as memory, their\ncognitive powers match or exceed those of ’higher’ vertebrates including non-human primates.\nFish’s long-term memories help them keep track of complex social relationships.\nInput: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than\nnormal chance of getting lung cancer.\nKnowledge: Those who consistently averaged less than one cigarette per day over their lifetime\nhad nine times the risk of dying from lung cancer than never smokers. Among people who smoked\nbetween one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times\nhigher than that of never smokers.\nInput: A rock is the same size as a pebble.\nKnowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the\nUdden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules\n(2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\nInput: {question}\nKnowledge:\nTable 9: Prompt for knowledge generation on CSQA2. Demonstration examples are selected from the CSQA2\ntraining set; we use the annotated Google featured snippet as the knowledge.\nTask Prompt\nQASC Generate some knowledge about the input. Examples:\nInput: What type of water formation is formed by clouds?\nKnowledge: Clouds are made of water vapor.\nInput: What can prevent food spoilage?\nKnowledge: Dehydrating food is used for preserving food.\nInput: The process by which genes are passed is\nKnowledge: Genes are passed from parent to offspring.\nInput: The stomach does what in the body?\nKnowledge: The stomach is part of the digestive system.\nInput: What can cause rocks to break down?\nKnowledge: Mechanical weathering is when rocks are broken down by mechanical means.\nInput: {question}\nKnowledge:\nTable 10: Prompt for knowledge generation on QASC. Demonstration examples are selected from the QASC\ntraining set; we use one of the gold separate facts as the knowledge.']","The impact of knowledge quantity on GKP performance is that generally, performance increases with the quantity of knowledge statements. It saturates at M = 20 and begins to decline when more knowledge statements are introduced, possibly because more noisy knowledge is generated.","Increasing the number of generated knowledge statements generally improves performance up to a saturation point, after which additional statements may introduce noise.,",1.0,0.9490997896855989,0.99999999995,1.0,0.0,0.8697566142048024
"What are the limitations of GKP highlighted in the paper?,","['a giraffe in its pen some dirt  logs and a fence', 'Several giraffes spending time in an enclosed, grassy area', 'there is a goat that has a leash on ']",The limitations of GKP are not highlighted in the paper.,"Limitations include dependency on well-crafted few-shot prompts and potential risks of harmful or irrelevant knowledge generation.,",0.0,0.9842679269968908,0.0,0.0,0.0,0.1997788047476169
"What are the potential risks of using GKP?,","['A man wears gloves as he installs a toilet.', 'A computer laying upside down with two hands holding knives prying open a section of the back.', 'A close up of a red pair of scissors and a green sharpie marker. ']","The potential risks of using GKP (gloves, knives, and scissors) in this context include accidental cuts or injuries while handling sharp objects.","Risks include the possibility of lowering task performance if generated knowledge is factually incorrect or irrelevant, emphasizing the importance of careful prompt design.,",0.0,0.9819161149251331,0.0,0.0,0.0,0.2025927048381137
"What was the sentence given to Count Alexander Ilyich Rostov by the committee?,","['21 June 1922\nAPPEARANCE OF COUNT ALEXANDER ILYICH ROSTOV\nBEFORE THE EMERGENCY COMMITTEE OF THE PEOPLE’S COMMISSARIAT FOR INTERNAL AFFAIRS\nPresiding: Comrades V. A. Ignatov, M. S. Zakovsky, A. N. Kosarev\nProsecuting: A. Y. Vyshinsky\nProsecutor Vyshinsky: State your name.\nRostov: Count Alexander Ilyich Rostov, recipient of the Order of Saint Andrew,\nmember of the Jockey Club, Master of the Hunt.\nVyshinsky: You may have your titles; they are of no use to anyone else. But for the\nrecord, are you not Alexander Rostov, born in St. Petersburg, 24 October 1889?\nRostov: I am he.\nVyshinsky: Before we begin, I must say, I do not think that I have ever seen a\njacket festooned with so many buttons.\nRostov: Thank you.\nVyshinsky: It was not meant as a compliment.\nRostov: In that case, I demand satisfaction on the field of honor.\n[Laughter.]\nSecretary Ignatov: Silence in the gallery.\nVyshinsky: What is your current address?\nRostov: Suite 317 at the Hotel Metropol, Moscow.\nVyshinsky: How long have you lived there?\nRostov: I have been in residence since the fifth of September 1918. Just under four\nyears.\nVyshinsky: And your occupation?\nRostov: It is not the business of gentlemen to have occupations.\nVyshinsky: Very well then. How do you spend your time?\nRostov: Dining, discussing. Reading, reflecting. The usual rigmarole.\nVyshinsky: And you write poetry?\nRostov: I have been known to fence with a quill.\nVyshinsky: [Holding up a pamphlet] Are you the author of this long poem of 1913:\nWhere Is It Now?\nRostov: It has been attributed to me.\nVyshinsky: Why did you write the poem?\nRostov: It demanded to be written. I simply happened to be sitting at the particular\ndesk on the particular morning when it chose to make its demands.\nVyshinsky: And where was that exactly?\nRostov: In the south parlor at Idlehour.\nVyshinsky: Idlehour?\nRostov: The Rostov estate in Nizhny Novgorod.\nVyshinsky: Ah, yes. Of course. How apt. But let us return our attention to your\npoem. Coming as it did—in the more subdued years after the failed revolt of 1905—\nmany considered it a call to action. Would you agree with that assessment?\nRostov: All poetry is a call to action.\nVyshinsky: [Checking notes] And it was in the spring of the following year that you\nleft Russia for Paris . . . ?\nRostov: I seem to remember blossoms on the apple trees. So, yes, in all likelihood\nit was spring.\nVyshinsky: May 16 to be precise. Now, we understand the reasons for your self-\nimposed exile; and we even have some sympathy with the actions that prompted your\nflight. What concerns us here is your return in 1918. One wonders if you came back\nwith the intention of taking up arms and, if so, whether for or against the\nRevolution.', 'In replying It is nothing to Andrey, the Count was not simply resorting to a\nGallic figure of speech. In point of fact, the Count deserved as much thanks\nfor his little intervention as a swallow deserves for its trill. For since the age\nof fifteen, Alexander Rostov had been a master of seating tables.\nWhenever he was home for the holidays, his grandmother would\ninevitably call him into the library, where she liked to knit by the fireplace\nin a solitary chair.\n“Come in, my boy, and sit with me a moment.”\n“Certainly, Grandmother,” replied the Count, balancing himself on the\nedge of the fire grate. “How can I be of assistance?”\n“The prelate is coming for dinner on Friday night—as are the Duchess\nObolensky, Count Keragin, and the Minsky-Polotovs. . . .”\nHere she would let her voice trail off without further explanation; but no\nfurther explanation was needed. The Countess was of a mind that dinner\nshould provide one with respite from life’s trials and tribulations. Thus, she\ncould not countenance discussions of religion, politics, or personal sorrows\nat her table. Further complicating matters, the prelate was deaf in his left\near, partial to Latin epigrams, and prone to stare at décolletage whenever he\ndrank a glass of wine; while the Duchess Obolensky, who was particularly\ncaustic in summer, frowned upon pithy sayings and could not abide\ndiscussions of the arts. And the Keragins? Their great-grandfather had been\ncalled a Bonapartist by Prince Minsky-Polotov in 1811, and they had not\nexchanged a word with a Minsky-Polotov since.\n“How many will be in attendance?” asked the Count.\n“Forty.”\n“The usual assembly?”\n“More or less.”\n“The Osipovs?”\n“Yes. But Pierre is in Moscow. . . .”\n“Ah,” said the Count with the smile of the chess champion who has been\nconfronted with a new gambit.\nThe Nizhny Novgorod Province had a hundred prominent families,\nwhich over the course of two centuries had intermarried and divorced,', 'A\n1922\nAn Ambassador\nt half past six on the twenty-first of June 1922, when Count Alexander\nIlyich Rostov was escorted through the gates of the Kremlin onto Red\nSquare, it was glorious and cool. Drawing his shoulders back without\nbreaking stride, the Count inhaled the air like one fresh from a swim. The\nsky was the very blue that the cupolas of St. Basil’s had been painted for.\nTheir pinks, greens, and golds shimmered as if it were the sole purpose of a\nreligion to cheer its Divinity. Even the Bolshevik girls conversing before\nthe windows of the State Department Store seemed dressed to celebrate the\nlast days of spring.\n“Hello, my good man,” the Count called to Fyodor, at the edge of the\nsquare. “I see the blackberries have come in early this year!”\nGiving the startled fruit seller no time to reply, the Count walked briskly\non, his waxed moustaches spread like the wings of a gull. Passing through\nResurrection Gate, he turned his back on the lilacs of the Alexander\nGardens and proceeded toward Theatre Square, where the Hotel Metropol\nstood in all its glory. When he reached the threshold, the Count gave a wink\nto Pavel, the afternoon doorman, and turned with a hand outstretched to the\ntwo soldiers trailing behind him.\n“Thank you, gentlemen, for delivering me safely. I shall no longer be in\nneed of your assistance.”\nThough strapping lads, both of the soldiers had to look up from under\ntheir caps to return the Count’s gaze—for like ten generations of Rostov\nmen, the Count stood an easy six foot three.\n“On you go,” said the more thuggish of the two, his hand on the butt of\nhis rifle. “We’re to see you to your rooms.”']",The sentence given to Count Alexander Ilyich Rostov by the committee was not mentioned in the context provided.,"Count Alexander Ilyich Rostov was sentenced to house arrest in the Metropol Hotel in Moscow for the rest of his life, under the condition that if he ever left the hotel, he would be shot.,",1.0,0.0,0.0,0.0,0.3333333322222222,0.21221337242008892
"What poem is associated with Count Rostov, and what is its significance?,","['Rostov: By that point, I’m afraid that my days of taking up arms were behind me.\nVyshinsky: Why then did you come back?\nRostov: I missed the climate.\n[Laughter.]\nVyshinsky: Count Rostov, you do not seem to appreciate the gravity of your position.\nNor do you show the respect that is due the men convened before you.\nRostov: The Tsarina had the same complaints about me in her day.\nIgnatov: Prosecutor Vyshinsky. If I may . . .\nVyshinsky: Secretary Ignatov.\nIgnatov: I have no doubt, Count Rostov, that many in the gallery are surprised to\nfind you so charming; but I, for one, am not surprised in the least. History has\nshown charm to be the final ambition of the leisure class. What I do find\nsurprising is that the author of the poem in question could have become a man so\nobviously without purpose.\nRostov: I have lived under the impression that a man’s purpose is known only to God.\nIgnatov: Indeed. How convenient that must have been for you.\n[The Committee recesses for twelve minutes.]\nIgnatov: Alexander Ilyich Rostov, taking into full account your own testimony, we\ncan only assume that the clear-eyed spirit who wrote the poem Where Is It Now? has\nsuccumbed irrevocably to the corruptions of his class—and now poses a threat to\nthe very ideals he once espoused. On that basis, our inclination would be to have\nyou taken from this chamber and put against the wall. But there are those within\nthe senior ranks of the Party who count you among the heroes of the\nprerevolutionary cause. Thus, it is the opinion of this committee that you should\nbe returned to that hotel of which you are so fond. But make no mistake: should\nyou ever set foot outside of the Metropol again, you will be shot. Next matter.\nBearing the signatures of\nV. A. Ignatov\nM. S. Zakovsky\nA. N. Kosarev', '“What with the revolt of 1905 and the repressions that followed, when\nwe graduated it was still a dangerous time for writing poems of political\nimpatience. Given Mishka’s background, the Okhrana would have swept\nhim up with a broom. So one night—after polishing off a particularly good\nbottle of Margaux—we decided to publish the poem under my name.”\n“But why yours?”\n“What were they going to do to Count Alexander Rostov—member of\nthe Jockey Club and godson of a counselor to the Tsar?” The Count shook\nhis head. “The irony, of course, is that the life which ended up being saved\nwas mine, not his. But for that poem, they would have shot me back in\n1922.”\nKaterina, who had listened to this story intently, was suddenly holding\nback tears.\n“Ah, but there you have him,” she said.\nThey were both silent as she regained her composure.\n“I want you to know,” said the Count, “how much I appreciate your\ncoming to tell me in person.” But Katerina dismissed his gratitude.\n“I came at Mikhail’s request. He asked me to bring you something.”\nFrom her satchel she took out a rectangular package wrapped in plain\nbrown paper and tied with twine.\nTaking the package in hand, the Count could tell from its weight that it\nwas a book.\n“It is his project,” said the Count with a smile.\n“Yes,” she said. Then she added with pointed emphasis: “He slaved over\nit.”\nThe Count nodded to express his understanding and to assure Katerina\nthat he did not take the bestowal lightly.\nKaterina looked once more around the room with a light shake of the\nhead as if it somehow exemplified the mystery of outcomes; then she said\nthat she should go.\nThe Count rose to his feet with her, setting Mishka’s project on the chair.\n“Are you going back to Yavas?” he asked.\n“No.”\n“Will you be staying in Moscow?”\n“No.”\n“Where then?”', 'Nevsky Prospekt and wandered all the way to Tikhvin Cemetery where, on\na spot overlooking the Neva River, this firefly, this pinwheel, this wonder of\nthe world had suddenly taken his hand.\n“Ah, Count Rostov,” exclaimed Arkady in passing. “There you are. I\nbelieve I have a message for you. . . .” Returning to the front desk, Arkady\nquickly rifled through some notes. “Here.”\nThe message, which had been taken down by the hotel’s receptionist,\nconveyed Mishka’s apologies and explained that as Katerina was under the\nweather, he was returning to St. Petersburg earlier than planned. Taking a\nmoment to mask his disappointment, the Count looked up from the note to\nthank Arkady, but the desk captain had already turned his attention to\nanother guest.\n“Good evening, Count Rostov.” Andrey took a quick look in the Book. “A\nparty of two tonight, isn’t it?”\n“I’m afraid it’s going to be a party of one, Andrey.”\n“Nonetheless, it is our pleasure to have you. Your table should be ready\nin just a few minutes.”\nWith the recent recognition of the USSR by Germany, England, and\nItaly, a wait of a few minutes had become increasingly common at the\nBoyarsky; but such was the price of being welcomed back into the\nsisterhood of nations and the brotherhood of trade.\nAs the Count stepped aside, a man with a pointed beard came marching\ndown the hallway with a protégé in tow. Though the Count had only seen\nhim once or twice before, the Count could tell he was the Commissar of\nSomething-or-Other, for he walked with urgency, talked with urgency, and\neven came to a stop with urgency.\n“Good evening, comrade Soslovsky,” said Andrey with a welcoming\nsmile.\n“Yes,” pronounced Soslovsky—as if he’d just been asked whether he\nwanted to be seated immediately.\nWith a nod of understanding, Andrey signaled a waiter, handed him two\nmenus, and directed him to lead the gentlemen to table fourteen.']","The poem associated with Count Rostov is ""Where Is It Now?"" The significance of the poem is that it was published under Rostov's name to protect the real author, Mishka, from being targeted by the authorities during a dangerous time for writing politically charged poems. The poem ultimately saved Rostov's life, as without it, he would have been shot in 1922.","The poem -Where Is It Now?- is attributed to Count Rostov and is considered a call to action written in the aftermath of the failed revolt of 1905.,",1.0,0.9587363017209553,0.9999999999,0.5,0.0,0.38406748635519006
"What is the significance of Suite 317 in the story?,","['Later that night, as he sat alone on his bed, the Count mulled over his visit\nto his old suite.\nWhat had stayed with him was not the sight of his family’s clock still\nticking by the door, nor the grandeur of the architecture, nor even the view\nfrom the northwest window. What had lingered with him was the sight of\nthe tea service on the table beside the folded paper.\nThat little tableaux, for all its innocence, was somehow suggestive of\nexactly what had been bearing down on the Count’s soul. For he understood\nevery aspect of the scene at a glance. Having returned from some outing at\nfour o’clock and having hung his jacket on the back of a chair, the room’s\ncurrent resident had called for tea and an afternoon edition. Then he had\nsettled himself down on the couch to while away a civilized hour before it\nwas time to dress for dinner. In other words, what the Count had observed\nin suite 317 was not simply an afternoon tea, but a moment in the daily life\nof a gentleman at liberty.\nIn light of these thoughts, the Count reviewed his new room—the one\nhundred square feet that had been assigned to him. Never had it seemed so\nsmall. The bed crowded the coffee table, the coffee table crowded the high-\nback chair, and the high-back chair had to be shoved aside every time one\nwished to open the closet. Simply put, there was not enough space to\naccommodate such a civilized hour.\nBut as the Count gazed around him with this forlorn thought, a voice\nonly half his own reminded him that in the Metropol there were rooms\nbehind rooms, and doors behind doors. . . .\nRising from his bed, the Count navigated his way around his\ngrandmother’s coffee table, set aside the high-back chair, and stood before\nhis telephone box of a closet. Running along the perimeter of where the\ncloset met the wall was an elegant molding. The Count had always thought\nthis flourish a little excessive; but what if the closet had been built in an old\ndoorframe? Opening the door, the Count parted his clothes and tentatively\nrapped on the back wall. The sound was promisingly thin. With three\nfingers he gave the barrier a push and could feel it flex. He took all his\njackets out and dumped them on the bed. Then holding the jambs of the\ndoor he kicked the inner wall with his heel. There came a pleasant crack.\nLeaning back, he kicked again and again until the barrier splintered. Then\nhe pulled the jagged planks back into his room and slipped through the gap.', 'common man may toss the banners of the old regime on the victory pyre,\nbut soon enough trumpets will blare and pomp will take its place at the side\nof the throne, having once again secured its dominion over history and\nkings.\nNina was running her fingers over the various serving implements with\na blend of admiration and awe. Then she came to a stop.\n“What is that?”\nOn the shelf behind a candelabra stood a three-inch-tall woman\nfashioned from silver with the hooped skirt and towering hair of a Marie\nAntoinette.\n“It’s a summoner,” said the Count.\n“A summoner?”\n“To be placed on the table beside the hostess.”\nThe Count picked up the little lady by her bouffant and when he\nwaggled her to and fro, out from under her skirt came that delightful jangle\n(at a high C) that had prompted the end of a thousand courses and the\nclearing of fifty thousand plates.\nIn the days that followed, Nina presented her curriculum systematically,\nleading her student from room to room. At the onset, the Count had\nassumed that all their classes would be held on the hotel’s lower levels,\nwhere its services were housed. But having visited the basement, the mail\nroom, the switchboard, and all the other nooks of the first floor, one\nafternoon they proceeded up the staircase to the suites.\nNow, admittedly, the exploration of private apartments represents\nsomething of a break with decorum, but Nina’s interest in visiting the rooms\nwas not thievery. Nor was it snooping per se. It was the views.\nEach of the rooms of the Metropol offered an entirely different\nperspective—one that was shaped not only by altitude and orientation, but\nby season and time of day. Thus, if by chance one cared to watch the\nbattalions marching toward Red Square on the Seventh of November, one\nshould go no further than room 322. But when one wished to drop\nsnowballs on unsuspecting strollers, this was best accomplished from the\ndeep-ledged windows of 405. Even room 244, a rather depressing little spot', 'overlooking the alley behind the hotel, had its allure: for from there, if one\nleaned far enough out of the window, one could watch the fruit sellers\ngather at the kitchen door and catch the occasional apple tossed from below.\nBut if one wished to watch the arrival of guests at the Bolshoi on a\nsummer night, the best vantage point, without question, was the northwest\nwindow of 317. And so . . .\nOn the twelfth of July at seven o’clock, as the Count was crossing the\nlobby, Nina caught his eye and gave him the signal. Two minutes later,\nhaving joined her on the stairs, he was trailing her past rooms 313, 314, and\n315, toward the door of his old suite. And when Nina turned the key and\nslipped inside, the Count dutifully followed—but with a palpable sense of\nforeboding.\nIn a glance the Count reacquainted himself with every inch of the room.\nThe couch and chairs upholstered in red remained, as did the grandfather\nclock and the large Chinese urns from Idlehour. On the French coffee table\n(that had been supplied to replace his grandmother’s) was a folded copy of\nPravda, a silver service, and an unfinished cup of tea.\n“Quick,” she said again, as she padded across the room to the window at\nthe northwest corner.\nAcross Theatre Square the Bolshoi was lit from portico to pediment. The\nBolsheviks who, as usual, were dressed like the cast of La Bohème, were\ntaking advantage of the warm night air by mingling among the columns.\nSuddenly, the lights in the lobby flickered. Scuffing out their cigarettes, the\nmen took their ladies by the elbow. But just as the last of the attendees was\ndisappearing through the doors, a taxi pulled to the curb, the door flung\nopen, and a woman in red dashed up the stairs with the hem of her dress in\nher hands.\nLeaning forward, Nina cupped her palms against the glass and squinted.\n“If only I were there and she were here,” she sighed.\nAnd there, thought the Count, was a suitable plaint for all mankind.']","Suite 317 holds significance in the story as it represents a moment in the daily life of a gentleman at liberty. It is where the Count observed a scene of a gentleman calling for tea and an afternoon edition, settling down on the couch to while away a civilized hour before dressing for dinner. The northwest window of Suite 317 also provides the best vantage point to watch the arrival of guests at the Bolshoi on a summer night. Suite 317 symbolizes a moment of leisure and observation in the midst of the Count's restricted circumstances in his new room.","Suite 317 at the Metropol Hotel was Count Rostov's residence prior to his house arrest; however, after the sentence, he was moved to a small attic room.,",1.0,0.991265866961102,0.8333333332916666,0.5,0.0,0.8792544144301448
"How does Count Rostov adapt to his new attic quarters?,","['corridor servicing a bathroom and six bedrooms reminiscent of monastic\ncells. This attic was originally built to house the butlers and ladies’ maids of\nthe Metropol’s guests; but when the practice of traveling with servants fell\nout of fashion, the unused rooms had been claimed by the caprices of casual\nurgency—thenceforth warehousing scraps of lumber, broken furniture, and\nother assorted debris.\nEarlier that day, the room closest to the stairwell had been cleared of all\nbut a cast-iron bed, a three-legged bureau, and a decade of dust. In the\ncorner near the door was a small closet, rather like a telephone box, that had\nbeen dropped in the room as an afterthought. Reflecting the pitch of the\nroof, the ceiling sloped at a gradual incline as it moved away from the door,\nsuch that at the room’s outer wall the only place where the Count could\nstand to his full height was where a dormer accommodated a window the\nsize of a chessboard.\nAs the two guards looked on smugly from the hall, the good captain\nexplained that he had summoned the bellhops to help the Count move what\nfew belongings his new quarters would accommodate.\n“And the rest?”\n“Becomes the property of the People.”\nSo this is their game, thought the Count.\n“Very well.”\nBack down the belfry he skipped as the guards hurried behind him, their\nrifles clacking against the wall. On the third floor, he marched along the\nhallway and into his suite where the two bellhops looked up with woeful\nexpressions.\n“It’s all right, fellows,” the Count assured and then began pointing:\n“This. That. Those. All the books.”\nAmong the furnishings destined for his new quarters, the Count chose\ntwo high-back chairs, his grandmother’s oriental coffee table, and a favorite\nset of her porcelain plates. He chose the two table lamps fashioned from\nebony elephants and the portrait of his sister, Helena, which Serov had\npainted during a brief stay at Idlehour in 1908. He did not forget the leather\ncase that had been fashioned especially for him by Asprey in London and\nwhich his good friend Mishka had so appropriately christened the\nAmbassador.', 'He was now inside a dark, narrow space that smelled of dry cedar,\npresumably the interior of the neighboring closet. Taking a breath, he turned\nthe knob, opened the door, and entered a room that was the mirror image of\nhis own—but in which five unused bedframes had been stored. At some\npoint, two of the frames, which had been leaning against the wall, had\nfallen, pinning the hallway door shut. Pulling the frames aside, the Count\nopened the door, dragged everything out of the room, and began to\nrefurnish.\nFirst, he reunited the two high-back chairs with his grandmother’s coffee\ntable. Then, taking the belfry stairs, he went down to the basement. From\nthe cabinet of curiosities, he retrieved one of his rugs, the standing lamp,\nand the small bookcase in three separate trips. Then vaulting the steps two\nat a time, he made one final visit in order to claim ten of the weighty novels\nthat had been abandoned. Once his new study was furnished, he went down\nthe hall and borrowed the roofer’s hammer and five nails.\nThe Count had not wielded a hammer since he was a boy at Idlehour\nwhen he would help Tikhon, the old caretaker, repair the fencing in the first\nweeks of spring. What a fine feeling it had been to bring the hammer down\nsquarely on the head of a nail, driving it through a plank into a fence post as\nthe impact echoed in the morning air. But on the very first stroke of this\nhammer what the Count squarely hit was the back of his thumb. (Lest you\nhave forgotten, it is quite excruciating to hammer the back of your thumb. It\ninevitably prompts a hopping up and down and the taking of the Lord’s\nname in vain.)\nBut Fortune does favor the bold. So, while the next swing of the\nhammer glanced off the nail’s head, on the third the Count hit home; and by\nthe second nail, he had recovered the rhythm of set, drive, and sink—that\nancient cadence which is not to be found in quadrilles, or hexameters, or in\nVronsky’s saddlebags!\nSuffice it to say that within half an hour four of the nails had been driven\nthrough the edge of the door into the doorframe—such that from that\nmoment forward the only access to the Count’s new room would be through\nthe sleeves of his jackets. The fifth nail he saved for the wall above the\nbookcase so that he could hang the portrait of his sister.\nHis work completed, the Count sat down in one of the high-back chairs\nand felt an almost surprising sense of bliss. The Count’s bedroom and this', 'So when he finally bid Mr. Webster goodnight (with a package of\nAmerican cigarettes for Anna in one pocket and a chocolate bar for Sofia in\nthe other), the Count headed homeward in an elevated frame of mind.\nThe fourth-floor hallway was empty and still. Behind the line of closed\ndoors slept the practical and predictable, the cautious and comfortable.\nTucked under their covers, they dreamt of breakfast, leaving the hallways of\nnight to be walked by the likes of Samuel Spadsky and Philip Marlov and\nAlexander Ilyich Rostov. . . .\n“Yes,” said the Count as he weaved down the hall: “I am the waiter.”\nThen with the finely attuned senses of his brotherhood, the Count\nnoticed something suggestive out of the corner of his eye. It was the door to\nroom 428.\nBoris Godunov was a production of three and a half hours. A post-\ntheater supper would last an hour and a half. So, in all likelihood, the\nItalians would not return to the hotel for another thirty minutes. The Count\nknocked and waited; he knocked again to be sure; then retrieving the key\nfrom his vest, he unlocked the door and crossed the threshold clear-eyed,\nquick, and without compunction.\nIn a glance, he could see that the night service had already visited the\nsuite, for everything was in its proper place: the chairs, the magazines, the\ncarafe of water and glasses. In the bedroom, he found the corners of the bed\nturned down at an angle of forty-five degrees.\nOpening the right closet door, he was about to take the newsboy’s cap\noff its hook when he noticed something he’d missed before. On the shelf\nabove the clothes was a bundle wrapped in paper and tied with twine—a\nbundle about the size of a small statuette. . . .\nPutting the newsboy’s cap on his head, the Count took the bundle off the\nshelf and laid it on the bed. He untied the string and carefully peeled back\nthe paper—only to find a set of Russian nesting dolls. Painted in a simple if\ntraditional style, available in a hundred Moscow shops, the matryoshka was\njust that sort of whimsical toy that two parents would bring home to their\nchild from a trip to Russia.\nAnd in which they could easily hide something . . .\nSitting on the bed, the Count opened the largest of the nesting dolls.\nThen he opened the second largest of the nesting dolls. Then he opened the']","Count Rostov adapts to his new attic quarters by clearing out the unused room closest to the stairwell, furnishing it with items he chooses from his previous quarters, and even hammering nails to secure the door and hang a portrait of his sister. He also retrieves items from the basement and makes the room his own by rearranging the furniture and personal belongings.","Count Rostov carefully selects a few personal belongings, including a favorite desk and chairs, and makes the small attic room feel more livable by arranging the limited space thoughtfully.,",1.0,0.9933740727177199,0.99999999995,0.0,0.0,0.37021774893597326
"How is the Metropol Hotel described in the narrative?,","['all took their first tentative steps into Russian society. But if this was so, it\nwas largely due to the hive of activity beneath the parlor floors. For there,\njust a few steps below street level, were the butlers, cooks, and footmen\nwho together ensured that when the notions of Darwin or Manet were first\nbandied about, all went off without a hitch.\nAnd so it was in the Metropol.\nEver since its opening in 1905, the hotel’s suites and restaurants had\nbeen a gathering spot for the glamorous, influential, and erudite; but the\neffortless elegance on display would not have existed without the services\nof the lower floor:\nComing off the wide marble steps that descended from the lobby, one\nfirst passed the newsstand, which offered a gentleman a hundred headlines,\nalbeit now just in Russian.\nNext was the shop of Fatima Federova, the florist. A natural casualty of\nthe times, Fatima’s shelves had been emptied and her windows papered\nover back in 1920, turning one of the hotel’s brightest spots into one of its\nmost forlorn. But in its day, the shop had sold flowers by the acre. It had\nprovided the towering arrangements for the lobby, the lilies for the rooms,\nthe bouquets of roses that were tossed at the feet of the Bolshoi ballerinas,\nas well as the boutonnieres on the men who did the tossing. What’s more,\nFatima was fluent in the floral codes that had governed polite society since\nthe Age of Chivalry. Not only did she know the flower that should be sent\nas an apology, she knew which flower to send when one has been late;\nwhen one has spoken out of turn; and when, having taking notice of the\nyoung lady at the door, one has carelessly overtrumped one’s partner. In\nshort, Fatima knew a flower’s fragrance, color, and purpose better than a\nbee.\nWell, Fatima’s may have been shuttered, reflected the Count, but\nweren’t the flower shops of Paris shuttered under the “reign” of\nRobespierre, and didn’t that city now abound in blossoms? Just so, the time\nfor flowers in the Metropol would surely come again.\nAt the very end of the hall, one finally came to Yaroslav’s barbershop. A\nland of optimism, precision, and political neutrality, it was the Switzerland\nof the hotel. If the Count had vowed to master his circumstances through\npracticalities, then here was a glimpse of the means: a religiously kept\nappointment for a weekly trim.', '“Meet me here,” she said, “tomorrow at two.”\nAnd thus began the Count’s education.\nHaving lived at the Metropol for four years, the Count considered himself\nsomething of an expert on the hotel. He knew its staff by name, its services\nby experience, and the decorative styles of its suites by heart. But once Nina\nhad taken him in hand, he realized what a novice he had been.\nIn the ten months that Nina had lived at the Metropol, she had been\nconfronted with her own version of confinement. For, as her father had been\nposted only “temporarily” to Moscow, he had not bothered to enroll her in\nschool. And as Nina’s governess still had one foot set firmly in the\nhinterlands, she preferred that her charge remain on the hotel’s premises\nwhere she was less likely to be corrupted by street lamps and trolley cars.\nSo, if the door of the Metropol was known the world over for spinning\nwithout stop, it spun not for Nina. But, an enterprising and tireless spirit, the\nyoung lady had made the most of her situation by personally investigating\nthe hotel until she knew every room, its purpose, and how it might be put to\nbetter use.\nYes, the Count had gone to the little window at the back of the lobby to\nask for his mail, but had he been to the sorting room where the incoming\nenvelopes were spilled on a table at ten and at two—including those that\nwere stamped in red with the unambiguous instruction For Immediate\nDelivery?\nAnd yes, he had visited Fatima’s in the days when it was open, but had\nhe been inside the cutting room? Through a narrow door at the back of her\nshop was that niche with a light green counter where stems had been\nsnipped and roses dethorned, where even now one could find scattered\nacross the floor the dried petals of ten perennials essential to the making of\npotions.\nOf course, exclaimed the Count to himself. Within the Metropol there\nwere rooms behind rooms and doors behind doors. The linen closets. The\nlaundries. The pantries. The switchboard!\nIt was like sailing on a steamship. Having enjoyed an afternoon shooting\nclay pigeons off the starboard bow, a passenger dresses for dinner, dines at', 'corridor servicing a bathroom and six bedrooms reminiscent of monastic\ncells. This attic was originally built to house the butlers and ladies’ maids of\nthe Metropol’s guests; but when the practice of traveling with servants fell\nout of fashion, the unused rooms had been claimed by the caprices of casual\nurgency—thenceforth warehousing scraps of lumber, broken furniture, and\nother assorted debris.\nEarlier that day, the room closest to the stairwell had been cleared of all\nbut a cast-iron bed, a three-legged bureau, and a decade of dust. In the\ncorner near the door was a small closet, rather like a telephone box, that had\nbeen dropped in the room as an afterthought. Reflecting the pitch of the\nroof, the ceiling sloped at a gradual incline as it moved away from the door,\nsuch that at the room’s outer wall the only place where the Count could\nstand to his full height was where a dormer accommodated a window the\nsize of a chessboard.\nAs the two guards looked on smugly from the hall, the good captain\nexplained that he had summoned the bellhops to help the Count move what\nfew belongings his new quarters would accommodate.\n“And the rest?”\n“Becomes the property of the People.”\nSo this is their game, thought the Count.\n“Very well.”\nBack down the belfry he skipped as the guards hurried behind him, their\nrifles clacking against the wall. On the third floor, he marched along the\nhallway and into his suite where the two bellhops looked up with woeful\nexpressions.\n“It’s all right, fellows,” the Count assured and then began pointing:\n“This. That. Those. All the books.”\nAmong the furnishings destined for his new quarters, the Count chose\ntwo high-back chairs, his grandmother’s oriental coffee table, and a favorite\nset of her porcelain plates. He chose the two table lamps fashioned from\nebony elephants and the portrait of his sister, Helena, which Serov had\npainted during a brief stay at Idlehour in 1908. He did not forget the leather\ncase that had been fashioned especially for him by Asprey in London and\nwhich his good friend Mishka had so appropriately christened the\nAmbassador.']","The Metropol Hotel is described as a glamorous, influential, and erudite gathering spot for the upper class, with lower floor staff such as butlers, cooks, and footmen ensuring the smooth running of the establishment.","The Metropol Hotel is depicted as a grand and opulent building in the heart of Moscow, featuring elegant suites, a fine dining restaurant called the Boyarsky, and a bustling Piazza dining room.,",1.0,0.9483265771487907,0.9999999999,,0.249999999375,0.2303871863792623
"What does the Count do with his sister Helena’s portrait?,","['courtship. I suffered through lunches and teas. I ground my teeth as I\nwatched them stroll through the gardens. But as I bided my time, his self-\ncontrol surpassed my wildest expectations. He pulled out her chair; he\npicked blossoms; he read verses; he wrote verses! And always when he\ncaught my eye there was that little twist in his smile.\n“But then on the afternoon of my sister’s twentieth birthday, when he\nwas off on maneuvers and we were paying a visit to a neighbor, we returned\nat dusk to find his troika in front of our house. From a glance at Helena, I\ncould sense her elation. He has rushed back all the way from his battalion,\nshe was thinking, to wish me well on my day. She nearly jumped from her\nhorse and ran up the steps; and I followed her like a condemned man to the\nnoose.”\nThe Count emptied his glass and slowly set it back onto the bar.\n“But there inside the entry hall, I did not find my sister in his arms. I\nfound her two steps from the door, trembling. Against the wall was\nNadezhda, my sister’s handmaiden. Her bodice torn open, her arms across\nher chest, her face scarlet with humiliation, she looked briefly at my sister\nthen ran up the stairs. In horror, my sister stumbled across the hall,\ncollapsed in a chair, and covered her face with her hands. And our noble\nlieutenant? He grinned at me like a cat.\n“When I began to express my outrage, he said: ‘Oh, come now,\nAlexander. It is Helena’s birthday. In her honor, let us call it even.’ Then\nroaring with laughter, he walked out the door without giving my sister a\nglance.”\nCharles whistled softly.\nThe Count nodded.\n“But at this juncture, Charles, I did not do nothing. I crossed the\nentryway to the wall where a pair of pistols hung beneath the family crest.\nWhen my sister grabbed at my sleeve and asked where I was going, I too\nwalked out the door without giving her a glance.”\nThe Count shook his head in condemnation of his own behavior.\n“He had a one-minute head start, but he hadn’t used it to put distance\nbetween us. He had casually climbed into his troika and set his horses\nmoving at little more than a trot. And there you have him in a nutshell, my\nfriend: a man who raced toward parties, and trotted from his own\nmisdeeds.”', 'the high grass on the branch line, where they would be met by a Rostov\ncoach-and-four. With their bags on top, the driver in the carriage, and\nAlexander at the reins, they would charge across the countryside waving at\nevery peasant girl until they turned onto the road lined with apple trees that\nled to the family seat.\nAs they shed their coats in the entry hall, their bags would be whisked to\nthe grand bedrooms of the east wing, where velvet cords could be pulled to\nsummon a cold glass of beer, or hot water for a bath. But first, they would\nproceed to the drawing room where—at this very table with its red pagoda\n—the Countess would be hosting some blue-blooded neighbor for tea.\nInvariably dressed in black, the Countess was one of those dowagers\nwhose natural independence of mind, authority of age, and impatience with\nthe petty made her the ally of all irreverent youth. She would not only\nabide, but enjoyed when her grandson would interrupt polite conversation\nto question the standing of the church or the ruling class. And when her\nguest grew red and responded in a huff, the Countess would give Mishka a\nconspiratorial wink, as if they stood arm in arm in the battle against boorish\ndecorum and the outmoded attitudes of the times.\nHaving paid their respects to the Countess, Mishka and Alexander\nwould head out the terrace doors in search of Helena. Sometimes they\nwould find her under the pergola overlooking the gardens and sometimes\nunder the elm tree at the bend in the river; but wherever they found her, at\nthe sound of their approach she would look up from her book and offer a\nwelcoming smile—not unlike the one captured in this portrait on the wall.\nWith Helena, Alexander was always his most outlandish, claiming as he\ncollapsed on the grass that they had just met Tolstoy on the train; or that he\nhad decided after careful consideration to join a monastery and take an\neternal vow of silence. Immediately. Without a moment’s delay. Or, as soon\nas they’d had lunch.\n“Do you really think that silence would suit you?” Helena would ask.\n“Like deafness suited Beethoven.”\nThen, after casting a friendly glance at Mishka, Helena would laugh,\nlook back at her brother, and ask, “What is to become of you, Alexander?”\nThey all asked that question of the Count. Helena, the Countess, the\nGrand Duke. What is to become of you, Alexander? But they asked it in\nthree different ways.', 'The Count looked into his empty glass. He hadn’t spoken of these events\nin so many years.\n“It is late,” he said. “And the story is long.”\nBy way of response, Charles refilled their glasses.\nSo the Count took Charles all the way back to the fall of 1913, when on\nan inclement night he had set out for the twenty-first birthday of the\nPrincess Novobaczky. He described the ice on the driveway, and Mrs.\nTrent’s roast, and the torn IOU—and how a few degrees here and there had\nlanded him on the terrace in the arms of the Princess while the rash\nlieutenant retched in the grass.\nCharles laughed.\n“But, Alexander, that sounds splendid. Surely, it’s not the reason you left\nRussia.”\n“No,” admitted the Count, but then he continued with his fateful tale:\n“Seven months pass, Charles. It is the spring of 1914, and I return to the\nfamily estate for a visit. Having paid my respects to my grandmother in the\nlibrary, I venture outside in search of my sister, Helena, who likes to read\nunder the great elm at the bend in the river. From a hundred feet away, I can\ntell that she is not herself—that is, I can tell that she is more than herself.\nUpon seeing me she sits up with a sparkle in her eye and a smile on her lips,\nclearly eager to share some piece of news, which I am now equally eager to\nhear. But just as I cross the lawn toward her, she looks over my shoulder\nand smiles even more brightly to see a lone figure approaching on a steed—\na lone figure in the uniform of the Hussars. . . .\n“You see the dilemma the fox had put me in, Charles. While I had been\ncarousing back in Moscow, he had sought my sister out. He had arranged an\nintroduction and then courted her carefully, patiently, successfully. And\nwhen he swung down from the saddle and our eyes met, he could barely\nkeep the twist of mirth from his lips. But how was I to explain the situation\nto Helena? This angel of a thousand virtues? How was I to tell her that the\nman she has fallen in love with has sought her affections not due to an\nappreciation of her qualities, but to settle a score?”\n“What did you do?”\n“Ah, Charles. What did I do? I did nothing. I thought surely his true\nnature would find occasion to express itself—much as it had at the\nNovobaczkys’. So in the weeks that followed, I hovered at the edge of their']",The Count hangs Helena's portrait on the wall.,"The Count takes his sister Helena's portrait with him to the attic room, where it serves as a cherished memento of his family and past life.,",0.0,0.974836733290914,0.49999999995,0.0,0.33333333277777777,0.23352844428228972
"What is the role of the Metropol Hotel staff in the story?,","['A fancy metal cloth is seen against huge barred windows at a stone train station.', 'Travelers making their way through a busy transport hub.', 'A train station with people standing on a platform beside a train. ']",There is no mention of the Metropol Hotel staff in the context provided.,"The staff, including characters like Andrey and Marina, serve as part of Count Rostov's social circle within the confines of the hotel, offering companionship and support.,",1.0,0.9273942586466487,0.0,0.0,0.0,0.20046972524995316
"What is the significance of the Count's desk?,","['A office desk with a monitor, laptop, and books. ', 'A desk with a computer and collectible figurines above.', 'A desk and chair in an office cubicle.']",There is no mention of a Count's desk in the context provided.,"The desk, inherited from the Grand Duke, symbolizes the Count's heritage and resilience, as it holds secret compartments with gold coins and serves as a source of stability in his confined life.,",1.0,0.9310847408403267,0.0,0.0,0.249999999375,0.20833474082037845
"How does Count Rostov maintain his dignity under house arrest?,","['A\n1922\nAn Ambassador\nt half past six on the twenty-first of June 1922, when Count Alexander\nIlyich Rostov was escorted through the gates of the Kremlin onto Red\nSquare, it was glorious and cool. Drawing his shoulders back without\nbreaking stride, the Count inhaled the air like one fresh from a swim. The\nsky was the very blue that the cupolas of St. Basil’s had been painted for.\nTheir pinks, greens, and golds shimmered as if it were the sole purpose of a\nreligion to cheer its Divinity. Even the Bolshevik girls conversing before\nthe windows of the State Department Store seemed dressed to celebrate the\nlast days of spring.\n“Hello, my good man,” the Count called to Fyodor, at the edge of the\nsquare. “I see the blackberries have come in early this year!”\nGiving the startled fruit seller no time to reply, the Count walked briskly\non, his waxed moustaches spread like the wings of a gull. Passing through\nResurrection Gate, he turned his back on the lilacs of the Alexander\nGardens and proceeded toward Theatre Square, where the Hotel Metropol\nstood in all its glory. When he reached the threshold, the Count gave a wink\nto Pavel, the afternoon doorman, and turned with a hand outstretched to the\ntwo soldiers trailing behind him.\n“Thank you, gentlemen, for delivering me safely. I shall no longer be in\nneed of your assistance.”\nThough strapping lads, both of the soldiers had to look up from under\ntheir caps to return the Count’s gaze—for like ten generations of Rostov\nmen, the Count stood an easy six foot three.\n“On you go,” said the more thuggish of the two, his hand on the butt of\nhis rifle. “We’re to see you to your rooms.”', 'In replying It is nothing to Andrey, the Count was not simply resorting to a\nGallic figure of speech. In point of fact, the Count deserved as much thanks\nfor his little intervention as a swallow deserves for its trill. For since the age\nof fifteen, Alexander Rostov had been a master of seating tables.\nWhenever he was home for the holidays, his grandmother would\ninevitably call him into the library, where she liked to knit by the fireplace\nin a solitary chair.\n“Come in, my boy, and sit with me a moment.”\n“Certainly, Grandmother,” replied the Count, balancing himself on the\nedge of the fire grate. “How can I be of assistance?”\n“The prelate is coming for dinner on Friday night—as are the Duchess\nObolensky, Count Keragin, and the Minsky-Polotovs. . . .”\nHere she would let her voice trail off without further explanation; but no\nfurther explanation was needed. The Countess was of a mind that dinner\nshould provide one with respite from life’s trials and tribulations. Thus, she\ncould not countenance discussions of religion, politics, or personal sorrows\nat her table. Further complicating matters, the prelate was deaf in his left\near, partial to Latin epigrams, and prone to stare at décolletage whenever he\ndrank a glass of wine; while the Duchess Obolensky, who was particularly\ncaustic in summer, frowned upon pithy sayings and could not abide\ndiscussions of the arts. And the Keragins? Their great-grandfather had been\ncalled a Bonapartist by Prince Minsky-Polotov in 1811, and they had not\nexchanged a word with a Minsky-Polotov since.\n“How many will be in attendance?” asked the Count.\n“Forty.”\n“The usual assembly?”\n“More or less.”\n“The Osipovs?”\n“Yes. But Pierre is in Moscow. . . .”\n“Ah,” said the Count with the smile of the chess champion who has been\nconfronted with a new gambit.\nThe Nizhny Novgorod Province had a hundred prominent families,\nwhich over the course of two centuries had intermarried and divorced,', 'Having acknowledged that a man must master his circumstances or\notherwise be mastered by them, the Count thought it worth considering how\none was most likely to achieve this aim when one had been sentenced to a\nlife of confinement.\nFor Edmond Dantès in the Château d’If, it was thoughts of revenge that\nkept him clear minded. Unjustly imprisoned, he sustained himself by\nplotting the systematic undoing of his personal agents of villainy. For\nCervantes, enslaved by pirates in Algiers, it was the promise of pages as yet\nunwritten that spurred him on. While for Napoleon on Elba, strolling among\nchickens, fending off flies, and sidestepping puddles of mud, it was visions\nof a triumphal return to Paris that galvanized his will to persevere.\nBut the Count hadn’t the temperament for revenge; he hadn’t the\nimagination for epics; and he certainly hadn’t the fanciful ego to dream of\nempires restored. No. His model for mastering his circumstances would be\na different sort of captive altogether: an Anglican washed ashore. Like\nRobinson Crusoe stranded on the Isle of Despair, the Count would maintain\nhis resolve by committing to the business of practicalities. Having\ndispensed with dreams of quick discovery, the world’s Crusoes seek shelter\nand a source of fresh water; they teach themselves to make fire from flint;\nthey study their island’s topography, its climate, its flora and fauna, all the\nwhile keeping their eyes trained for sails on the horizon and footprints in\nthe sand.\nIt was to this end that the Count had given the old Greek three notes to\ndeliver. Within a matter of hours, the Count had been visited by two\nmessengers: a young lad from Muir & Mirrielees bearing fine linens and a\nsuitable pillow; and another from Petrovsky Passage with four bars of the\nCount’s favorite soap.\nAnd the third respondent? She must have arrived while the Count was at\ndinner. For waiting on his bed was a light blue box with a single mille-\nfeuille.']","Count Rostov maintains his dignity under house arrest by committing to the business of practicalities, similar to Robinson Crusoe stranded on the Isle of Despair. He focuses on mastering his circumstances by attending to practical tasks, such as giving notes to messengers, receiving deliveries, and ensuring his comfort within the confines of the Hotel Metropol. He does not resort to thoughts of revenge or grandiose dreams, but instead focuses on the immediate tasks at hand to maintain his resolve and dignity.","Count Rostov maintains his dignity by adhering to routines, hosting small gatherings, and finding ways to adapt creatively to his circumstances.,",1.0,0.990412560401469,0.3333333333,1.0,0.0,0.48358054573919906
"What relationship does the Count have with the young girl in the hotel?,","['“To the privy . . .”\nThe Count looked up at the waiter and then at Sofia.\n“Say no more, Martyn.”\nThe waiter bowed and excused himself.\n“Sofia,” the Count suggested tentatively, “shall we visit the ladies’\nroom?”\nStill biting her lip, Sofia nodded.\n“Do you need me to . . . accompany you inside?” he asked, after leading\nher down the hallway.\nSofia shook her head and disappeared behind the washroom door.\nAs he waited, the Count chastised himself for his lug-headedness. Not\nonly had he failed to cut her meat and bring her to the ladies’ room, he\nclearly hadn’t thought to help her unpack, because she was wearing the\nexact same clothes she had worn the day before.\n“And you call yourself a waiter . . . ,” he said to himself.\nA moment later, Sofia emerged, looking relieved. But then, despite her\nreadily apparent love of interrogatives, she hesitated like one who is\nstruggling with whether to ask a question.\n“What is it, my dear? Is there something on your mind?”\nSofia struggled for another moment, then worked up the nerve:\n“Can we still have dessert, Uncle Alexander?”\nNow, it was the Count who looked relieved.\n“Without a doubt, my dear. Without a doubt.”', 'meeting, he assured her with a glance at his watch; unfortunately, he was\nexpected elsewhere. Then he made a beeline for the door.\nAs the young director crossed the lobby, the Count was of the opinion\nthat Anna looked every bit as radiant as she had in 1923. But the moment\nthe director disappeared into the street, the actress’s smile and shoulders\ndrooped. Then having passed a hand across her brow, she turned from the\ndoor—only to meet the gaze of the Count.\nIn an instant, she drew back her shoulders, raised her chin, and strolled\ntoward the staircase. But having mastered the art of descending the stairs to\na gathering of admirers, she had yet to master the art of ascending the stairs\nalone. (Perhaps no one has.) On the third step, she stopped. She stood\nmotionless. Then she turned, came back down, and crossed to where the\nCount was standing.\n“Whenever I am in this lobby with you,” she said, “it seems that I am\ndestined to be humiliated.”\nThe Count looked surprised.\n“Humiliated? You have no cause to feel humiliated, as far as I can see.”\n“I gather you’re blind.”\nShe looked toward the revolving door as if it were still spinning from\nthe young director’s exit.\n“I invited him for a nightcap. He said he had an early start.”\n“I have never had an early start in my life,” said the Count.\nOffering her first genuine smile of the evening, she waved a hand at the\nstairs.\n“Then you might as well come on up.”\nAt the time, Anna was staying in room 428. It was not the finest room\non the fourth floor, nor was it the worst. Off the small bedroom, it had a\nsmall sitting area with a small couch, a small coffee table, and two small\nwindows looking over the trolley tracks on Teatralny Proyezd. It was the\nroom of one who hoped to make an impression when she could not easily\nafford to do so. On the coffee table were two glasses, a serving of caviar,\nand a bottle of vodka in a bucket of melting ice.\nAs they looked over this little mise-en-scène, she shook her head.\n“That’ll cost me a pretty penny.”\n“Then we mustn’t let it go to waste.”\nThe Count drew the bottle from the ice and poured them both a glass.', 'He would not even have attempted to dissuade her.\nHow could he?\nThis was the very woman who, as a child herself, had crossed the Piazza\nwithout hesitation in order to become his friend; who had shown him the\nhidden corners of the hotel and bestowed upon him, quite literally, the key\nto its mysteries. When such a friend has sought one out to ask for aid—\nparticularly one for whom asking favors in a time of need does not come\nnaturally—then there is only one acceptable response.\nThe Count slipped the photograph in his pocket. He composed himself.\nThen he turned to find his new charge looking up at him.\n“Well, Sofia. Are you hungry? Would you like something to eat?”\nShe shook her head.\n“Then why don’t we head upstairs and get ourselves situated.”\nThe Count helped Sofia down from the chair and led her across the\nlobby. But as he was about to mount the stairs, he noticed her staring when\nthe elevator doors opened to let off two of the hotel’s guests.\n“Have you ever ridden on an elevator before?” he asked.\nGripping her doll by the neck, Sofia shook her head again.\n“In that case . . .”\nHolding the doors open, the Count gestured for Sofia to proceed. With\nan expression of cautious curiosity, she stepped onto the elevator, made\nroom for the Count, and then watched as the doors slid shut.\nWith a theatrical flourish and the command of Presto! the Count pressed\nthe button for the fifth floor. The elevator lurched and began to move. Sofia\nsteadied herself; then she leaned a little to her right so that she could watch\nthe floors pass by through the caging.\n“Voilà,” said the Count when they arrived a moment later at their\ndestination.\nLeading Sofia down the hall and into the belfry, the Count gestured\nagain for her to proceed. But having looked up the narrow twisting stair,\nSofia turned to the Count and raised both hands in the air in the\ninternational symbol of Pick me up.\n“Hmm,” said the Count. Then despite his age, he picked her up.\nShe yawned.\nOnce in his room, the Count sat Sofia on his bed, put her knapsack on\nthe Grand Duke’s desk, and then told her he would be right back. He']",The Count has a paternal relationship with the young girl in the hotel.,"The Count forms a bond with Nina, a curious young girl staying at the Metropol, who adds a sense of playfulness and intrigue to his life in confinement.,",1.0,0.9820564733233432,0.3333333333,0.0,0.3333333322222222,0.4720764641192359
"How is the Metropol Hotel a microcosm of Russian society?,","['all took their first tentative steps into Russian society. But if this was so, it\nwas largely due to the hive of activity beneath the parlor floors. For there,\njust a few steps below street level, were the butlers, cooks, and footmen\nwho together ensured that when the notions of Darwin or Manet were first\nbandied about, all went off without a hitch.\nAnd so it was in the Metropol.\nEver since its opening in 1905, the hotel’s suites and restaurants had\nbeen a gathering spot for the glamorous, influential, and erudite; but the\neffortless elegance on display would not have existed without the services\nof the lower floor:\nComing off the wide marble steps that descended from the lobby, one\nfirst passed the newsstand, which offered a gentleman a hundred headlines,\nalbeit now just in Russian.\nNext was the shop of Fatima Federova, the florist. A natural casualty of\nthe times, Fatima’s shelves had been emptied and her windows papered\nover back in 1920, turning one of the hotel’s brightest spots into one of its\nmost forlorn. But in its day, the shop had sold flowers by the acre. It had\nprovided the towering arrangements for the lobby, the lilies for the rooms,\nthe bouquets of roses that were tossed at the feet of the Bolshoi ballerinas,\nas well as the boutonnieres on the men who did the tossing. What’s more,\nFatima was fluent in the floral codes that had governed polite society since\nthe Age of Chivalry. Not only did she know the flower that should be sent\nas an apology, she knew which flower to send when one has been late;\nwhen one has spoken out of turn; and when, having taking notice of the\nyoung lady at the door, one has carelessly overtrumped one’s partner. In\nshort, Fatima knew a flower’s fragrance, color, and purpose better than a\nbee.\nWell, Fatima’s may have been shuttered, reflected the Count, but\nweren’t the flower shops of Paris shuttered under the “reign” of\nRobespierre, and didn’t that city now abound in blossoms? Just so, the time\nfor flowers in the Metropol would surely come again.\nAt the very end of the hall, one finally came to Yaroslav’s barbershop. A\nland of optimism, precision, and political neutrality, it was the Switzerland\nof the hotel. If the Count had vowed to master his circumstances through\npracticalities, then here was a glimpse of the means: a religiously kept\nappointment for a weekly trim.', 'expansion of the Criminal Code to allow the arrest of anyone even\ncountenancing dissension, these were only tidings, omens, underpinnings.\nAnd it would be a decade before their effects were fully felt.\nNo. For most of us, the late 1920s were not characterized by a series of\nmomentous events. Rather, the passage of those years was like the turn of a\nkaleidoscope.\nAt the bottom of a kaleidoscope’s cylinder lie shards of colored glass in\nrandom arrangement; but thanks to a glint of sunlight, the interplay of\nmirrors, and the magic of symmetry, when one peers inside what one finds\nis a pattern so colorful, so perfectly intricate, it seems certain to have been\ndesigned with the utmost care. Then by the slightest turn of the wrist, the\nshards begin to shift and settle into a new configuration—a configuration\nwith its own symmetry of shapes, its own intricacy of colors, its own hints\nof design.\nSo it was in the city of Moscow in the late 1920s.\nAnd so it was at the Metropol Hotel.\nIn fact, if a seasoned Muscovite were to cross Theatre Square on the last\nday of spring in 1930, he would find the hotel much as he remembered it.\nThere on the front steps still stands Pavel Ivanovich in his greatcoat\nlooking as stalwart as ever (though his hip now gives him some trouble on\nfoggy afternoons). On the other side of the revolving doors are the same\neager lads in the same blue caps ready to whisk one’s suitcases up the stairs\n(though they now answer to Grisha and Genya rather than Pasha and Petya).\nVasily, with his uncanny awareness of whereabouts, still mans the\nconcierge’s desk directly across from Arkady, who remains ready to spin\nthe register and offer you a pen. And in the manager’s office, Mr. Halecki\nstill sits behind his spotless desk (though a new assistant manager with the\nsmile of an ecclesiast is prone to interrupt his reveries over the slightest\ninfraction of the hotel’s rules).\nIn the Piazza, Russians cut from every cloth (or at least those who have\naccess to foreign currency) gather to linger over coffee and happen upon\nfriends. While in the ballroom, the weighty remarks and late arrivals that\nonce characterized the Assemblies now characterize Dinners of State', '“The way things stand, I’d be happy to design a birdhouse.”\nGiven the Count’s expression of curiosity, the young man elaborated:\n“For the time being, there are a lot of buildings being built in Moscow, but\nlittle need for architects. So I have taken a job with Intourist. They’re\nputting together a brochure of the city’s finer hotels and I’m drawing the\ninteriors.”*\n“Ah,” said the Count. “Because a photograph cannot capture the feeling\nof a place!”\n“Actually,” replied the architect, “because a photograph too readily\ncaptures the condition of a place.”\n“Oh, I see,” said the Count, feeling a little insulted on the Piazza’s\nbehalf. In its defense, he couldn’t help pointing out that while the restaurant\nhad been celebrated for its elegance in its time, the room’s grandeur had\nnever been defined by its furnishings or architectural details.\n“By what then?” the young man asked.\n“The citizenry.”\n“How do you mean?”\nThe Count turned his chair so that he could better face his neighbor.\n“In my day, I had the luxury of doing a good bit of travel. And I can tell\nyou from personal experience that the majority of hotel restaurants—not\nsimply in Russia, you understand, but across Europe—were designed for\nand have served the guests of the hotel. But this restaurant wasn’t and\nhasn’t. It was designed to be and has been a gathering place for the entire\ncity of Moscow.”\nThe Count gestured toward the center of the room.\n“For most of the last forty years, on a typical Saturday night you could\nfind Russians cut from every cloth crowded around that fountain, stumbling\ninto conversations with whosoever happened to be at the neighboring table.\nNaturally, this has led to impromptu romances and heartfelt debates on the\nmerits of Pushkin over Petrarch. Why, I’ve watched cabbies rub elbows\nwith commisars and bishops with black marketeers; and on at least one\noccasion, I have actually seen a young lady change an old man’s point of\nview.”\nThe Count pointed to a spot about twenty feet away.\n“You see those two tables there? One afternoon in 1939 I watched as\ntwo strangers, finding each other vaguely familiar, spent their appetizer,']","The Metropol Hotel is a microcosm of Russian society because it serves as a gathering spot for glamorous, influential, and erudite individuals. The lower floor of the hotel, where the butlers, cooks, and footmen work, ensures that the hotel runs smoothly and elegantly. The hotel's restaurant is not just for guests of the hotel, but for the entire city of Moscow, where people from all walks of life come together to socialize, debate, and form connections. The hotel represents a place where different segments of Russian society intersect and interact.","The Metropol Hotel reflects the societal changes and upheavals of early Soviet Russia, serving as a meeting place for people of different classes and ideologies.,",1.0,0.9750027336165948,0.9999999999666667,1.0,0.0,0.6616082154281651
"What does the Count’s daily routine reveal about his character?,","['knowing that a gentleman’s presence was best announced by his bearing,\nhis remarks, and his manners. Not by the cut of his coat.\nYes, thought the Count, the world does spin.\nIn fact, it spins on its axis even as it revolves around the sun. And the\ngalaxy turns as well, a wheel within a greater wheel, producing a chime of\nan entirely different nature than that of a tiny hammer in a clock. And when\nthat celestial chime sounds, perhaps a mirror will suddenly serve its truer\npurpose—revealing to a man not who he imagines himself to be, but who\nhe has become.\nThe Count resumed his place in the chair.\n“A clean shave,” he said to the barber. “A clean shave, my friend.”', 'As a young man, the Count would never have been inconvenienced by a\nfellow soul. He sought out congenial company the moment he awoke.\nWhen he had read in his chair, no interruption could be counted as a\ndisturbance. In fact, he preferred to read with a little racket in the\nbackground. Like the shouts of a vendor in the street; or the scales of a\npiano in a neighboring apartment; or best of all, footsteps on the stair—\nfootsteps that having quickly ascended two flights would suddenly stop,\nbang on his door, and breathlessly explain that two friends in a coach-and-\nfour were waiting at the curb. (After all, isn’t that why the pages of books\nare numbered? To facilitate the finding of one’s place after a reasonable\ninterruption?)\nAs to possessions, he hadn’t cared a whit about them. He was the first to\nlend a book or an umbrella to an acquaintance (never mind that no\nacquaintance since Adam had returned a book or an umbrella).\nAnd routines? He had prided himself on never having one. He would\nbreakfast at 10:00 A.M. one day and 2:00 P.M. the next. At his favorite\nrestaurants, he had never ordered the same dish twice in a season. Rather,\nhe traveled across their menus like Mr. Livingstone traveled across Africa\nand Magellan the seven seas.\nNo, at the age of twenty-two, Count Alexander Rostov could not be\ninconvenienced, interrupted, or unsettled. For every unexpected appearance,\ncomment, or turn of events had been welcomed like a burst of fireworks in\na summer sky—as something to be marveled at and cheered.\nBut apparently, this was no longer the case. . . .\nThe unanticipated arrival of a thirty-pound package had torn the veil\nfrom his eyes. Without his even noticing—without his acknowledgment,\ninput, or permission—routine had established itself within his daily life.\nApparently, he now ate his breakfast at an appointed hour. Apparently, he\nmust sip his coffee and nibble his biscuits without interruption. He must\nread in a particular chair tilted at a particular angle with no more than the\nscuffing of a pigeon’s feet to distract him. He must shave his right cheek,\nshave his left, and only then move on to the underside of his chin.\nTo that end, the Count now tilted back his head and raised his razor, but\nthe change in the angle of his gaze revealed two fathomless eyes staring\nback at him from the reflection in the mirror.\n“Egads!”', 'Later that night, as he sat alone on his bed, the Count mulled over his visit\nto his old suite.\nWhat had stayed with him was not the sight of his family’s clock still\nticking by the door, nor the grandeur of the architecture, nor even the view\nfrom the northwest window. What had lingered with him was the sight of\nthe tea service on the table beside the folded paper.\nThat little tableaux, for all its innocence, was somehow suggestive of\nexactly what had been bearing down on the Count’s soul. For he understood\nevery aspect of the scene at a glance. Having returned from some outing at\nfour o’clock and having hung his jacket on the back of a chair, the room’s\ncurrent resident had called for tea and an afternoon edition. Then he had\nsettled himself down on the couch to while away a civilized hour before it\nwas time to dress for dinner. In other words, what the Count had observed\nin suite 317 was not simply an afternoon tea, but a moment in the daily life\nof a gentleman at liberty.\nIn light of these thoughts, the Count reviewed his new room—the one\nhundred square feet that had been assigned to him. Never had it seemed so\nsmall. The bed crowded the coffee table, the coffee table crowded the high-\nback chair, and the high-back chair had to be shoved aside every time one\nwished to open the closet. Simply put, there was not enough space to\naccommodate such a civilized hour.\nBut as the Count gazed around him with this forlorn thought, a voice\nonly half his own reminded him that in the Metropol there were rooms\nbehind rooms, and doors behind doors. . . .\nRising from his bed, the Count navigated his way around his\ngrandmother’s coffee table, set aside the high-back chair, and stood before\nhis telephone box of a closet. Running along the perimeter of where the\ncloset met the wall was an elegant molding. The Count had always thought\nthis flourish a little excessive; but what if the closet had been built in an old\ndoorframe? Opening the door, the Count parted his clothes and tentatively\nrapped on the back wall. The sound was promisingly thin. With three\nfingers he gave the barrier a push and could feel it flex. He took all his\njackets out and dumped them on the bed. Then holding the jambs of the\ndoor he kicked the inner wall with his heel. There came a pleasant crack.\nLeaning back, he kicked again and again until the barrier splintered. Then\nhe pulled the jagged planks back into his room and slipped through the gap.']","The Count's daily routine reveals that he used to be carefree, spontaneous, and welcoming of interruptions and unexpected events. However, as he has aged, he has become more set in his ways and accustomed to routine, which has caused him to feel confined and restricted in his new living situation. This change in routine reflects a shift in his character from being open and flexible to being more rigid and controlled.","The Count's disciplined daily routine, including reading Montaigne and maintaining social interactions, highlights his resilience, adaptability, and philosophical outlook on life.,",,0.9187257956160645,0.49999999995,0.0,0.0,0.32432652156826175
"What is the significance of the wine Château de Baudelaire in the story?,","['Today, the dining room was nearly empty and the Count was being\nserved by someone who appeared not only new to the Piazza, but new to the\nart of waiting. Tall and thin, with a narrow head and superior demeanor, he\nlooked rather like a bishop that had been plucked from a chessboard. When\nthe Count took his seat with a newspaper in hand—the international symbol\nof dining alone—the chap didn’t bother to clear the second setting; when\nthe Count closed his menu and placed it beside his plate—the international\nsymbol of readiness to order—the chap needed to be beckoned with a wave\nof the hand; and when the Count ordered the okroshka and filet of sole, the\nchap asked if he might like a glass of Sauterne. A perfect suggestion, no\ndoubt, if only the Count had ordered foie gras!\n“Perhaps a bottle of the Château de Baudelaire,” the Count corrected\npolitely.\n“Of course,” the Bishop replied with an ecclesiastical smile.\nGranted, a bottle of Baudelaire was something of an extravagance for a\nsolitary lunch, but after spending another morning with the indefatigable\nMichel de Montaigne, the Count felt that his morale could use the boost.\nFor several days, in fact, he had been fending off a state of restlessness. On\nhis regular descent to the lobby, he caught himself counting the steps. As he\nbrowsed the headlines in his favorite chair, he found he was lifting his\nhands to twirl the tips of moustaches that were no longer there. He found he\nwas walking through the door of the Piazza at 12:01 for lunch. And at 1:35,\nwhen he climbed the 110 steps to his room, he was already calculating the\nminutes until he could come back downstairs for a drink. If he continued\nalong this course, it would not take long for the ceiling to edge downward,\nthe walls to edge inward, and the floor to edge upward, until the entire hotel\nhad been collapsed into the size of a biscuit tin.\nAs the Count waited for his wine, he gazed around the restaurant, but his\nfellow diners offered no relief. Across the way was a table occupied by two\nstragglers from the diplomatic corps who picked at their food while they\nawaited an era of diplomacy. Over there in the corner was a spectacled\ndenizen of the second floor with four enormous documents spread across\nhis table, comparing them word for word. No one appeared particularly gay;\nand no one paid the Count any mind. That is, except for the young girl with\nthe penchant for yellow who appeared to be spying on him from her table\nbehind the fountain.', 'express all the natural phenomena of its vintage. In a sip, it would evoke the\ntiming of that winter’s thaw, the extent of that summer’s rain, the prevailing\nwinds, and the frequency of clouds.\nYes, a bottle of wine was the ultimate distillation of time and place; a\npoetic expression of individuality itself. Yet here it was, cast back into the\nsea of anonymity, that realm of averages and unknowns.\nAnd suddenly, the Count had his own moment of lucidity. Just as\nMishka had come to understand the present as the natural by-product of the\npast, and could see with perfect clarity how it would shape the future, the\nCount now understood his place in the passage of time.\nAs we age, we are bound to find comfort from the notion that it takes\ngenerations for a way of life to fade. We are familiar with the songs our\ngrandparents favored, after all, even though we never danced to them\nourselves. At festive holidays, the recipes we pull from the drawer are\nroutinely decades old, and in some cases even written in the hand of a\nrelative long since dead. And the objects in our homes? The oriental coffee\ntables and well-worn desks that have been handed down from generation to\ngeneration? Despite being “out of fashion,” not only do they add beauty to\nour daily lives, they lend material credibility to our presumption that the\npassing of an era will be glacial.\nBut under certain circumstances, the Count finally acknowledged, this\nprocess can occur in the comparative blink of an eye. Popular upheaval,\npolitical turmoil, industrial progress—any combination of these can cause\nthe evolution of a society to leapfrog generations, sweeping aside aspects of\nthe past that might otherwise have lingered for decades. And this must be\nespecially so, when those with newfound power are men who distrust any\nform of hesitation or nuance, and who prize self-assurance above all.\nFor years now, with a bit of a smile, the Count had remarked that this or\nthat was behind him—like his days of poetry or travel or romance. But in so\ndoing, he had never really believed it. In his heart of hearts, he had\nimagined that, even if unattended to, these aspects of his life were lingering\nsomewhere on the periphery, waiting to be recalled. But looking at the\nbottle in his hand, the Count was struck by the realization that, in fact, it\nwas all behind him. Because the Bolsheviks, who were so intent upon\nrecasting the future from a mold of their own making, would not rest until\nevery last vestige of his Russia had been uprooted, shattered, or erased.', 'For the second time in an hour, the unshrugging Andrey shrugged.\n“A meeting was held, a vote was taken, an order was handed down. . . .\nHenceforth, the Boyarsky shall sell only red and white wine with every\nbottle at a single price.”\nWith a hand that was never meant to serve such a purpose, Andrey\ngestured to the corner, where beside five barrels of water a confusion of\nlabels lay on the floor. “It took ten men ten days to complete the task,” he\nsaid sadly.\n“But who on earth would file such a complaint?”\n“I am not certain; though I have been told it may have originated with\nyour friend. . . .”\n“My friend?”\n“Your waiter from downstairs.”\nThe Count looked at Andrey in amazement. But then a memory\npresented itself—a memory of a Christmas past when the Count had leaned\nfrom his chair to correct a certain waiter’s recommendation of a Rioja to\naccompany a Latvian stew. How smugly the Count had observed at the time\nthat there was no substitute for experience.\nWell, thought the Count, here is your substitute.\nWith Andrey a few paces behind him, the Count began walking the\ncellar’s center aisle, much as a commander and his lieutenant might walk\nthrough a field hospital in the aftermath of battle. Near the end of the aisle,\nthe Count turned down one of the rows. With a quick accounting of\ncolumns and shelves, the Count determined that in this row alone, there\nwere over a thousand bottles—a thousand bottles virtually identical in\nshape and weight.\nPicking up one at random, he reflected how perfectly the curve of the\nglass fit in the palm of the hand, how perfectly its volume weighed upon the\narm. But inside? Inside this dark green glass was what exactly? A\nChardonnay to complement a Camembert? A Sauvignon Blanc to go with\nsome chèvre?\nWhichever wine was within, it was decidedly not identical to its\nneighbors. On the contrary, the contents of the bottle in his hand was the\nproduct of a history as unique and complex as that of a nation, or a man. In\nits color, aroma, and taste, it would certainly express the idiosyncratic\ngeology and prevailing climate of its home terrain. But in addition, it would']","The significance of the wine Château de Baudelaire in the story is that it represents a moment of lucidity for the Count. It symbolizes the passage of time and the uniqueness of individuality, as well as the idea that certain aspects of life can be left behind due to societal changes and upheavals. The Count's choice of this particular wine reflects his realization that his past way of life in Russia is being erased by the Bolsheviks, who are reshaping the future according to their own ideals.","The Château de Baudelaire symbolizes the Count’s taste for the finer things in life and his desire to preserve small luxuries even in times of hardship.,",1.0,0.9899272810226479,0.99999999995,0.0,0.999999995,0.2299122601771429
"How does Count Rostov view the concept of possessions?,","['in misery, he was visited by his grandmother. Taking a seat at the foot of the\nbed, the Countess expressed a measure of sympathy: “There is nothing\npleasant to be said about losing,” she began, “and the Obolensky boy is a\npill. But, Sasha, my dear, why on earth would you give him the\nsatisfaction?” It was in this spirit that he and his grandmother parted\nwithout tears on the docks in Peterhof. Then the Count returned to the\nfamily estate in order to administer its shuttering.\nIn quick succession came the sweeping of chimneys, the clearing of\npantries, and the shrouding of furniture. It was just as if the family were\nreturning to St. Petersburg for the season, except that the dogs were released\nfrom their kennels, the horses from their stables, and the servants from their\nduties. Then, having filled a single wagon with some of the finest of the\nRostovs’ furniture, the Count bolted the doors and set out for Moscow.\n’Tis a funny thing, reflected the Count as he stood ready to abandon his\nsuite. From the earliest age, we must learn to say good-bye to friends and\nfamily. We see our parents and siblings off at the station; we visit cousins,\nattend schools, join the regiment; we marry, or travel abroad. It is part of the\nhuman experience that we are constantly gripping a good fellow by the\nshoulders and wishing him well, taking comfort from the notion that we\nwill hear word of him soon enough.\nBut experience is less likely to teach us how to bid our dearest\npossessions adieu. And if it were to? We wouldn’t welcome the education.\nFor eventually, we come to hold our dearest possessions more closely than\nwe hold our friends. We carry them from place to place, often at\nconsiderable expense and inconvenience; we dust and polish their surfaces\nand reprimand children for playing too roughly in their vicinity—all the\nwhile, allowing memories to invest them with greater and greater\nimportance. This armoire, we are prone to recall, is the very one in which\nwe hid as a boy; and it was these silver candelabra that lined our table on\nChristmas Eve; and it was with this handkerchief that she once dried her\ntears, et cetera, et cetera. Until we imagine that these carefully preserved\npossessions might give us genuine solace in the face of a lost companion.\nBut, of course, a thing is just a thing.\nAnd so, slipping his sister’s scissors into his pocket, the Count looked\nonce more at what heirlooms remained and then expunged them from his\nheartache forever.', 'Someone had shown the courtesy of having one of the Count’s traveling\ntrunks brought to his bedroom. So, as the bellhops carried the\naforementioned upward, the Count filled the trunk with clothes and\npersonal effects. Noting that the guards were eyeing the two bottles of\nbrandy on the console, the Count tossed them in as well. And once the trunk\nhad been carried upstairs, he finally pointed to the desk.\nThe two bellhops, their bright blue uniforms already smudged from their\nefforts, took hold of it by the corners.\n“But it weighs a ton,” said one to the other.\n“A king fortifies himself with a castle,” observed the Count, “a\ngentleman with a desk.”\nAs the bellhops lugged it into the hall, the Rostovs’ grandfather clock,\nwhich was fated to be left behind, tolled a doleful eight. The captain had\nlong since returned to his post and the guards, having swapped their\nbelligerence for boredom, now leaned against the wall and let the ashes\nfrom their cigarettes fall on the parquet floor while into the grand salon\npoured the undiminished light of the Moscow summer solstice.\nWith a wistful eye, the Count approached the windows at the suite’s\nnorthwest corner. How many hours had he spent before them? How many\nmornings dressed in his robe with his coffee in hand had he observed the\nnew arrivals from St. Petersburg disembarking from their cabs, worn and\nweary from the overnight train? On how many winter eves had he watched\nthe snow slowly descending as some lone silhouette, stocky and short,\npassed under a street lamp? At that very instant, at the square’s northern\nextreme a young Red Army officer rushed up the steps of the Bolshoi,\nhaving missed the first half hour of the evening’s performance.\nThe Count smiled to remember his own youthful preference for arriving\nentr’acte. Having insisted at the English Club that he could only stay for\none more drink, he stayed for three. Then leaping into the waiting carriage,\nhe’d flash across the city, vault the fabled steps, and like this young fellow\nslip through the golden doors. As the ballerinas danced gracefully across\nthe stage, the Count would be whispering his excusez-moi’s, making his\nway to his usual seat in the twentieth row with its privileged view of the\nladies in the loges.\nArriving late, thought the Count with a sigh. What a delicacy of youth.', 'In replying It is nothing to Andrey, the Count was not simply resorting to a\nGallic figure of speech. In point of fact, the Count deserved as much thanks\nfor his little intervention as a swallow deserves for its trill. For since the age\nof fifteen, Alexander Rostov had been a master of seating tables.\nWhenever he was home for the holidays, his grandmother would\ninevitably call him into the library, where she liked to knit by the fireplace\nin a solitary chair.\n“Come in, my boy, and sit with me a moment.”\n“Certainly, Grandmother,” replied the Count, balancing himself on the\nedge of the fire grate. “How can I be of assistance?”\n“The prelate is coming for dinner on Friday night—as are the Duchess\nObolensky, Count Keragin, and the Minsky-Polotovs. . . .”\nHere she would let her voice trail off without further explanation; but no\nfurther explanation was needed. The Countess was of a mind that dinner\nshould provide one with respite from life’s trials and tribulations. Thus, she\ncould not countenance discussions of religion, politics, or personal sorrows\nat her table. Further complicating matters, the prelate was deaf in his left\near, partial to Latin epigrams, and prone to stare at décolletage whenever he\ndrank a glass of wine; while the Duchess Obolensky, who was particularly\ncaustic in summer, frowned upon pithy sayings and could not abide\ndiscussions of the arts. And the Keragins? Their great-grandfather had been\ncalled a Bonapartist by Prince Minsky-Polotov in 1811, and they had not\nexchanged a word with a Minsky-Polotov since.\n“How many will be in attendance?” asked the Count.\n“Forty.”\n“The usual assembly?”\n“More or less.”\n“The Osipovs?”\n“Yes. But Pierre is in Moscow. . . .”\n“Ah,” said the Count with the smile of the chess champion who has been\nconfronted with a new gambit.\nThe Nizhny Novgorod Province had a hundred prominent families,\nwhich over the course of two centuries had intermarried and divorced,']","Count Rostov views possessions as items that hold sentimental value and memories, but ultimately recognizes that they are just things and not as important as friends or family. He reflects on how people hold onto possessions more closely than they do with friends, investing them with memories and importance. However, he ultimately decides to expunge his dearest possessions from his heartache forever, realizing that they are just material objects.","Count Rostov reflects on how possessions often carry sentimental value and memories, but ultimately sees them as replaceable compared to human connections.,",0.7142857142857143,0.9473847108588745,0.99999999995,1.0,0.0,0.7839587985216528
"What event disrupts the Count’s weekly visit to Yaroslav’s barbershop?,","['“Splendid, Yaroslav. At my utmost.”\n“And what is on the docket for today?”\n“Just a trim, my friend. Just a trim.”\nAs the scissors began their delicate snipping, it seemed to the Count that\nthe heavyset customer on the bench had undergone something of a\ntransformation. Although the Count had given his friendly nod just\nmoments before, in the interim the fellow’s face seemed to have taken on a\nrosier hue. The Count was sure of it, in fact, because the color was\nspreading to his ears.\nThe Count tried to make eye contact again, intending to offer another\nfriendly nod, but the fellow had fixed his gaze on Yaroslav’s back.\n“I was next,” he said.\nYaroslav, who like most artists tended to lose himself in his craft,\ncontinued clipping away with efficiency and grace. So, the fellow was\nforced to repeat himself, if a little more emphatically.\n“I was next.”\nDrawn from his artistic spell by the sharper intonation, Yaroslav offered\na courteous reply:\n“I will be with you in just a moment, sir.”\n“That is what you said when I arrived.”\nThis was said with such unmistakable hostility that Yaroslav paused in\nhis clipping and turned to meet his customer’s glare with a startled\nexpression.\nThough the Count had been raised never to interrupt a conversation, he\nfelt that the barber should not be put in the position of having to explain the\nsituation on his behalf. So, he interceded:\n“Yaroslav meant no offense, my good man. It just so happens that I have\na standing appointment at twelve o’clock on Tuesdays.”\nThe fellow now turned his glare upon the Count.\n“A standing appointment,” he repeated.\n“Yes.”\nThen he rose so abruptly that he knocked his bench back into the wall.\nAt full height, he was no more than five foot six. His fists, which jutted\nfrom the cuffs of his jacket, were as red as his ears. When he advanced a\nstep, Yaroslav backed against the edge of his counter. The fellow took\nanother step toward the barber and wrested one of the scissors from his', 'G\nAchilles Agonistes\nreetings, Arkady.”\n“Greetings to you, Count Rostov. Is there something I can do for\nyou this morning?”\n“If it wouldn’t be too much trouble, could you spare a bit of stationery?”\n“Certainly.”\nStanding at the front desk, the Count penned a one-sentence note under\nthe hotel’s moniker and addressed the envelope in an appropriately slanted\nscript; he waited until the bell captain was otherwise occupied, casually\ncrossed the lobby, slipped the note onto the bell captain’s desk, and then\nheaded downstairs for his weekly visit to the barber.\nIt had been many years since Yaroslav Yaroslavl had worked his magic\nin the barbershop of the Metropol, and in the interim any number of\nsuccessors had attempted to fill his shoes. The most recent fellow—Boris\nSomething-or-other-ovich—was perfectly qualified to shorten a man’s hair;\nbut he was neither the artist nor the conversationalist that Yaroslav had\nbeen. In fact, he went about his business with such mute efficiency, one\nsuspected he was part machine.\n“Trim?” he asked the Count, wasting no time with subjects, verbs, or the\nother superfluities of language.\nGiven the Count’s thinning hair and the barber’s predisposition to\nefficiency, a trim might take all of ten minutes.\n“Yes, a trim,” said the Count. “But perhaps a shave as well. . . .”\nThe barber furrowed his brow. The man in him, no doubt, was inclined\nto point out that the Count had obviously shaved a few hours before; but the\nmachinery in him was so finely tuned, it was already putting down the\nscissors and reaching for the shaving brush.\nHaving whipped a sufficient lather, Boris dabbed it on those areas of the\nCount’s face where whiskers would have been had the Count been in need\nof a shave. He sharpened one of his razors on his strop, leaned over the', 'When the Count entered the shop, Yaroslav was attending to a silver-haired\ncustomer in a light gray suit while a heavyset fellow in a rumpled jacket\nbided his time on the bench by the wall. Greeting the Count with a smile,\nthe barber directed him to the empty chair at his side.\nAs the Count climbed into the chair, he offered a friendly nod to the\nheavyset fellow, then leaned back and let his eyes settle on that marvel of\nYaroslav’s shop: his cabinet. Were one to ask Larousse to define the word\ncabinet, the acclaimed lexicographer might reply: A piece of furniture often\nadorned with decorative detail in which items may be stowed away from\nsight. A serviceable definition, no doubt—one that would encompass\neverything from a kitchen cupboard in the countryside to a Chippendale in\nBuckingham Palace. But Yaroslav’s cabinet would not fit so neatly into\nsuch a description, for having been made solely of nickel and glass it had\nbeen designed not to hide its contents, but to reveal them to the naked eye.\nAnd rightfully so. For this cabinet could be proud of all it contained:\nFrench soaps wrapped in waxed papers; British lathers in ivory drums;\nItalian tonics in whimsically shaped vials. And hidden in the back? That\nlittle black bottle that Yaroslav referred to with a wink as the Fountain of\nYouth.\nIn the mirror’s reflection, the Count now let his gaze shift to where\nYaroslav was working his magic on the silver-haired gentleman with two\nsets of scissors simultaneously. In Yaroslav’s hands, the scissors initially\nrecalled the entrechat of the danseur in a ballet, his legs switching back and\nforth in midair. But as the barber progressed, his hands moved with\nincreasing speed until they leapt and kicked like a Cossack doing the\nhopak! Upon the execution of the final snip, it would have been perfectly\nappropriate for a curtain to drop only to be raised again a moment later so\nthat the audience could applaud as the barber took a bow.\nYaroslav swung the white cape off his customer and snapped it in the\nair; he clicked his heels when accepting payment for a job well done; and as\nthe gentleman exited the shop (looking younger and more distinguished\nthan when he’d arrived), the barber approached the Count with a fresh cape.\n“Your Excellency. How are you?”']",The heavyset customer becoming hostile and demanding to be next disrupts the Count's weekly visit to Yaroslav's barbershop.,"A stranger waiting for service reacts angrily to the Count’s standing appointment, resulting in an altercation where the Count's mustache is partially cut.,",1.0,0.9032056873530925,0.99999999995,0.0,0.3333333322222222,0.40989015703504306
"How does the Count react to the stranger cutting his mustache?,","['hand. Then, with the deftness of a much slighter man, he turned, took the\nCount by the collar, and severed the right wing of his moustaches with a\nsingle snip. Tightening his hold, he pulled the Count forward until they\nwere nearly nose to nose.\n“You’ll have your appointment soon enough,” he said.\nThen shoving the Count back in the chair, he tossed the scissors on the\nfloor and strolled from the shop.\n“Your Excellency,” exclaimed Yaroslav, aghast. “I have never seen the\nman in my life. I don’t even know if he resides in the hotel. But he is not\nwelcome here again, I assure you of that.”\nThe Count, who was standing now, was inclined to echo Yaroslav’s\nindignation and commend a punishment that fit the crime. But then, what\ndid the Count know about his assailant?\nWhen he had first seen him sitting on the bench in his rumpled jacket,\nthe Count had summed him up in an instant as some hardworking sort who,\nhaving stumbled upon the barbershop, had decided to treat himself to a cut.\nBut for all the Count knew, this fellow could have been one of the new\nresidents of the second floor. Having come of age in an ironworks, he could\nhave joined a union in 1912, led a strike in 1916, captained a Red battalion\nin 1918, and now found himself in command of an entire industry.\n“He was perfectly right,” the Count said to Yaroslav. “He had been\nwaiting in good faith. You only wished to honor my appointment. It was for\nme to cede the chair and suggest that you attend to him first.”\n“But what are we to do?”\nThe Count turned to the mirror and surveyed himself. He surveyed\nhimself, perhaps, for the first time in years.\nLong had he believed that a gentleman should turn to a mirror with a\nsense of distrust. For rather than being tools of self-discovery, mirrors\ntended to be tools of self-deceit. How many times had he watched as a\nyoung beauty turned thirty degrees before her mirror to ensure that she saw\nherself to the best advantage? (As if henceforth all the world would see her\nsolely from that angle!) How often had he seen a grande dame don a hat\nthat was horribly out of fashion, but that seemed au courant to her because\nher mirror had been framed in the style of the same bygone era? The Count\ntook pride in wearing a well-tailored jacket; but he took greater pride in', 'chair, and with an unflinching hand shaved the Count’s right upper cheek in\na single pass. Wiping the blade on the towel at his waist, he then leaned\nover the Count’s left upper cheek, and shaved it with equal alacrity.\nAt this rate, fretted the Count, he’ll be done in a minute and a half.\nUsing a bent knuckle, the barber now raised the Count’s chin. The Count\ncould feel the metal of the razor make contact with his throat. And that’s\nwhen one of the new bellhops appeared in the door.\n“Excuse me, sir.”\n“Yes?” said the barber with his blade held fast at the Count’s jugular.\n“I have a note for you.”\n“On the bench.”\n“But it is urgent,” said the young man with some anxiety.\n“Urgent?”\n“Yes, sir. From the manager.”\nThe barber looked back at the bellhop for the first time.\n“The manager?”\n“Yes, sir.”\nAfter an extended exhalation, the barber removed the blade from the\nCount’s throat, accepted the missive, and—as the bellhop disappeared down\nthe hall—slit the envelope open with his razor.\nUnfolding the note, the barber stared at it for a full minute. In those sixty\nseconds, he must have read it ten times over because it was composed of\nonly four words: Come see me immediately!\nThe barber exhaled again then looked at the wall.\n“I can’t imagine,” he said to no one. Then having thought it over for\nanother minute, he turned to the Count: “I must see to something.”\n“By all means. Do what you must. I am in no hurry.”\nTo underscore his point, the Count leaned back his head and closed his\neyes as if to nap; but when the barber’s footsteps had receded down the hall,\nthe Count leapt from the chair like a cat.\nWhen the Count was a young man, he prided himself on the fact that he was\nunmoved by the ticking of the clock. In the early years of the twentieth\ncentury, there were those of his acquaintance who brought a new sense of', 'result of a glancing blow that had hoped to cleave his skull. Apparently\nunhurried, the man was playing with his spoon.\n“Good evening,” said the Count with a bow.\n“Good evening,” replied the man with a smile, returning his spoon to the\ntable.\n“May I bring you something to drink while you wait?”\n“There will be no one else coming.”\n“Ah,” said the Count. He began clearing the second place setting.\n“You needn’t clear those.”\n“I’m sorry. I thought you weren’t expecting anyone else.”\n“I am not expecting anyone else. I am expecting you, Alexander Ilyich.”\nThe two men studied each other for a moment.\n“Please,” said the man. “Have a seat.”\nThe Count hesitated to take the offered chair.\nUnder the circumstances, one might leap to the conclusion that the\nCount hesitated due to a suspicion or even dread about this stranger. But\nprincipally, he hesitated because as a matter of decorum, it seemed utterly\ninappropriate for one to sit at a table when one is dressed to wait upon it.\n“Come now,” the stranger said amiably. “You wouldn’t refuse a solitary\ndiner the pleasure of your company.”\n“Certainly not,” replied the Count.\nBut having accepted the chair, he did not place the napkin in his lap.\nAfter a rap at the door, it opened to admit the Goliath. Without looking\nat the Count, he approached the table and held out a bottle for the stranger’s\nconsideration.\nThe host leaned forward and squinted at the label.\n“Excellent,” he said. “Thank you, Vladimir.”\nPresumably, Vladimir could simply have broken the top off the bottle,\nbut with surprising agility he produced a corkscrew from a pocket, spun it\nin his hand, and pulled the cork. Then, having received a nod from his\nsuperior, he placed the open bottle on the table and retreated back to the\nhall. The stranger poured a glass for himself. Then, with the bottle hovering\nover the table at a forty-five-degree angle, he looked to the Count.\n“Won’t you join me?”\n“With pleasure.”\nAfter the stranger poured, they both raised their glasses and drank.']","The Count is initially shocked and indignant when the stranger cuts his mustache, but then he realizes that he was in the wrong for not offering the chair to the stranger first. He ultimately accepts the situation and does not hold a grudge against the stranger.","The Count chooses to take responsibility for the situation, understanding the stranger's frustrations, and later requests a clean shave from Yaroslav.,",0.5,0.9935225895029353,0.9999999999,0.5,0.999999995,0.6775326990214214
"How does the story depict the changing role of aristocracy in Soviet Russia?,","['*In those early years of the Soviet Union, how did the Bolsheviks countenance the idea of gilded\nchairs and Louis Quatorze dressers in the mansions of starlets? For that matter, how did they stomach\nthem in their own apartments? Simple. Nailed to the bottom of every piece of fine furniture was a\nsmall copper plate embossed with a number. This number served to identify the piece as part of the\nvast inventory of the People. Thus, a good Bolshevik could sleep soundly in the knowledge that the\nmahogany bed he was lying on was not his; and despite the fact that his apartment was furnished with\npriceless antiques, he had fewer possessions than a pauper!', 'L\n1938\nAn Arrival\net us concede that the early thirties in Russia were unkind.\nIn addition to starvation in the countryside, the famine of ’32\neventually led to a migration of peasants to the cities, which, in turn,\ncontributed to overcrowded housing, shortages of essential goods, even\nhooliganism. At the same time, the most stalwart workers in the urban\ncenters were wearying under the burden of the continuous workweek;\nartists faced tighter constraints on what they could or could not imagine;\nchurches were shuttered, repurposed, or razed; and when revolutionary hero\nSergei Kirov was assassinated, the nation was purged of an array of\npolitically unreliable elements.\nBut then, on the seventeenth of November 1935, at the First All-Union\nConference of Stakhanovites, Stalin himself declared: Life has improved,\ncomrades. Life is more joyous. . . .\nYes, generally speaking such a remark falling from the lips of a\nstatesman should be swept from the floor with the dust and the lint. But\nwhen it fell from the lips of Soso, one had good reason to lend it credence.\nFor it was often through secondary remarks in secondary speeches that the\nGeneral Secretary of the Central Committee of the Communist Party\nsignaled the shifts in his thinking.\nIn point of fact, a few days before giving this speech, Soso had seen a\nphotograph in the Herald Tribune of three healthy young Bolshevik girls\nstanding before a factory gate—dressed in the tunic and kerchief long\nfavored by the Party. Normally, such a picture would have warmed the\ncockles of his heart. But in the context of the Western press, it struck the\nSecretary of Secretaries that this simple attire might suggest to the world\nthat after eighteen years of Communism, Russian girls still lived like', 'H\nArachne’s Art\nistory is the business of identifying momentous events from the\ncomfort of a high-back chair. With the benefit of time, the historian\nlooks back and points to a date in the manner of a gray-haired field marshal\npointing to a bend in a river on a map: There it was, he says. The turning\npoint. The decisive factor. The fateful day that fundamentally altered all that\nwas to follow.\nThere on the third of January 1928, the historians tell us, was the launch\nof the First Five-Year Plan—that initiative which would begin the\ntransformation of Russia from a nineteenth-century agrarian society into a\ntwentieth-century industrial power. There on the seventeenth of November\n1929, Nikolai Bukharin, founding father, editor of Pravda, and last true\nfriend of the peasant, was outmaneuvered by Stalin and ousted from the\nPolitburo—clearing the way for a return to autocracy in all but name. And\nthere on the twenty-fifth of February 1927, was the drafting of Article 58 of\nthe Criminal Code—the net that would eventually ensnare us all.\nThere on the twenty-seventh of May, or there on the sixth of December;\nat eight or nine in the morning.\nThere it was, they say. As if—like at the opera—a curtain has closed, a\nlever has been pulled, one set has been whisked to the rafters and another\nhas dropped to the stage, such that when the curtain opens a moment later\nthe audience will find itself transported from a richly appointed ballroom to\nthe banks of a wooded stream. . . .\nBut the events that transpired on those various dates did not throw the\ncity of Moscow into upheaval. When the page was torn from the calendar,\nthe bedroom windows did not suddenly shine with the light of a million\nelectric lamps; that Fatherly gaze did not suddenly hang over every desk\nand appear in every dream; nor did the drivers of a hundred Black Marias\nturn the keys in their ignitions and fan out into the shadowy streets. For the\nlaunch of the First Five-Year Plan, Bukharin’s fall from grace, and the']","The story depicts the changing role of aristocracy in Soviet Russia by showing how the Bolsheviks implemented a system where fine furniture was marked with copper plates to identify them as part of the People's inventory. This system allowed even the Bolsheviks themselves to have fewer possessions than a pauper, indicating a shift towards equality and the devaluing of aristocratic possessions in Soviet Russia.","Through the Count’s experiences, the story explores the decline of the aristocracy and their adaptation—or lack thereof—to the new Soviet social order.,",0.5,0.9899544561565792,0.8333333332916666,1.0,0.0,0.5947964456834485
"What role does the poem - Where Is It Now? - play during Count Rostov’s trial?,","['Rostov: By that point, I’m afraid that my days of taking up arms were behind me.\nVyshinsky: Why then did you come back?\nRostov: I missed the climate.\n[Laughter.]\nVyshinsky: Count Rostov, you do not seem to appreciate the gravity of your position.\nNor do you show the respect that is due the men convened before you.\nRostov: The Tsarina had the same complaints about me in her day.\nIgnatov: Prosecutor Vyshinsky. If I may . . .\nVyshinsky: Secretary Ignatov.\nIgnatov: I have no doubt, Count Rostov, that many in the gallery are surprised to\nfind you so charming; but I, for one, am not surprised in the least. History has\nshown charm to be the final ambition of the leisure class. What I do find\nsurprising is that the author of the poem in question could have become a man so\nobviously without purpose.\nRostov: I have lived under the impression that a man’s purpose is known only to God.\nIgnatov: Indeed. How convenient that must have been for you.\n[The Committee recesses for twelve minutes.]\nIgnatov: Alexander Ilyich Rostov, taking into full account your own testimony, we\ncan only assume that the clear-eyed spirit who wrote the poem Where Is It Now? has\nsuccumbed irrevocably to the corruptions of his class—and now poses a threat to\nthe very ideals he once espoused. On that basis, our inclination would be to have\nyou taken from this chamber and put against the wall. But there are those within\nthe senior ranks of the Party who count you among the heroes of the\nprerevolutionary cause. Thus, it is the opinion of this committee that you should\nbe returned to that hotel of which you are so fond. But make no mistake: should\nyou ever set foot outside of the Metropol again, you will be shot. Next matter.\nBearing the signatures of\nV. A. Ignatov\nM. S. Zakovsky\nA. N. Kosarev', 'How well I remember\nWhen it came as a visitor on foot\nAnd dwelt a while amongst us\nA melody in the semblance of a mountain cat.\nWell, where is our purpose now?\nLike so many questions\nI answer this one\nWith the eye-averted peeling of a pear.\nWith a bow I bid goodnight\nAnd pass through terrace doors\nInto the simple splendors\nOf another temperate spring;\nBut this much I know:\nIt is not lost among the autumn leaves on Peter’s Square.\nIt is not among the ashes in the Athenaeum ash cans.\nIt is not inside the blue pagodas of your fine Chinoiserie.\nIt is not in Vronsky’s saddlebags;\nNot in Sonnet XXX, stanza one;\nNot on twenty-seven red . . .\nWhere Is It Now? (Lines 1–19)\nCount Alexander Ilyich Rostov\n1913', '“What with the revolt of 1905 and the repressions that followed, when\nwe graduated it was still a dangerous time for writing poems of political\nimpatience. Given Mishka’s background, the Okhrana would have swept\nhim up with a broom. So one night—after polishing off a particularly good\nbottle of Margaux—we decided to publish the poem under my name.”\n“But why yours?”\n“What were they going to do to Count Alexander Rostov—member of\nthe Jockey Club and godson of a counselor to the Tsar?” The Count shook\nhis head. “The irony, of course, is that the life which ended up being saved\nwas mine, not his. But for that poem, they would have shot me back in\n1922.”\nKaterina, who had listened to this story intently, was suddenly holding\nback tears.\n“Ah, but there you have him,” she said.\nThey were both silent as she regained her composure.\n“I want you to know,” said the Count, “how much I appreciate your\ncoming to tell me in person.” But Katerina dismissed his gratitude.\n“I came at Mikhail’s request. He asked me to bring you something.”\nFrom her satchel she took out a rectangular package wrapped in plain\nbrown paper and tied with twine.\nTaking the package in hand, the Count could tell from its weight that it\nwas a book.\n“It is his project,” said the Count with a smile.\n“Yes,” she said. Then she added with pointed emphasis: “He slaved over\nit.”\nThe Count nodded to express his understanding and to assure Katerina\nthat he did not take the bestowal lightly.\nKaterina looked once more around the room with a light shake of the\nhead as if it somehow exemplified the mystery of outcomes; then she said\nthat she should go.\nThe Count rose to his feet with her, setting Mishka’s project on the chair.\n“Are you going back to Yavas?” he asked.\n“No.”\n“Will you be staying in Moscow?”\n“No.”\n“Where then?”']","The poem ""Where Is It Now?"" is used as evidence during Count Rostov's trial to show that he has succumbed to the corruptions of his class and now poses a threat to the ideals he once espoused. It is mentioned that the clear-eyed spirit who wrote the poem has become a man without purpose, which is used against Count Rostov during the trial.","The poem is used by the prosecutor to suggest the Count’s revolutionary sympathies and potential as a political threat, influencing the decision to sentence him to house arrest.,",0.6,0.9648735493691504,0.8333333332916666,1.0,0.0,0.4854868938695942
"How does the Count view his life at the Metropol compared to his past?,","['Later that night, as he sat alone on his bed, the Count mulled over his visit\nto his old suite.\nWhat had stayed with him was not the sight of his family’s clock still\nticking by the door, nor the grandeur of the architecture, nor even the view\nfrom the northwest window. What had lingered with him was the sight of\nthe tea service on the table beside the folded paper.\nThat little tableaux, for all its innocence, was somehow suggestive of\nexactly what had been bearing down on the Count’s soul. For he understood\nevery aspect of the scene at a glance. Having returned from some outing at\nfour o’clock and having hung his jacket on the back of a chair, the room’s\ncurrent resident had called for tea and an afternoon edition. Then he had\nsettled himself down on the couch to while away a civilized hour before it\nwas time to dress for dinner. In other words, what the Count had observed\nin suite 317 was not simply an afternoon tea, but a moment in the daily life\nof a gentleman at liberty.\nIn light of these thoughts, the Count reviewed his new room—the one\nhundred square feet that had been assigned to him. Never had it seemed so\nsmall. The bed crowded the coffee table, the coffee table crowded the high-\nback chair, and the high-back chair had to be shoved aside every time one\nwished to open the closet. Simply put, there was not enough space to\naccommodate such a civilized hour.\nBut as the Count gazed around him with this forlorn thought, a voice\nonly half his own reminded him that in the Metropol there were rooms\nbehind rooms, and doors behind doors. . . .\nRising from his bed, the Count navigated his way around his\ngrandmother’s coffee table, set aside the high-back chair, and stood before\nhis telephone box of a closet. Running along the perimeter of where the\ncloset met the wall was an elegant molding. The Count had always thought\nthis flourish a little excessive; but what if the closet had been built in an old\ndoorframe? Opening the door, the Count parted his clothes and tentatively\nrapped on the back wall. The sound was promisingly thin. With three\nfingers he gave the barrier a push and could feel it flex. He took all his\njackets out and dumped them on the bed. Then holding the jambs of the\ndoor he kicked the inner wall with his heel. There came a pleasant crack.\nLeaning back, he kicked again and again until the barrier splintered. Then\nhe pulled the jagged planks back into his room and slipped through the gap.', '“Meet me here,” she said, “tomorrow at two.”\nAnd thus began the Count’s education.\nHaving lived at the Metropol for four years, the Count considered himself\nsomething of an expert on the hotel. He knew its staff by name, its services\nby experience, and the decorative styles of its suites by heart. But once Nina\nhad taken him in hand, he realized what a novice he had been.\nIn the ten months that Nina had lived at the Metropol, she had been\nconfronted with her own version of confinement. For, as her father had been\nposted only “temporarily” to Moscow, he had not bothered to enroll her in\nschool. And as Nina’s governess still had one foot set firmly in the\nhinterlands, she preferred that her charge remain on the hotel’s premises\nwhere she was less likely to be corrupted by street lamps and trolley cars.\nSo, if the door of the Metropol was known the world over for spinning\nwithout stop, it spun not for Nina. But, an enterprising and tireless spirit, the\nyoung lady had made the most of her situation by personally investigating\nthe hotel until she knew every room, its purpose, and how it might be put to\nbetter use.\nYes, the Count had gone to the little window at the back of the lobby to\nask for his mail, but had he been to the sorting room where the incoming\nenvelopes were spilled on a table at ten and at two—including those that\nwere stamped in red with the unambiguous instruction For Immediate\nDelivery?\nAnd yes, he had visited Fatima’s in the days when it was open, but had\nhe been inside the cutting room? Through a narrow door at the back of her\nshop was that niche with a light green counter where stems had been\nsnipped and roses dethorned, where even now one could find scattered\nacross the floor the dried petals of ten perennials essential to the making of\npotions.\nOf course, exclaimed the Count to himself. Within the Metropol there\nwere rooms behind rooms and doors behind doors. The linen closets. The\nlaundries. The pantries. The switchboard!\nIt was like sailing on a steamship. Having enjoyed an afternoon shooting\nclay pigeons off the starboard bow, a passenger dresses for dinner, dines at', 'finer buildings had been lifted and set back, most had been razed and\nreplaced with towers, in accordance with a new ordinance that buildings on\nfirst-rate streets stand at least ten stories tall. As a result, the Count would\nhave had to strain to pick out other familiar landmarks as the van moved\nalong. But he had stopped looking for what was familiar, and instead was\nwatching the blur of facades and street lamps receding rapidly from his\nview, as if they were being pulled into the distance.\nBack in the attic of the Metropol, the Count found his door still open and\nMontaigne on the floor. Picking up his father’s book, the Count sat down on\nSofia’s bed. Then for the first time that night, he let himself weep, his chest\nheaving lightly with the release. But if tears fell freely down his face, they\nwere not tears of grief. They were the tears of the luckiest man in all of\nRussia.\nAfter a few minutes, the Count breathed deeply and felt a sense of\npeace. Realizing that his father’s book was still in his hand, he rose from\nSofia’s bed to set it down—and that’s when he saw the black leather case\nthat had been left on the Grand Duke’s desk. It was about a foot square and\nsix inches high with a handle in leather and clasps of chrome. Taped on top\nwas a note addressed to him in an unfamiliar script. Pulling the note free,\nthe Count unfolded it and read:\nAlexander,\nWhat a pleasure meeting you tonight. As I mentioned, I am\nheaded home for a spell. In the meantime, I thought you could make\ngood use of this. You might pay special attention to the contents in\nthe uppermost sleeve, as I think you will find it very apropos of our\nchat.\nWith warm regards till next we meet, \nRichard Vanderwhile\nThrowing the clasps, the Count opened the lid of the case. It was a\nportable phonograph. Inside there was a small stack of records in brown']","The Count views his life at the Metropol as fortunate and lucky, despite the changes in his circumstances and the restrictions he faces. He feels a sense of peace and gratitude, considering himself the luckiest man in all of Russia.","The Count sees his life at the Metropol as a significant adjustment but maintains a sense of purpose by embracing the routines and relationships within the hotel.,",0.8333333333333334,0.9594245727601516,0.9999999999666667,1.0,0.6666666644444444,0.22696963732390443
"How does the Count embody resilience in the face of adversity?,","['Having acknowledged that a man must master his circumstances or\notherwise be mastered by them, the Count thought it worth considering how\none was most likely to achieve this aim when one had been sentenced to a\nlife of confinement.\nFor Edmond Dantès in the Château d’If, it was thoughts of revenge that\nkept him clear minded. Unjustly imprisoned, he sustained himself by\nplotting the systematic undoing of his personal agents of villainy. For\nCervantes, enslaved by pirates in Algiers, it was the promise of pages as yet\nunwritten that spurred him on. While for Napoleon on Elba, strolling among\nchickens, fending off flies, and sidestepping puddles of mud, it was visions\nof a triumphal return to Paris that galvanized his will to persevere.\nBut the Count hadn’t the temperament for revenge; he hadn’t the\nimagination for epics; and he certainly hadn’t the fanciful ego to dream of\nempires restored. No. His model for mastering his circumstances would be\na different sort of captive altogether: an Anglican washed ashore. Like\nRobinson Crusoe stranded on the Isle of Despair, the Count would maintain\nhis resolve by committing to the business of practicalities. Having\ndispensed with dreams of quick discovery, the world’s Crusoes seek shelter\nand a source of fresh water; they teach themselves to make fire from flint;\nthey study their island’s topography, its climate, its flora and fauna, all the\nwhile keeping their eyes trained for sails on the horizon and footprints in\nthe sand.\nIt was to this end that the Count had given the old Greek three notes to\ndeliver. Within a matter of hours, the Count had been visited by two\nmessengers: a young lad from Muir & Mirrielees bearing fine linens and a\nsuitable pillow; and another from Petrovsky Passage with four bars of the\nCount’s favorite soap.\nAnd the third respondent? She must have arrived while the Count was at\ndinner. For waiting on his bed was a light blue box with a single mille-\nfeuille.', 'epaulettes, the Grand Duke spoke four languages and read six. Never to\nwed, he represented his country at Portsmouth, managed three estates, and\ngenerally prized industry over nonsense. But before all of that, he had\nserved alongside the Count’s father as a devil-may-care cadet in the cavalry.\nThus had the Grand Duke become the Count’s watchful guardian. And\nwhen the Count’s parents succumbed to cholera within hours of each other\nin 1900, it was the Grand Duke who took the young Count aside and\nexplained that he must be strong for his sister’s sake; that adversity presents\nitself in many forms; and that if a man does not master his circumstances\nthen he is bound to be mastered by them.\nThe Count ran his hand across the desk’s dimpled surface.\nHow many of the Grand Duke’s words did those faint indentations\nreflect? Here over forty years had been written concise instructions to\ncaretakers; persuasive arguments to statesmen; exquisite counsel to friends.\nIn other words, it was a desk to be reckoned with.\nEmptying his glass, the Count pushed his chair back and sat on the floor.\nHe ran his hand behind the desk’s right front leg until he found the catch.\nWhen he pressed it, a seamless door opened to reveal a velvet-lined hollow\nthat, like the hollows in the other three legs, was stacked with pieces of\ngold.', 'entering the academy; and Nadya, dear Nadya, will soon be of an age to\nmarry. . . .\nBut just as important, a careful accounting of days allows the isolated to\nnote that another year of hardship has been endured; survived; bested.\nWhether they have found the strength to persevere through a tireless\ndetermination or some foolhardy optimism, those 365 hatch marks stand as\nproof of their indomitability. For after all, if attentiveness should be\nmeasured in minutes and discipline measured in hours, then indomitability\nmust be measured in years. Or, if philosophical investigations are not to\nyour taste, then let us simply agree that the wise man celebrates what he\ncan.\nThus, the Count donned his finest smoking jacket (custom-made in Paris\nfrom a burgundy velvet) and headed down the stairs.\nWhen the Count reached the lobby, before he could continue to the\nbarbershop his eyes were drawn to a willowy figure coming through the\nhotel’s doors. But then all eyes in the lobby were drawn to her. A tall\nwoman in her midtwenties with arched eyebrows and auburn hair, she was\nindisputably striking. And as she approached the front desk, she walked\nwith a breezy sureness as seemingly unaware of the feathers projecting\nfrom her hat as of the bellhops dragging her luggage behind her. But what\nguaranteed her position as the natural center of attention were the two\nborzois she had on leash.\nIn an instant the Count could see that they were magnificent beasts.\nTheir coats silver, their loins lean, their every sense alert, these dogs had\nbeen raised to give chase in the cold October air with a hunting party hot on\ntheir heels. And at day’s end? They were meant to sit at the feet of their\nmaster before a fire in a manor house—not adorn the hands of a willow in\nthe lobby of a grand hotel. . . .\nThe injustice of this was not lost on the dogs. As their mistress\naddressed Arkady at the front desk, they tugged every which way, sniffing\nabout for familiar landmarks.\n“Stop it!” the willow commanded in a surprisingly husky voice. Then\nshe yanked in a manner that showed she had no more familiarity with the']","The Count embodies resilience in the face of adversity by committing to practicalities and maintaining his resolve through perseverance and attentiveness. He does not give in to revenge, grandiose dreams, or fanciful ego, but instead focuses on practical tasks and careful planning to master his circumstances. He also draws strength from the wise words of his guardian, the Grand Duke, who taught him the importance of strength and perseverance in the face of adversity.","The Count embodies resilience by maintaining his charm, wit, and cultural pursuits despite his confined circumstances, finding meaning and community within the hotel.,",1.0,0.9755597792447265,0.9999999999666667,0.0,0.4999999975,0.3241618301881709
"What was the sentence given to Count Alexander Ilyich Rostov by the committee?,","['21 June 1922\nAPPEARANCE OF COUNT ALEXANDER ILYICH ROSTOV\nBEFORE THE EMERGENCY COMMITTEE OF THE PEOPLE’S COMMISSARIAT FOR INTERNAL AFFAIRS\nPresiding: Comrades V. A. Ignatov, M. S. Zakovsky, A. N. Kosarev\nProsecuting: A. Y. Vyshinsky\nProsecutor Vyshinsky: State your name.\nRostov: Count Alexander Ilyich Rostov, recipient of the Order of Saint Andrew,\nmember of the Jockey Club, Master of the Hunt.\nVyshinsky: You may have your titles; they are of no use to anyone else. But for the\nrecord, are you not Alexander Rostov, born in St. Petersburg, 24 October 1889?\nRostov: I am he.\nVyshinsky: Before we begin, I must say, I do not think that I have ever seen a\njacket festooned with so many buttons.\nRostov: Thank you.\nVyshinsky: It was not meant as a compliment.\nRostov: In that case, I demand satisfaction on the field of honor.\n[Laughter.]\nSecretary Ignatov: Silence in the gallery.\nVyshinsky: What is your current address?\nRostov: Suite 317 at the Hotel Metropol, Moscow.\nVyshinsky: How long have you lived there?\nRostov: I have been in residence since the fifth of September 1918. Just under four\nyears.\nVyshinsky: And your occupation?\nRostov: It is not the business of gentlemen to have occupations.\nVyshinsky: Very well then. How do you spend your time?\nRostov: Dining, discussing. Reading, reflecting. The usual rigmarole.\nVyshinsky: And you write poetry?\nRostov: I have been known to fence with a quill.\nVyshinsky: [Holding up a pamphlet] Are you the author of this long poem of 1913:\nWhere Is It Now?\nRostov: It has been attributed to me.\nVyshinsky: Why did you write the poem?\nRostov: It demanded to be written. I simply happened to be sitting at the particular\ndesk on the particular morning when it chose to make its demands.\nVyshinsky: And where was that exactly?\nRostov: In the south parlor at Idlehour.\nVyshinsky: Idlehour?\nRostov: The Rostov estate in Nizhny Novgorod.\nVyshinsky: Ah, yes. Of course. How apt. But let us return our attention to your\npoem. Coming as it did—in the more subdued years after the failed revolt of 1905—\nmany considered it a call to action. Would you agree with that assessment?\nRostov: All poetry is a call to action.\nVyshinsky: [Checking notes] And it was in the spring of the following year that you\nleft Russia for Paris . . . ?\nRostov: I seem to remember blossoms on the apple trees. So, yes, in all likelihood\nit was spring.\nVyshinsky: May 16 to be precise. Now, we understand the reasons for your self-\nimposed exile; and we even have some sympathy with the actions that prompted your\nflight. What concerns us here is your return in 1918. One wonders if you came back\nwith the intention of taking up arms and, if so, whether for or against the\nRevolution.', 'In replying It is nothing to Andrey, the Count was not simply resorting to a\nGallic figure of speech. In point of fact, the Count deserved as much thanks\nfor his little intervention as a swallow deserves for its trill. For since the age\nof fifteen, Alexander Rostov had been a master of seating tables.\nWhenever he was home for the holidays, his grandmother would\ninevitably call him into the library, where she liked to knit by the fireplace\nin a solitary chair.\n“Come in, my boy, and sit with me a moment.”\n“Certainly, Grandmother,” replied the Count, balancing himself on the\nedge of the fire grate. “How can I be of assistance?”\n“The prelate is coming for dinner on Friday night—as are the Duchess\nObolensky, Count Keragin, and the Minsky-Polotovs. . . .”\nHere she would let her voice trail off without further explanation; but no\nfurther explanation was needed. The Countess was of a mind that dinner\nshould provide one with respite from life’s trials and tribulations. Thus, she\ncould not countenance discussions of religion, politics, or personal sorrows\nat her table. Further complicating matters, the prelate was deaf in his left\near, partial to Latin epigrams, and prone to stare at décolletage whenever he\ndrank a glass of wine; while the Duchess Obolensky, who was particularly\ncaustic in summer, frowned upon pithy sayings and could not abide\ndiscussions of the arts. And the Keragins? Their great-grandfather had been\ncalled a Bonapartist by Prince Minsky-Polotov in 1811, and they had not\nexchanged a word with a Minsky-Polotov since.\n“How many will be in attendance?” asked the Count.\n“Forty.”\n“The usual assembly?”\n“More or less.”\n“The Osipovs?”\n“Yes. But Pierre is in Moscow. . . .”\n“Ah,” said the Count with the smile of the chess champion who has been\nconfronted with a new gambit.\nThe Nizhny Novgorod Province had a hundred prominent families,\nwhich over the course of two centuries had intermarried and divorced,', 'A\n1922\nAn Ambassador\nt half past six on the twenty-first of June 1922, when Count Alexander\nIlyich Rostov was escorted through the gates of the Kremlin onto Red\nSquare, it was glorious and cool. Drawing his shoulders back without\nbreaking stride, the Count inhaled the air like one fresh from a swim. The\nsky was the very blue that the cupolas of St. Basil’s had been painted for.\nTheir pinks, greens, and golds shimmered as if it were the sole purpose of a\nreligion to cheer its Divinity. Even the Bolshevik girls conversing before\nthe windows of the State Department Store seemed dressed to celebrate the\nlast days of spring.\n“Hello, my good man,” the Count called to Fyodor, at the edge of the\nsquare. “I see the blackberries have come in early this year!”\nGiving the startled fruit seller no time to reply, the Count walked briskly\non, his waxed moustaches spread like the wings of a gull. Passing through\nResurrection Gate, he turned his back on the lilacs of the Alexander\nGardens and proceeded toward Theatre Square, where the Hotel Metropol\nstood in all its glory. When he reached the threshold, the Count gave a wink\nto Pavel, the afternoon doorman, and turned with a hand outstretched to the\ntwo soldiers trailing behind him.\n“Thank you, gentlemen, for delivering me safely. I shall no longer be in\nneed of your assistance.”\nThough strapping lads, both of the soldiers had to look up from under\ntheir caps to return the Count’s gaze—for like ten generations of Rostov\nmen, the Count stood an easy six foot three.\n“On you go,” said the more thuggish of the two, his hand on the butt of\nhis rifle. “We’re to see you to your rooms.”']",The sentence given to Count Alexander Ilyich Rostov by the committee was not mentioned in the context provided.,"Count Alexander Ilyich Rostov was sentenced to house arrest in the Metropol Hotel in Moscow for the rest of his life, under the condition that if he ever left the hotel, he would be shot.,",1.0,0.0,0.0,,0.3333333322222222,0.21221337242008892
"What are the limitations of chain-of-thought prompting noted in the paper?,","['Chain-of-Thought Prompting Elicits Reasoning\nin Large Language Models\nJason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma\nBrian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou\nGoogle Research, Brain Team\n{jasonwei,dennyzhou}@google.com\nAbstract\nWe explore how generating a chain of thought—a series of intermediate reasoning\nsteps—signiﬁcantly improves the ability of large language models to perform\ncomplex reasoning. In particular, we show how such reasoning abilities emerge\nnaturally in sufﬁciently large language models via a simple method called chain-of-\nthought prompting, where a few chain of thought demonstrations are provided as\nexemplars in prompting.\nExperiments on three large language models show that chain-of-thought prompting\nimproves performance on a range of arithmetic, commonsense, and symbolic\nreasoning tasks. The empirical gains can be striking. For instance, prompting a\nPaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art\naccuracy on the GSM8K benchmark of math word problems, surpassing even\nﬁnetuned GPT-3 with a veriﬁer.\nA: The cafeteria had 23 apples originally. They used \n20 to make lunch. So they had 23 - 20 = 3. They \nbought 6 more apples, so they have 3 + 6 = 9. The \nanswer is 9.\nChain-of-Thought Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of \ntennis balls. Each can has 3 tennis balls. How many \ntennis balls does he have now? \nA: The answer is 11. \nQ: The cafeteria had 23 apples. If they used 20 to \nmake lunch and bought 6 more, how many apples \ndo they have?\nA: The answer is 27.\nStandard Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of \ntennis balls. Each can has 3 tennis balls. How many \ntennis balls does he have now? \nA: Roger started with 5 balls. 2 cans of 3 tennis balls \neach is 6 tennis balls. 5 + 6 = 11. The answer is 11. \nQ: The cafeteria had 23 apples. If they used 20 to \nmake lunch and bought 6 more, how many apples \ndo they have?\nModel Input\nModel Output Model Output\nModel Input\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic,\ncommonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\narXiv:2201.11903v6  [cs.CL]  10 Jan 2023', 'C Extended Related Work\nChain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-\ning, natural language explanations, program synthesis/execution, numeric and logical reasoning, and\nintermediate language steps.\nC.1 Prompting\nThe recent success of large-scale language models has led to growing interest in improving their\ncapability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a\nsurvey). This paper falls in the category of general prompting approaches, whereby input prompts are\noptimized to allow a single large language model to better perform a variety of tasks (Li and Liang,\n2021; Lester et al., 2021; Reif et al., 2022, inter alia).\nOne recent line of work aims to improve the ability of language models to perform a task by providing\ninstructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh\net al., 2022; Wang et al., 2022b). This line of work is related because it also augments input–output\npairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically\nprepended to the inputs), chain-of-thought prompting augments the outputs of language models.\nAnother related direction is sequentially combining the outputs of language models; human–computer\ninteraction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of\nlanguage models improves task outcomes in a 20-person user study.\nC.2 Natural language explanations\nAnother closely related direction uses natural language explanations (NLEs), often with the goal of\nimproving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi´c, 2021, inter alia). That\nline of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,\n2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ﬁnal\nprediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,\nthe chain of thought processing considered in this paper occurs before the ﬁnal answer. And while\nNLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of\nchain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple\nsteps—interpretability is just a side effect. Marasovi ´c et al. (2022) show that prompt-based ﬁnetuning\nwith NLE improves NLI and classiﬁcation performance, though they largely focus on evaluating\nexplanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,\nand symbolic tasks that require multi-hop reasoning.\nC.3 Program synthesis and execution\nUsing intermediate reasoning steps has a long history in program synthesis and execution (Zaremba\nand Sutskever, 2014, inter alia). Recent work along in this direction has included a number of\narchitectural innovations (Cai et al., 2017; Dong et al., 2019; Yan et al., 2020), as well as the use of\nlarge language models (Chen et al., 2021; Austin et al., 2021). The program execution work closest to\nours is perhaps Nye et al. (2021), which show that large language models can perform up to 10-digit\naddition, evaluate polynomials, and execute python programs. Whereas generating a program and\nthen executing it can be viewed as a type of reasoning, our work generalizes such domain-speciﬁc\nprimitives to natural language, which is open-domain and relevant to any text-to-text NLP task in\nprinciple.\nC.4 Numeric and logical reasoning\nNumeric and logical reasoning has been a long-studied task in machine learning and natural language\nprocessing (Lev et al., 2004, inter alia). Recent work has also aimed to inject numeric reasoning\nabilities in language models in various ways, such as augmenting BERT with a predeﬁned set of\nexecutable operations (Andor et al., 2019), including a graph neural network (Ran et al., 2019), and\nusing specialized training procedures (Pi˛ ekos et al., 2021). Another line of work aims to enable\nlanguage models to perform logical or formal reasoning, often by verablizing the rules in natural\nlanguage formal rules using language (Clark et al., 2020; Saeed et al., 2021; Liang et al., 2021).\n24', 'experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\nreasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\nchain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\nall experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\nmodel. No language models were ﬁnetuned in the process of writing this paper.\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\n(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ﬂat scaling curve, chain-\nof-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\nappears to expand the set of tasks that large language models can perform successfully—in other\nwords, our work underscores that standard prompting only provides a lower bound on the capabilities\nof large language models. This observation likely raises more questions than it answers—for instance,\nhow much more can we expect reasoning ability to improve with a further increase in model scale?\nWhat other prompting methods might expand the range of tasks that language models can solve?\nAs for limitations, we ﬁrst qualify that although chain of thought emulates the thought processes of\nhuman reasoners, this does not answer whether the neural network is actually “reasoning,” which\nwe leave as an open question. Second, although the cost of manually augmenting exemplars with\nchains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\nﬁnetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\ngeneralization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\nand incorrect answers; improving factual generations of language models is an open direction for\nfuture work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022,inter alia). Finally,\nthe emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\nreal-world applications; further research could explore how to induce reasoning in smaller models.\n7 Related Work\nThis work is inspired by many research areas, which we detail in an extended related work section\n(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\nThe ﬁrst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\npioneer the idea of using natural language rationales to solve math word problems through a series\nof intermediate steps. Their work is a remarkable contrast to the literature using formal languages\nto reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\net al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ﬁnetune a pretrained\nlanguage model rather than training a model from scratch. In the domain of program synthesis,\nNye et al. (2021) leverage language models to predict the ﬁnal outputs of Python programs via\nﬁrst line-to-line predicting the intermediate computational results, and show that their step-by-step\nprediction method performs better than directly predicting the ﬁnal outputs.\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the\npopularization of few-shot prompting as given by Brown et al. (2020), several general approaches\nhave improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\n2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\net al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\noutputs of language models with a chain of thought.\n8 Conclusions\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\ning reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\nreasoning, we ﬁnd that chain-of-thought reasoning is an emergent property of model scale that allows\nsufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves.\nBroadening the range of reasoning tasks that language models can perform will hopefully inspire\nfurther work on language-based approaches to reasoning.\n9']","The limitations of chain-of-thought prompting noted in the paper are:
1. It is not clear whether the neural network is actually ""reasoning.""
2. The cost of manually augmenting exemplars with chains of thought may be prohibitive for finetuning.
3. There is no guarantee of correct reasoning paths, leading to both correct and incorrect answers.
4. The emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in real-world applications.","Chain-of-thought reasoning requires large model scales, is not always factually accurate, and may incur high annotation costs for generating reasoning steps.,",,0.9904536776033833,0.9999999999666667,,0.0,0.7901151510851534
"What datasets were used to evaluate GKP?,","['A group of people in a field with various kites in the sky.', 'many kites with different shapes all laying on the ground', 'People flying kites over a lake in a park ']",There is no information in the context provided about datasets being used to evaluate GKP.,"GKP was evaluated on four commonsense reasoning datasets: NumerSense, CommonsenseQA, CommonsenseQA 2.0, and QASC.,",0.5,0.0,0.0,0.0,0.0,0.20849150682966905
"What poem is associated with Count Rostov, and what is its significance?,","['Rostov: By that point, I’m afraid that my days of taking up arms were behind me.\nVyshinsky: Why then did you come back?\nRostov: I missed the climate.\n[Laughter.]\nVyshinsky: Count Rostov, you do not seem to appreciate the gravity of your position.\nNor do you show the respect that is due the men convened before you.\nRostov: The Tsarina had the same complaints about me in her day.\nIgnatov: Prosecutor Vyshinsky. If I may . . .\nVyshinsky: Secretary Ignatov.\nIgnatov: I have no doubt, Count Rostov, that many in the gallery are surprised to\nfind you so charming; but I, for one, am not surprised in the least. History has\nshown charm to be the final ambition of the leisure class. What I do find\nsurprising is that the author of the poem in question could have become a man so\nobviously without purpose.\nRostov: I have lived under the impression that a man’s purpose is known only to God.\nIgnatov: Indeed. How convenient that must have been for you.\n[The Committee recesses for twelve minutes.]\nIgnatov: Alexander Ilyich Rostov, taking into full account your own testimony, we\ncan only assume that the clear-eyed spirit who wrote the poem Where Is It Now? has\nsuccumbed irrevocably to the corruptions of his class—and now poses a threat to\nthe very ideals he once espoused. On that basis, our inclination would be to have\nyou taken from this chamber and put against the wall. But there are those within\nthe senior ranks of the Party who count you among the heroes of the\nprerevolutionary cause. Thus, it is the opinion of this committee that you should\nbe returned to that hotel of which you are so fond. But make no mistake: should\nyou ever set foot outside of the Metropol again, you will be shot. Next matter.\nBearing the signatures of\nV. A. Ignatov\nM. S. Zakovsky\nA. N. Kosarev', '“What with the revolt of 1905 and the repressions that followed, when\nwe graduated it was still a dangerous time for writing poems of political\nimpatience. Given Mishka’s background, the Okhrana would have swept\nhim up with a broom. So one night—after polishing off a particularly good\nbottle of Margaux—we decided to publish the poem under my name.”\n“But why yours?”\n“What were they going to do to Count Alexander Rostov—member of\nthe Jockey Club and godson of a counselor to the Tsar?” The Count shook\nhis head. “The irony, of course, is that the life which ended up being saved\nwas mine, not his. But for that poem, they would have shot me back in\n1922.”\nKaterina, who had listened to this story intently, was suddenly holding\nback tears.\n“Ah, but there you have him,” she said.\nThey were both silent as she regained her composure.\n“I want you to know,” said the Count, “how much I appreciate your\ncoming to tell me in person.” But Katerina dismissed his gratitude.\n“I came at Mikhail’s request. He asked me to bring you something.”\nFrom her satchel she took out a rectangular package wrapped in plain\nbrown paper and tied with twine.\nTaking the package in hand, the Count could tell from its weight that it\nwas a book.\n“It is his project,” said the Count with a smile.\n“Yes,” she said. Then she added with pointed emphasis: “He slaved over\nit.”\nThe Count nodded to express his understanding and to assure Katerina\nthat he did not take the bestowal lightly.\nKaterina looked once more around the room with a light shake of the\nhead as if it somehow exemplified the mystery of outcomes; then she said\nthat she should go.\nThe Count rose to his feet with her, setting Mishka’s project on the chair.\n“Are you going back to Yavas?” he asked.\n“No.”\n“Will you be staying in Moscow?”\n“No.”\n“Where then?”', 'Nevsky Prospekt and wandered all the way to Tikhvin Cemetery where, on\na spot overlooking the Neva River, this firefly, this pinwheel, this wonder of\nthe world had suddenly taken his hand.\n“Ah, Count Rostov,” exclaimed Arkady in passing. “There you are. I\nbelieve I have a message for you. . . .” Returning to the front desk, Arkady\nquickly rifled through some notes. “Here.”\nThe message, which had been taken down by the hotel’s receptionist,\nconveyed Mishka’s apologies and explained that as Katerina was under the\nweather, he was returning to St. Petersburg earlier than planned. Taking a\nmoment to mask his disappointment, the Count looked up from the note to\nthank Arkady, but the desk captain had already turned his attention to\nanother guest.\n“Good evening, Count Rostov.” Andrey took a quick look in the Book. “A\nparty of two tonight, isn’t it?”\n“I’m afraid it’s going to be a party of one, Andrey.”\n“Nonetheless, it is our pleasure to have you. Your table should be ready\nin just a few minutes.”\nWith the recent recognition of the USSR by Germany, England, and\nItaly, a wait of a few minutes had become increasingly common at the\nBoyarsky; but such was the price of being welcomed back into the\nsisterhood of nations and the brotherhood of trade.\nAs the Count stepped aside, a man with a pointed beard came marching\ndown the hallway with a protégé in tow. Though the Count had only seen\nhim once or twice before, the Count could tell he was the Commissar of\nSomething-or-Other, for he walked with urgency, talked with urgency, and\neven came to a stop with urgency.\n“Good evening, comrade Soslovsky,” said Andrey with a welcoming\nsmile.\n“Yes,” pronounced Soslovsky—as if he’d just been asked whether he\nwanted to be seated immediately.\nWith a nod of understanding, Andrey signaled a waiter, handed him two\nmenus, and directed him to lead the gentlemen to table fourteen.']","The poem associated with Count Rostov is ""Where Is It Now?"" The significance of the poem is that it was published under Rostov's name to protect the real author, Mishka, from being targeted by the authorities during a dangerous time for writing politically charged poems. The poem ultimately saved Rostov's life, as without it, he would have been shot in 1922.","The poem -Where Is It Now?- is attributed to Count Rostov and is considered a call to action written in the aftermath of the failed revolt of 1905.,",1.0,0.9587363017209553,0.99999999995,0.5,0.0,0.38406748635519006
"What was Count Rostov’s sentence during his trial in 1922?,","['21 June 1922\nAPPEARANCE OF COUNT ALEXANDER ILYICH ROSTOV\nBEFORE THE EMERGENCY COMMITTEE OF THE PEOPLE’S COMMISSARIAT FOR INTERNAL AFFAIRS\nPresiding: Comrades V. A. Ignatov, M. S. Zakovsky, A. N. Kosarev\nProsecuting: A. Y. Vyshinsky\nProsecutor Vyshinsky: State your name.\nRostov: Count Alexander Ilyich Rostov, recipient of the Order of Saint Andrew,\nmember of the Jockey Club, Master of the Hunt.\nVyshinsky: You may have your titles; they are of no use to anyone else. But for the\nrecord, are you not Alexander Rostov, born in St. Petersburg, 24 October 1889?\nRostov: I am he.\nVyshinsky: Before we begin, I must say, I do not think that I have ever seen a\njacket festooned with so many buttons.\nRostov: Thank you.\nVyshinsky: It was not meant as a compliment.\nRostov: In that case, I demand satisfaction on the field of honor.\n[Laughter.]\nSecretary Ignatov: Silence in the gallery.\nVyshinsky: What is your current address?\nRostov: Suite 317 at the Hotel Metropol, Moscow.\nVyshinsky: How long have you lived there?\nRostov: I have been in residence since the fifth of September 1918. Just under four\nyears.\nVyshinsky: And your occupation?\nRostov: It is not the business of gentlemen to have occupations.\nVyshinsky: Very well then. How do you spend your time?\nRostov: Dining, discussing. Reading, reflecting. The usual rigmarole.\nVyshinsky: And you write poetry?\nRostov: I have been known to fence with a quill.\nVyshinsky: [Holding up a pamphlet] Are you the author of this long poem of 1913:\nWhere Is It Now?\nRostov: It has been attributed to me.\nVyshinsky: Why did you write the poem?\nRostov: It demanded to be written. I simply happened to be sitting at the particular\ndesk on the particular morning when it chose to make its demands.\nVyshinsky: And where was that exactly?\nRostov: In the south parlor at Idlehour.\nVyshinsky: Idlehour?\nRostov: The Rostov estate in Nizhny Novgorod.\nVyshinsky: Ah, yes. Of course. How apt. But let us return our attention to your\npoem. Coming as it did—in the more subdued years after the failed revolt of 1905—\nmany considered it a call to action. Would you agree with that assessment?\nRostov: All poetry is a call to action.\nVyshinsky: [Checking notes] And it was in the spring of the following year that you\nleft Russia for Paris . . . ?\nRostov: I seem to remember blossoms on the apple trees. So, yes, in all likelihood\nit was spring.\nVyshinsky: May 16 to be precise. Now, we understand the reasons for your self-\nimposed exile; and we even have some sympathy with the actions that prompted your\nflight. What concerns us here is your return in 1918. One wonders if you came back\nwith the intention of taking up arms and, if so, whether for or against the\nRevolution.', 'A\n1922\nAn Ambassador\nt half past six on the twenty-first of June 1922, when Count Alexander\nIlyich Rostov was escorted through the gates of the Kremlin onto Red\nSquare, it was glorious and cool. Drawing his shoulders back without\nbreaking stride, the Count inhaled the air like one fresh from a swim. The\nsky was the very blue that the cupolas of St. Basil’s had been painted for.\nTheir pinks, greens, and golds shimmered as if it were the sole purpose of a\nreligion to cheer its Divinity. Even the Bolshevik girls conversing before\nthe windows of the State Department Store seemed dressed to celebrate the\nlast days of spring.\n“Hello, my good man,” the Count called to Fyodor, at the edge of the\nsquare. “I see the blackberries have come in early this year!”\nGiving the startled fruit seller no time to reply, the Count walked briskly\non, his waxed moustaches spread like the wings of a gull. Passing through\nResurrection Gate, he turned his back on the lilacs of the Alexander\nGardens and proceeded toward Theatre Square, where the Hotel Metropol\nstood in all its glory. When he reached the threshold, the Count gave a wink\nto Pavel, the afternoon doorman, and turned with a hand outstretched to the\ntwo soldiers trailing behind him.\n“Thank you, gentlemen, for delivering me safely. I shall no longer be in\nneed of your assistance.”\nThough strapping lads, both of the soldiers had to look up from under\ntheir caps to return the Count’s gaze—for like ten generations of Rostov\nmen, the Count stood an easy six foot three.\n“On you go,” said the more thuggish of the two, his hand on the butt of\nhis rifle. “We’re to see you to your rooms.”', 'In replying It is nothing to Andrey, the Count was not simply resorting to a\nGallic figure of speech. In point of fact, the Count deserved as much thanks\nfor his little intervention as a swallow deserves for its trill. For since the age\nof fifteen, Alexander Rostov had been a master of seating tables.\nWhenever he was home for the holidays, his grandmother would\ninevitably call him into the library, where she liked to knit by the fireplace\nin a solitary chair.\n“Come in, my boy, and sit with me a moment.”\n“Certainly, Grandmother,” replied the Count, balancing himself on the\nedge of the fire grate. “How can I be of assistance?”\n“The prelate is coming for dinner on Friday night—as are the Duchess\nObolensky, Count Keragin, and the Minsky-Polotovs. . . .”\nHere she would let her voice trail off without further explanation; but no\nfurther explanation was needed. The Countess was of a mind that dinner\nshould provide one with respite from life’s trials and tribulations. Thus, she\ncould not countenance discussions of religion, politics, or personal sorrows\nat her table. Further complicating matters, the prelate was deaf in his left\near, partial to Latin epigrams, and prone to stare at décolletage whenever he\ndrank a glass of wine; while the Duchess Obolensky, who was particularly\ncaustic in summer, frowned upon pithy sayings and could not abide\ndiscussions of the arts. And the Keragins? Their great-grandfather had been\ncalled a Bonapartist by Prince Minsky-Polotov in 1811, and they had not\nexchanged a word with a Minsky-Polotov since.\n“How many will be in attendance?” asked the Count.\n“Forty.”\n“The usual assembly?”\n“More or less.”\n“The Osipovs?”\n“Yes. But Pierre is in Moscow. . . .”\n“Ah,” said the Count with the smile of the chess champion who has been\nconfronted with a new gambit.\nThe Nizhny Novgorod Province had a hundred prominent families,\nwhich over the course of two centuries had intermarried and divorced,']",Count Rostov was sentenced to house arrest at the Hotel Metropol in Moscow.,"Count Rostov was sentenced to house arrest in the Metropol Hotel in Moscow, under the condition that if he ever left the hotel, he would be shot.,",0.0,0.90339216872892,0.0,0.0,0.3333333322222222,0.8408498460034732
"Why did Count Rostov return to Russia from Paris in 1918?,","['A\n1922\nAn Ambassador\nt half past six on the twenty-first of June 1922, when Count Alexander\nIlyich Rostov was escorted through the gates of the Kremlin onto Red\nSquare, it was glorious and cool. Drawing his shoulders back without\nbreaking stride, the Count inhaled the air like one fresh from a swim. The\nsky was the very blue that the cupolas of St. Basil’s had been painted for.\nTheir pinks, greens, and golds shimmered as if it were the sole purpose of a\nreligion to cheer its Divinity. Even the Bolshevik girls conversing before\nthe windows of the State Department Store seemed dressed to celebrate the\nlast days of spring.\n“Hello, my good man,” the Count called to Fyodor, at the edge of the\nsquare. “I see the blackberries have come in early this year!”\nGiving the startled fruit seller no time to reply, the Count walked briskly\non, his waxed moustaches spread like the wings of a gull. Passing through\nResurrection Gate, he turned his back on the lilacs of the Alexander\nGardens and proceeded toward Theatre Square, where the Hotel Metropol\nstood in all its glory. When he reached the threshold, the Count gave a wink\nto Pavel, the afternoon doorman, and turned with a hand outstretched to the\ntwo soldiers trailing behind him.\n“Thank you, gentlemen, for delivering me safely. I shall no longer be in\nneed of your assistance.”\nThough strapping lads, both of the soldiers had to look up from under\ntheir caps to return the Count’s gaze—for like ten generations of Rostov\nmen, the Count stood an easy six foot three.\n“On you go,” said the more thuggish of the two, his hand on the butt of\nhis rifle. “We’re to see you to your rooms.”', '21 June 1922\nAPPEARANCE OF COUNT ALEXANDER ILYICH ROSTOV\nBEFORE THE EMERGENCY COMMITTEE OF THE PEOPLE’S COMMISSARIAT FOR INTERNAL AFFAIRS\nPresiding: Comrades V. A. Ignatov, M. S. Zakovsky, A. N. Kosarev\nProsecuting: A. Y. Vyshinsky\nProsecutor Vyshinsky: State your name.\nRostov: Count Alexander Ilyich Rostov, recipient of the Order of Saint Andrew,\nmember of the Jockey Club, Master of the Hunt.\nVyshinsky: You may have your titles; they are of no use to anyone else. But for the\nrecord, are you not Alexander Rostov, born in St. Petersburg, 24 October 1889?\nRostov: I am he.\nVyshinsky: Before we begin, I must say, I do not think that I have ever seen a\njacket festooned with so many buttons.\nRostov: Thank you.\nVyshinsky: It was not meant as a compliment.\nRostov: In that case, I demand satisfaction on the field of honor.\n[Laughter.]\nSecretary Ignatov: Silence in the gallery.\nVyshinsky: What is your current address?\nRostov: Suite 317 at the Hotel Metropol, Moscow.\nVyshinsky: How long have you lived there?\nRostov: I have been in residence since the fifth of September 1918. Just under four\nyears.\nVyshinsky: And your occupation?\nRostov: It is not the business of gentlemen to have occupations.\nVyshinsky: Very well then. How do you spend your time?\nRostov: Dining, discussing. Reading, reflecting. The usual rigmarole.\nVyshinsky: And you write poetry?\nRostov: I have been known to fence with a quill.\nVyshinsky: [Holding up a pamphlet] Are you the author of this long poem of 1913:\nWhere Is It Now?\nRostov: It has been attributed to me.\nVyshinsky: Why did you write the poem?\nRostov: It demanded to be written. I simply happened to be sitting at the particular\ndesk on the particular morning when it chose to make its demands.\nVyshinsky: And where was that exactly?\nRostov: In the south parlor at Idlehour.\nVyshinsky: Idlehour?\nRostov: The Rostov estate in Nizhny Novgorod.\nVyshinsky: Ah, yes. Of course. How apt. But let us return our attention to your\npoem. Coming as it did—in the more subdued years after the failed revolt of 1905—\nmany considered it a call to action. Would you agree with that assessment?\nRostov: All poetry is a call to action.\nVyshinsky: [Checking notes] And it was in the spring of the following year that you\nleft Russia for Paris . . . ?\nRostov: I seem to remember blossoms on the apple trees. So, yes, in all likelihood\nit was spring.\nVyshinsky: May 16 to be precise. Now, we understand the reasons for your self-\nimposed exile; and we even have some sympathy with the actions that prompted your\nflight. What concerns us here is your return in 1918. One wonders if you came back\nwith the intention of taking up arms and, if so, whether for or against the\nRevolution.', 'Nina, who had taken her first steps into the modern world and found it\njust as worthy of her unblinking intelligence as the study of princesses, was\nmoving with her father to a large apartment in one of the new buildings\ndesignated for the use of Party officials.\nAnd as it was the third week of June, the Fourth Annual Congress of\nRAPP was underway, but Mishka was not in attendance, having taken a\nleave from his post at the university in order to finish his short story\nanthology (now in five volumes) and to follow his Katerina back to Kiev,\nwhere she was teaching in an elementary school.\nOn occasion, the Count still shared a cup of coffee on the roof with the\nhandyman, Abram, where they would talk of summer nights in Nizhny\nNovgorod. But the old man was now so nearsighted and uncertain on his\nfeet that one morning earlier that month, as if in anticipation of his\nretirement, the bees had disappeared from their hives.\nSo, yes, life was rolling along, just as it always had.\nLooking back, the Count recalled how on the first night of his house\narrest, in the spirit of his godfather’s old maxim, he had committed himself\nto mastering his circumstances. Well, in retrospect, there was another story\nhis godfather told that was just as worthy of emulation. It entailed the\nGrand Duke’s close friend, Admiral Stepan Makarov, who commanded the\nImperial Russian Navy during the Russo-Japanese War. On the thirteenth of\nApril 1904, with Port Arthur under attack, Makarov led his battleships into\nthe fray and drove the Japanese fleet back into the Yellow Sea. But upon\nreturning to port on calm seas, the flagship struck a Japanese mine and\nbegan to take on water. So, with the battle won and the shores of his\nhomeland in sight, Makarov ascended to the helm in full military dress and\nwent down with his ship.\nThe Count’s bottle of White (which he was fairly certain was a\nChardonnay from Burgundy and best served at 55˚) sat sweating on the\ntable. Reaching across his plate, he picked up the bottle and served himself.\nThen having made a toast of gratitude to the Boyarsky, the Count emptied\nhis glass and headed to the Shalyapin for one last snifter of brandy.']",Count Rostov returned to Russia from Paris in 1918 because he was forced to return due to the political situation and was placed under house arrest in the Hotel Metropol in Moscow.,"Count Rostov returned to Russia in 1918 because he missed the climate and wanted to tend to his family estate during a period of upheaval.,",0.6666666666666666,0.9902043164765818,0.0,0.0,0.0,0.4827710625445248
"What happened to Count Rostov’s family estate, Idlehour, after the Revolution?,","['In replying It is nothing to Andrey, the Count was not simply resorting to a\nGallic figure of speech. In point of fact, the Count deserved as much thanks\nfor his little intervention as a swallow deserves for its trill. For since the age\nof fifteen, Alexander Rostov had been a master of seating tables.\nWhenever he was home for the holidays, his grandmother would\ninevitably call him into the library, where she liked to knit by the fireplace\nin a solitary chair.\n“Come in, my boy, and sit with me a moment.”\n“Certainly, Grandmother,” replied the Count, balancing himself on the\nedge of the fire grate. “How can I be of assistance?”\n“The prelate is coming for dinner on Friday night—as are the Duchess\nObolensky, Count Keragin, and the Minsky-Polotovs. . . .”\nHere she would let her voice trail off without further explanation; but no\nfurther explanation was needed. The Countess was of a mind that dinner\nshould provide one with respite from life’s trials and tribulations. Thus, she\ncould not countenance discussions of religion, politics, or personal sorrows\nat her table. Further complicating matters, the prelate was deaf in his left\near, partial to Latin epigrams, and prone to stare at décolletage whenever he\ndrank a glass of wine; while the Duchess Obolensky, who was particularly\ncaustic in summer, frowned upon pithy sayings and could not abide\ndiscussions of the arts. And the Keragins? Their great-grandfather had been\ncalled a Bonapartist by Prince Minsky-Polotov in 1811, and they had not\nexchanged a word with a Minsky-Polotov since.\n“How many will be in attendance?” asked the Count.\n“Forty.”\n“The usual assembly?”\n“More or less.”\n“The Osipovs?”\n“Yes. But Pierre is in Moscow. . . .”\n“Ah,” said the Count with the smile of the chess champion who has been\nconfronted with a new gambit.\nThe Nizhny Novgorod Province had a hundred prominent families,\nwhich over the course of two centuries had intermarried and divorced,', 'At ten o’clock the Count was seated in the Boyarsky with an empty plate\nand a nearly empty bottle of White on the table. With the day drawing\nrapidly to a close, he took some pride in knowing that everything was in\norder.\nThat morning, having received a visit from Konstantin Konstantinovich,\nthe Count had brought his accounts up to date at Muir & Mirrielees (now\nknown as the Central Universal Department Store), Filippov’s (the First\nMoscow Bakery), and, of course, the Metropol. At the Grand Duke’s desk,\nhe had written a letter to Mishka, which he had then entrusted to Petya with\ninstructions it be mailed on the following day. In the afternoon, he had paid\nhis weekly visit to the barber and tidied up his rooms. He had donned his\nburgundy smoking jacket (which, to be perfectly frank, was disconcertingly\nsnug), and in its pocket he placed a single gold coin for the undertaker with\ninstructions that he be dressed in the freshly pressed black suit (which had\nbeen laid out on his bed), and that his body be buried in the family plot at\nIdlehour.\nBut if the Count took pride in knowing that everything was in order, he\ntook comfort in knowing that the world would carry on without him—and,\nin fact, already had. The night before, he had happened to be standing at the\nconcierge’s desk when Vasily produced a map of Moscow for one of the\nhotel’s guests. As Vasily drew a zigzagging line from the center of the city\nto the Garden Ring, more than half of the streets he named were unfamiliar\nto the Count. Earlier that day, Vasily had informed him that the famed blue-\nand-gold lobby of the Bolshoi had been painted over in white, while in the\nArbat Andreyev’s moody statue of Gogol had been plucked from its\npedestal and replaced with a more uplifting one of Gorky. Just like that, the\ncity of Moscow could boast new street names, new lobbies, and new statues\n—and neither the tourists, the theatergoers, nor the pigeons seemed\nparticularly put out.\nThe staffing trend that had begun with the appointment of the Bishop\nhad continued unabated—such that any young man with more influence\nthan experience could now don the white jacket, clear from the left, and\npour wine into water glasses.\nMarina, who once had welcomed the Count’s company as she stitched in\nthe stitching room, now had a junior seamstress to watch over as well as a\ntoddler at home (God bless).', 'beyond the fiftieth page, he had worn out more shoe leather than a pilgrim\non the road to St. Paul’s.\nSo, the storming of hotel suites and the wearing out of carpets was not\nparticularly out of character for his old friend. But as Mishka had recently\nreceived a new appointment at their alma mater in St. Petersburg, the Count\nwas surprised to have him appear so suddenly, and in such a state.\nAfter embracing, the two men climbed the five flights to the attic.\nHaving been told what to expect, Mishka took in his friend’s new\ncircumstances without an expression of surprise. But he paused before the\nthree-legged bureau and tilted his head to give its base a second look.\n“The Essays of Montaigne?”\n“Yes,” affirmed the Count.\n“I gather they didn’t agree with you.”\n“On the contrary. I found them to be the perfect height. But tell me, my\nfriend, what brings you to Moscow?”\n“Nominally, Sasha, I am here to help plan the inaugural congress of\nRAPP, which is to be held in June. But of greater consequence . . .”\nHere Mishka reached into a shoulder satchel and produced a bottle of\nwine with an image of two crossed keys embossed in the glass above the\nlabel.\n“I hope I am not too late.”\nThe Count took the bottle in hand. He ran his thumb over the surface of\nthe insignia. Then he shook his head with the smile of the deeply moved.\n“No, Mishka. As always, you are right on time.” Then he led his old\nfriend through his jackets.\nAs the Count excused himself to rinse a pair of glasses from the\nAmbassador, Mishka surveyed his friend’s study with a sympathetic gaze.\nFor the tables, the chairs, the objets d’art, he recognized them all. And well\nhe knew that they had been culled from the halls of Idlehour as reminders\nof Elysian days.\nIt must have been in 1908 that Alexander began inviting him to Idlehour\nfor the month of July. Having traveled from St. Petersburg by a series of\nconsecutively smaller trains, they would finally arrive at that little halt in']","Count Rostov's family estate, Idlehour, was likely confiscated or taken over after the Revolution, as the Count had to cull items from the estate to use in his study as reminders of past days.","Count Rostov had to shutter the family estate, Idlehour, releasing the servants and leaving the house empty.,",0.6666666666666666,0.9901347226339426,0.0,0.0,0.4999999975,0.2313793696262289
"Who is the author of the poem Where Is It Now?, written in 1913?,","['How well I remember\nWhen it came as a visitor on foot\nAnd dwelt a while amongst us\nA melody in the semblance of a mountain cat.\nWell, where is our purpose now?\nLike so many questions\nI answer this one\nWith the eye-averted peeling of a pear.\nWith a bow I bid goodnight\nAnd pass through terrace doors\nInto the simple splendors\nOf another temperate spring;\nBut this much I know:\nIt is not lost among the autumn leaves on Peter’s Square.\nIt is not among the ashes in the Athenaeum ash cans.\nIt is not inside the blue pagodas of your fine Chinoiserie.\nIt is not in Vronsky’s saddlebags;\nNot in Sonnet XXX, stanza one;\nNot on twenty-seven red . . .\nWhere Is It Now? (Lines 1–19)\nCount Alexander Ilyich Rostov\n1913', 'Rostov: By that point, I’m afraid that my days of taking up arms were behind me.\nVyshinsky: Why then did you come back?\nRostov: I missed the climate.\n[Laughter.]\nVyshinsky: Count Rostov, you do not seem to appreciate the gravity of your position.\nNor do you show the respect that is due the men convened before you.\nRostov: The Tsarina had the same complaints about me in her day.\nIgnatov: Prosecutor Vyshinsky. If I may . . .\nVyshinsky: Secretary Ignatov.\nIgnatov: I have no doubt, Count Rostov, that many in the gallery are surprised to\nfind you so charming; but I, for one, am not surprised in the least. History has\nshown charm to be the final ambition of the leisure class. What I do find\nsurprising is that the author of the poem in question could have become a man so\nobviously without purpose.\nRostov: I have lived under the impression that a man’s purpose is known only to God.\nIgnatov: Indeed. How convenient that must have been for you.\n[The Committee recesses for twelve minutes.]\nIgnatov: Alexander Ilyich Rostov, taking into full account your own testimony, we\ncan only assume that the clear-eyed spirit who wrote the poem Where Is It Now? has\nsuccumbed irrevocably to the corruptions of his class—and now poses a threat to\nthe very ideals he once espoused. On that basis, our inclination would be to have\nyou taken from this chamber and put against the wall. But there are those within\nthe senior ranks of the Party who count you among the heroes of the\nprerevolutionary cause. Thus, it is the opinion of this committee that you should\nbe returned to that hotel of which you are so fond. But make no mistake: should\nyou ever set foot outside of the Metropol again, you will be shot. Next matter.\nBearing the signatures of\nV. A. Ignatov\nM. S. Zakovsky\nA. N. Kosarev', '“It’s Mishka, isn’t it . . . ,” the Count said after a moment.\n“Yes.”\n“When?”\n“A week ago today.”\nThe Count nodded, as one who had been expecting the news for some\ntime. He didn’t ask Katerina how his old friend had died, and she didn’t\noffer to tell him. It was plain enough that he had been betrayed by his times.\n“Were you with him?” asked the Count.\n“Yes.”\n“In Yavas?”\n“Yes.”\n. . .\n“I was under the impression that . . .”\n“I lost my husband some time ago.”\n“I’m sorry. I didn’t know. Do you have children . . . ?”\n“No.”\nShe said it curtly, as if in response to a foolish question; but then she\ncontinued more softly. “I received word from Mikhail in January. I went to\nhim in Yavas. We have been together these last six months.” After a\nmoment, she added: “He spoke of you often.”\n“He was a loyal friend,” said the Count.\n“He was a man of devotions,” corrected Katerina.\nThe Count had been about to remark on Mishka’s propensity for getting\ninto scrapes and his love of pacing, but she had just described his old friend\nbetter than he ever had. Mikhail Fyodorovich Mindich was a man of\ndevotions.\n“And a fine poet,” the Count added, almost to himself.\n“One of two.”\nThe Count looked to Katerina as if he didn’t understand. Then he\noffered a wistful smile.\n“I’ve never written a poem in my life,” he said.\nNow, it was Katerina who didn’t understand.\n“What do you mean? What about Where Is It Now?”\n“It was Mishka who wrote that poem. In the south parlor at Idlehour . . .\nIn the summer of 1913 . . .”\nAs Katerina still looked confused, the Count elaborated.']","Mikhail Fyodorovich Mindich is the author of the poem Where Is It Now?, written in 1913.","The author of the poem Where Is It Now? is Count Alexander Ilyich Rostov.,",1.0,0.9544023704180935,0.99999999995,1.0,0.4999999975,0.22930286113077475
"How did Count Rostov describe his occupation during the trial?,","['“Count Alexander Ilyich Rostov,” he said after returning his glass to the\ntable. “Recipient of the Order of Saint Andrew, member of the Jockey Club,\nMaster of the Hunt . . .”\n“You have me at a disadvantage.”\n“You don’t know who I am?”\n“I know that you are a man who can secure one of the Boyarsky’s\nprivate rooms in which to dine alone while a behemoth waits at the door.”\nThe stranger laughed.\n“Very good,” he said, leaning back in his chair. “What else do you see?”\nThe Count studied his host more indiscreetly and then shrugged.\n“I’d say you were a man of forty and were a soldier once. I suspect you\njoined the infantry, but were a colonel by the end of the war.”\n“How would you know that I became a colonel?”\n“It is the business of a gentleman to distinguish between men of rank.”\n“The business of a gentleman,” the colonel repeated with a smile, as if\nhe appreciated the turn of phrase. “And can you tell where I’m from?”\nThe Count dismissed the question with a wave of the hand.\n“The surest way to insult a Walloon is to mistake him for a Frenchman,\nthough they live but a few miles apart and share the same language.”\n“I suppose that’s true,” the colonel conceded. “Nonetheless. I’m\ninterested in your guesswork; and I promise I won’t be insulted.”\nThe Count took a sip of his wine and returned the glass to the table.\n“You are almost certainly from eastern Georgia.”\nThe captain sat up with an expression of enthusiasm.\n“Extraordinary. Do I have an accent?”\n“Not that’s distinguishable. But then armies, like universities, are where\naccents are most commonly shed.”\n“Then why eastern Georgia?”\nThe Count gestured to the wine.\n“Only an eastern Georgian would start his meal with a bottle of\nRkatsiteli.”\n“Because he’s a hayseed?”\n“Because he misses home.”\nThe colonel laughed again.\n“What a canny fellow you are.”', 'In replying It is nothing to Andrey, the Count was not simply resorting to a\nGallic figure of speech. In point of fact, the Count deserved as much thanks\nfor his little intervention as a swallow deserves for its trill. For since the age\nof fifteen, Alexander Rostov had been a master of seating tables.\nWhenever he was home for the holidays, his grandmother would\ninevitably call him into the library, where she liked to knit by the fireplace\nin a solitary chair.\n“Come in, my boy, and sit with me a moment.”\n“Certainly, Grandmother,” replied the Count, balancing himself on the\nedge of the fire grate. “How can I be of assistance?”\n“The prelate is coming for dinner on Friday night—as are the Duchess\nObolensky, Count Keragin, and the Minsky-Polotovs. . . .”\nHere she would let her voice trail off without further explanation; but no\nfurther explanation was needed. The Countess was of a mind that dinner\nshould provide one with respite from life’s trials and tribulations. Thus, she\ncould not countenance discussions of religion, politics, or personal sorrows\nat her table. Further complicating matters, the prelate was deaf in his left\near, partial to Latin epigrams, and prone to stare at décolletage whenever he\ndrank a glass of wine; while the Duchess Obolensky, who was particularly\ncaustic in summer, frowned upon pithy sayings and could not abide\ndiscussions of the arts. And the Keragins? Their great-grandfather had been\ncalled a Bonapartist by Prince Minsky-Polotov in 1811, and they had not\nexchanged a word with a Minsky-Polotov since.\n“How many will be in attendance?” asked the Count.\n“Forty.”\n“The usual assembly?”\n“More or less.”\n“The Osipovs?”\n“Yes. But Pierre is in Moscow. . . .”\n“Ah,” said the Count with the smile of the chess champion who has been\nconfronted with a new gambit.\nThe Nizhny Novgorod Province had a hundred prominent families,\nwhich over the course of two centuries had intermarried and divorced,', '21 June 1922\nAPPEARANCE OF COUNT ALEXANDER ILYICH ROSTOV\nBEFORE THE EMERGENCY COMMITTEE OF THE PEOPLE’S COMMISSARIAT FOR INTERNAL AFFAIRS\nPresiding: Comrades V. A. Ignatov, M. S. Zakovsky, A. N. Kosarev\nProsecuting: A. Y. Vyshinsky\nProsecutor Vyshinsky: State your name.\nRostov: Count Alexander Ilyich Rostov, recipient of the Order of Saint Andrew,\nmember of the Jockey Club, Master of the Hunt.\nVyshinsky: You may have your titles; they are of no use to anyone else. But for the\nrecord, are you not Alexander Rostov, born in St. Petersburg, 24 October 1889?\nRostov: I am he.\nVyshinsky: Before we begin, I must say, I do not think that I have ever seen a\njacket festooned with so many buttons.\nRostov: Thank you.\nVyshinsky: It was not meant as a compliment.\nRostov: In that case, I demand satisfaction on the field of honor.\n[Laughter.]\nSecretary Ignatov: Silence in the gallery.\nVyshinsky: What is your current address?\nRostov: Suite 317 at the Hotel Metropol, Moscow.\nVyshinsky: How long have you lived there?\nRostov: I have been in residence since the fifth of September 1918. Just under four\nyears.\nVyshinsky: And your occupation?\nRostov: It is not the business of gentlemen to have occupations.\nVyshinsky: Very well then. How do you spend your time?\nRostov: Dining, discussing. Reading, reflecting. The usual rigmarole.\nVyshinsky: And you write poetry?\nRostov: I have been known to fence with a quill.\nVyshinsky: [Holding up a pamphlet] Are you the author of this long poem of 1913:\nWhere Is It Now?\nRostov: It has been attributed to me.\nVyshinsky: Why did you write the poem?\nRostov: It demanded to be written. I simply happened to be sitting at the particular\ndesk on the particular morning when it chose to make its demands.\nVyshinsky: And where was that exactly?\nRostov: In the south parlor at Idlehour.\nVyshinsky: Idlehour?\nRostov: The Rostov estate in Nizhny Novgorod.\nVyshinsky: Ah, yes. Of course. How apt. But let us return our attention to your\npoem. Coming as it did—in the more subdued years after the failed revolt of 1905—\nmany considered it a call to action. Would you agree with that assessment?\nRostov: All poetry is a call to action.\nVyshinsky: [Checking notes] And it was in the spring of the following year that you\nleft Russia for Paris . . . ?\nRostov: I seem to remember blossoms on the apple trees. So, yes, in all likelihood\nit was spring.\nVyshinsky: May 16 to be precise. Now, we understand the reasons for your self-\nimposed exile; and we even have some sympathy with the actions that prompted your\nflight. What concerns us here is your return in 1918. One wonders if you came back\nwith the intention of taking up arms and, if so, whether for or against the\nRevolution.']","Count Rostov described his occupation during the trial as ""It is not the business of gentlemen to have occupations.""","Count Rostov stated, “It is not the business of gentlemen to have occupations,” and described his time as spent dining, discussing, reading, and reflecting.,",0.5,0.0,0.3333333333,1.0,0.0,0.44988466047865183
"What did the Count take with him to his attic quarters after his house arrest began?,","['Later that night, as he sat alone on his bed, the Count mulled over his visit\nto his old suite.\nWhat had stayed with him was not the sight of his family’s clock still\nticking by the door, nor the grandeur of the architecture, nor even the view\nfrom the northwest window. What had lingered with him was the sight of\nthe tea service on the table beside the folded paper.\nThat little tableaux, for all its innocence, was somehow suggestive of\nexactly what had been bearing down on the Count’s soul. For he understood\nevery aspect of the scene at a glance. Having returned from some outing at\nfour o’clock and having hung his jacket on the back of a chair, the room’s\ncurrent resident had called for tea and an afternoon edition. Then he had\nsettled himself down on the couch to while away a civilized hour before it\nwas time to dress for dinner. In other words, what the Count had observed\nin suite 317 was not simply an afternoon tea, but a moment in the daily life\nof a gentleman at liberty.\nIn light of these thoughts, the Count reviewed his new room—the one\nhundred square feet that had been assigned to him. Never had it seemed so\nsmall. The bed crowded the coffee table, the coffee table crowded the high-\nback chair, and the high-back chair had to be shoved aside every time one\nwished to open the closet. Simply put, there was not enough space to\naccommodate such a civilized hour.\nBut as the Count gazed around him with this forlorn thought, a voice\nonly half his own reminded him that in the Metropol there were rooms\nbehind rooms, and doors behind doors. . . .\nRising from his bed, the Count navigated his way around his\ngrandmother’s coffee table, set aside the high-back chair, and stood before\nhis telephone box of a closet. Running along the perimeter of where the\ncloset met the wall was an elegant molding. The Count had always thought\nthis flourish a little excessive; but what if the closet had been built in an old\ndoorframe? Opening the door, the Count parted his clothes and tentatively\nrapped on the back wall. The sound was promisingly thin. With three\nfingers he gave the barrier a push and could feel it flex. He took all his\njackets out and dumped them on the bed. Then holding the jambs of the\ndoor he kicked the inner wall with his heel. There came a pleasant crack.\nLeaning back, he kicked again and again until the barrier splintered. Then\nhe pulled the jagged planks back into his room and slipped through the gap.', 'finer buildings had been lifted and set back, most had been razed and\nreplaced with towers, in accordance with a new ordinance that buildings on\nfirst-rate streets stand at least ten stories tall. As a result, the Count would\nhave had to strain to pick out other familiar landmarks as the van moved\nalong. But he had stopped looking for what was familiar, and instead was\nwatching the blur of facades and street lamps receding rapidly from his\nview, as if they were being pulled into the distance.\nBack in the attic of the Metropol, the Count found his door still open and\nMontaigne on the floor. Picking up his father’s book, the Count sat down on\nSofia’s bed. Then for the first time that night, he let himself weep, his chest\nheaving lightly with the release. But if tears fell freely down his face, they\nwere not tears of grief. They were the tears of the luckiest man in all of\nRussia.\nAfter a few minutes, the Count breathed deeply and felt a sense of\npeace. Realizing that his father’s book was still in his hand, he rose from\nSofia’s bed to set it down—and that’s when he saw the black leather case\nthat had been left on the Grand Duke’s desk. It was about a foot square and\nsix inches high with a handle in leather and clasps of chrome. Taped on top\nwas a note addressed to him in an unfamiliar script. Pulling the note free,\nthe Count unfolded it and read:\nAlexander,\nWhat a pleasure meeting you tonight. As I mentioned, I am\nheaded home for a spell. In the meantime, I thought you could make\ngood use of this. You might pay special attention to the contents in\nthe uppermost sleeve, as I think you will find it very apropos of our\nchat.\nWith warm regards till next we meet, \nRichard Vanderwhile\nThrowing the clasps, the Count opened the lid of the case. It was a\nportable phonograph. Inside there was a small stack of records in brown', 'corridor servicing a bathroom and six bedrooms reminiscent of monastic\ncells. This attic was originally built to house the butlers and ladies’ maids of\nthe Metropol’s guests; but when the practice of traveling with servants fell\nout of fashion, the unused rooms had been claimed by the caprices of casual\nurgency—thenceforth warehousing scraps of lumber, broken furniture, and\nother assorted debris.\nEarlier that day, the room closest to the stairwell had been cleared of all\nbut a cast-iron bed, a three-legged bureau, and a decade of dust. In the\ncorner near the door was a small closet, rather like a telephone box, that had\nbeen dropped in the room as an afterthought. Reflecting the pitch of the\nroof, the ceiling sloped at a gradual incline as it moved away from the door,\nsuch that at the room’s outer wall the only place where the Count could\nstand to his full height was where a dormer accommodated a window the\nsize of a chessboard.\nAs the two guards looked on smugly from the hall, the good captain\nexplained that he had summoned the bellhops to help the Count move what\nfew belongings his new quarters would accommodate.\n“And the rest?”\n“Becomes the property of the People.”\nSo this is their game, thought the Count.\n“Very well.”\nBack down the belfry he skipped as the guards hurried behind him, their\nrifles clacking against the wall. On the third floor, he marched along the\nhallway and into his suite where the two bellhops looked up with woeful\nexpressions.\n“It’s all right, fellows,” the Count assured and then began pointing:\n“This. That. Those. All the books.”\nAmong the furnishings destined for his new quarters, the Count chose\ntwo high-back chairs, his grandmother’s oriental coffee table, and a favorite\nset of her porcelain plates. He chose the two table lamps fashioned from\nebony elephants and the portrait of his sister, Helena, which Serov had\npainted during a brief stay at Idlehour in 1908. He did not forget the leather\ncase that had been fashioned especially for him by Asprey in London and\nwhich his good friend Mishka had so appropriately christened the\nAmbassador.']","The Count took two high-back chairs, his grandmother’s oriental coffee table, a favorite set of her porcelain plates, two table lamps fashioned from ebony elephants, a portrait of his sister Helena, and a leather case that had been fashioned especially for him by Asprey in London.","The Count took two high-back chairs, his grandmother’s coffee table, a favorite set of plates, two elephant lamps, a portrait of his sister, and his books.,",1.0,0.9104751585447176,0.3333333333,,0.14285714265306124,0.8665105198901139
"What was Count Rostov’s height?,","['A\n1922\nAn Ambassador\nt half past six on the twenty-first of June 1922, when Count Alexander\nIlyich Rostov was escorted through the gates of the Kremlin onto Red\nSquare, it was glorious and cool. Drawing his shoulders back without\nbreaking stride, the Count inhaled the air like one fresh from a swim. The\nsky was the very blue that the cupolas of St. Basil’s had been painted for.\nTheir pinks, greens, and golds shimmered as if it were the sole purpose of a\nreligion to cheer its Divinity. Even the Bolshevik girls conversing before\nthe windows of the State Department Store seemed dressed to celebrate the\nlast days of spring.\n“Hello, my good man,” the Count called to Fyodor, at the edge of the\nsquare. “I see the blackberries have come in early this year!”\nGiving the startled fruit seller no time to reply, the Count walked briskly\non, his waxed moustaches spread like the wings of a gull. Passing through\nResurrection Gate, he turned his back on the lilacs of the Alexander\nGardens and proceeded toward Theatre Square, where the Hotel Metropol\nstood in all its glory. When he reached the threshold, the Count gave a wink\nto Pavel, the afternoon doorman, and turned with a hand outstretched to the\ntwo soldiers trailing behind him.\n“Thank you, gentlemen, for delivering me safely. I shall no longer be in\nneed of your assistance.”\nThough strapping lads, both of the soldiers had to look up from under\ntheir caps to return the Count’s gaze—for like ten generations of Rostov\nmen, the Count stood an easy six foot three.\n“On you go,” said the more thuggish of the two, his hand on the butt of\nhis rifle. “We’re to see you to your rooms.”', 'Alexander Rostov. A man inclined to see the best in all of us.”\n“Hear! Hear!”\n“And you needn’t worry, Papa,” Sofia continued. “For no matter who\ncomes knocking at our door, I have no intention of ever leaving the\nMetropol.”\nAfter joining in a cheer, the members of the gathering emptied their\nglasses, stumbled through the closet, and exited into the hall. Opening the\ndoor to the belfry, the Count gave a slight bow and gestured for everyone to\nproceed. But just as the Count was about to follow the others into the\nstairwell, a woman in late middle age with a satchel on her shoulder and a\nkerchief in her hair stepped from the shadows at the end of the hall. Though\nthe Count had never seen her before, it was clear from her demeanor that\nshe had been waiting to speak with him alone.\n“Andrey,” the Count called into the belfry, “I’ve forgotten something in\nthe room. You all go ahead. I’ll be down in a moment. . . .”\nOnly when the last sound of voices had receded down the stairs did the\nwoman approach. In the light, the Count could see that she had an almost\nsevere beauty about her—like one for whom there would be no half\nmeasures in matters of the heart.\n“I’m Katerina Litvinova,” she said without a smile.\nIt took a moment for the Count to realize that this was none other than\nMishka’s Katerina, the poet from Kiev whom he had lived with back in the\n1920s.\n“Katerina Litvinova! How extraordinary. To what do I owe—”\n“Is there somewhere we could talk?”\n“Why, yes . . . Of course . . .”\nThe Count led Katerina into the bedroom and then, after a moment’s\nhesitation, took her through the jackets into the study. Apparently, he\nneedn’t have hesitated, for she looked around the room as one who had\nheard descriptions of it before, nodding lightly to herself as her gaze shifted\nfrom the bookcase to the coffee table to the Ambassador. Taking her satchel\nfrom her shoulder, she suddenly appeared tired.\n“Here,” said the Count, offering a chair.\nShe sat down, putting the satchel in her lap. Then passing a hand over\nher head, she removed her kerchief, revealing light brown hair cut as short\nas a man’s.', '21 June 1922\nAPPEARANCE OF COUNT ALEXANDER ILYICH ROSTOV\nBEFORE THE EMERGENCY COMMITTEE OF THE PEOPLE’S COMMISSARIAT FOR INTERNAL AFFAIRS\nPresiding: Comrades V. A. Ignatov, M. S. Zakovsky, A. N. Kosarev\nProsecuting: A. Y. Vyshinsky\nProsecutor Vyshinsky: State your name.\nRostov: Count Alexander Ilyich Rostov, recipient of the Order of Saint Andrew,\nmember of the Jockey Club, Master of the Hunt.\nVyshinsky: You may have your titles; they are of no use to anyone else. But for the\nrecord, are you not Alexander Rostov, born in St. Petersburg, 24 October 1889?\nRostov: I am he.\nVyshinsky: Before we begin, I must say, I do not think that I have ever seen a\njacket festooned with so many buttons.\nRostov: Thank you.\nVyshinsky: It was not meant as a compliment.\nRostov: In that case, I demand satisfaction on the field of honor.\n[Laughter.]\nSecretary Ignatov: Silence in the gallery.\nVyshinsky: What is your current address?\nRostov: Suite 317 at the Hotel Metropol, Moscow.\nVyshinsky: How long have you lived there?\nRostov: I have been in residence since the fifth of September 1918. Just under four\nyears.\nVyshinsky: And your occupation?\nRostov: It is not the business of gentlemen to have occupations.\nVyshinsky: Very well then. How do you spend your time?\nRostov: Dining, discussing. Reading, reflecting. The usual rigmarole.\nVyshinsky: And you write poetry?\nRostov: I have been known to fence with a quill.\nVyshinsky: [Holding up a pamphlet] Are you the author of this long poem of 1913:\nWhere Is It Now?\nRostov: It has been attributed to me.\nVyshinsky: Why did you write the poem?\nRostov: It demanded to be written. I simply happened to be sitting at the particular\ndesk on the particular morning when it chose to make its demands.\nVyshinsky: And where was that exactly?\nRostov: In the south parlor at Idlehour.\nVyshinsky: Idlehour?\nRostov: The Rostov estate in Nizhny Novgorod.\nVyshinsky: Ah, yes. Of course. How apt. But let us return our attention to your\npoem. Coming as it did—in the more subdued years after the failed revolt of 1905—\nmany considered it a call to action. Would you agree with that assessment?\nRostov: All poetry is a call to action.\nVyshinsky: [Checking notes] And it was in the spring of the following year that you\nleft Russia for Paris . . . ?\nRostov: I seem to remember blossoms on the apple trees. So, yes, in all likelihood\nit was spring.\nVyshinsky: May 16 to be precise. Now, we understand the reasons for your self-\nimposed exile; and we even have some sympathy with the actions that prompted your\nflight. What concerns us here is your return in 1918. One wonders if you came back\nwith the intention of taking up arms and, if so, whether for or against the\nRevolution.']",Count Rostov's height was six foot three.,"Count Rostov stood an easy six foot three, as was traditional for Rostov men.,",1.0,0.9639312210466563,0.9999999999,1.0,0.0,0.7404169927036149
"How did Count Rostov respond to Vyshinsky’s accusations of aristocratic corruption?,","['In replying It is nothing to Andrey, the Count was not simply resorting to a\nGallic figure of speech. In point of fact, the Count deserved as much thanks\nfor his little intervention as a swallow deserves for its trill. For since the age\nof fifteen, Alexander Rostov had been a master of seating tables.\nWhenever he was home for the holidays, his grandmother would\ninevitably call him into the library, where she liked to knit by the fireplace\nin a solitary chair.\n“Come in, my boy, and sit with me a moment.”\n“Certainly, Grandmother,” replied the Count, balancing himself on the\nedge of the fire grate. “How can I be of assistance?”\n“The prelate is coming for dinner on Friday night—as are the Duchess\nObolensky, Count Keragin, and the Minsky-Polotovs. . . .”\nHere she would let her voice trail off without further explanation; but no\nfurther explanation was needed. The Countess was of a mind that dinner\nshould provide one with respite from life’s trials and tribulations. Thus, she\ncould not countenance discussions of religion, politics, or personal sorrows\nat her table. Further complicating matters, the prelate was deaf in his left\near, partial to Latin epigrams, and prone to stare at décolletage whenever he\ndrank a glass of wine; while the Duchess Obolensky, who was particularly\ncaustic in summer, frowned upon pithy sayings and could not abide\ndiscussions of the arts. And the Keragins? Their great-grandfather had been\ncalled a Bonapartist by Prince Minsky-Polotov in 1811, and they had not\nexchanged a word with a Minsky-Polotov since.\n“How many will be in attendance?” asked the Count.\n“Forty.”\n“The usual assembly?”\n“More or less.”\n“The Osipovs?”\n“Yes. But Pierre is in Moscow. . . .”\n“Ah,” said the Count with the smile of the chess champion who has been\nconfronted with a new gambit.\nThe Nizhny Novgorod Province had a hundred prominent families,\nwhich over the course of two centuries had intermarried and divorced,', '21 June 1922\nAPPEARANCE OF COUNT ALEXANDER ILYICH ROSTOV\nBEFORE THE EMERGENCY COMMITTEE OF THE PEOPLE’S COMMISSARIAT FOR INTERNAL AFFAIRS\nPresiding: Comrades V. A. Ignatov, M. S. Zakovsky, A. N. Kosarev\nProsecuting: A. Y. Vyshinsky\nProsecutor Vyshinsky: State your name.\nRostov: Count Alexander Ilyich Rostov, recipient of the Order of Saint Andrew,\nmember of the Jockey Club, Master of the Hunt.\nVyshinsky: You may have your titles; they are of no use to anyone else. But for the\nrecord, are you not Alexander Rostov, born in St. Petersburg, 24 October 1889?\nRostov: I am he.\nVyshinsky: Before we begin, I must say, I do not think that I have ever seen a\njacket festooned with so many buttons.\nRostov: Thank you.\nVyshinsky: It was not meant as a compliment.\nRostov: In that case, I demand satisfaction on the field of honor.\n[Laughter.]\nSecretary Ignatov: Silence in the gallery.\nVyshinsky: What is your current address?\nRostov: Suite 317 at the Hotel Metropol, Moscow.\nVyshinsky: How long have you lived there?\nRostov: I have been in residence since the fifth of September 1918. Just under four\nyears.\nVyshinsky: And your occupation?\nRostov: It is not the business of gentlemen to have occupations.\nVyshinsky: Very well then. How do you spend your time?\nRostov: Dining, discussing. Reading, reflecting. The usual rigmarole.\nVyshinsky: And you write poetry?\nRostov: I have been known to fence with a quill.\nVyshinsky: [Holding up a pamphlet] Are you the author of this long poem of 1913:\nWhere Is It Now?\nRostov: It has been attributed to me.\nVyshinsky: Why did you write the poem?\nRostov: It demanded to be written. I simply happened to be sitting at the particular\ndesk on the particular morning when it chose to make its demands.\nVyshinsky: And where was that exactly?\nRostov: In the south parlor at Idlehour.\nVyshinsky: Idlehour?\nRostov: The Rostov estate in Nizhny Novgorod.\nVyshinsky: Ah, yes. Of course. How apt. But let us return our attention to your\npoem. Coming as it did—in the more subdued years after the failed revolt of 1905—\nmany considered it a call to action. Would you agree with that assessment?\nRostov: All poetry is a call to action.\nVyshinsky: [Checking notes] And it was in the spring of the following year that you\nleft Russia for Paris . . . ?\nRostov: I seem to remember blossoms on the apple trees. So, yes, in all likelihood\nit was spring.\nVyshinsky: May 16 to be precise. Now, we understand the reasons for your self-\nimposed exile; and we even have some sympathy with the actions that prompted your\nflight. What concerns us here is your return in 1918. One wonders if you came back\nwith the intention of taking up arms and, if so, whether for or against the\nRevolution.', 'A\n1922\nAn Ambassador\nt half past six on the twenty-first of June 1922, when Count Alexander\nIlyich Rostov was escorted through the gates of the Kremlin onto Red\nSquare, it was glorious and cool. Drawing his shoulders back without\nbreaking stride, the Count inhaled the air like one fresh from a swim. The\nsky was the very blue that the cupolas of St. Basil’s had been painted for.\nTheir pinks, greens, and golds shimmered as if it were the sole purpose of a\nreligion to cheer its Divinity. Even the Bolshevik girls conversing before\nthe windows of the State Department Store seemed dressed to celebrate the\nlast days of spring.\n“Hello, my good man,” the Count called to Fyodor, at the edge of the\nsquare. “I see the blackberries have come in early this year!”\nGiving the startled fruit seller no time to reply, the Count walked briskly\non, his waxed moustaches spread like the wings of a gull. Passing through\nResurrection Gate, he turned his back on the lilacs of the Alexander\nGardens and proceeded toward Theatre Square, where the Hotel Metropol\nstood in all its glory. When he reached the threshold, the Count gave a wink\nto Pavel, the afternoon doorman, and turned with a hand outstretched to the\ntwo soldiers trailing behind him.\n“Thank you, gentlemen, for delivering me safely. I shall no longer be in\nneed of your assistance.”\nThough strapping lads, both of the soldiers had to look up from under\ntheir caps to return the Count’s gaze—for like ten generations of Rostov\nmen, the Count stood an easy six foot three.\n“On you go,” said the more thuggish of the two, his hand on the butt of\nhis rifle. “We’re to see you to your rooms.”']",Count Rostov responded to Vyshinsky's accusations of aristocratic corruption with humor and wit. He did not take the accusations seriously and even demanded satisfaction on the field of honor in a joking manner.,"Count Rostov sarcastically said, “The Tsarina had the same complaints about me in her day,” showcasing his charm and wit.,",1.0,0.9764537965999319,0.49999999995,0.0,0.0,0.8224719022961264
"What punishment did Count Rostov face if he left the Metropol Hotel?,","['As Viktor took a sip from his coffee, the Count watched the accordion\nplayer with interest.\n“Have you ever seen Casablanca?” he asked.\nSomewhat bewildered, Viktor admitted that he had not.\n“Ah. You must see it one day.”\nAnd so the Count told Viktor about his friend Osip and their recent\nviewing of the movie. In particular, he described the scene in which a small-\ntime crook was dragged away by the police and how the American\nsaloonkeeper, having assured his customers that everything was all right,\ncasually instructed his bandleader to play on.\n“My friend was very impressed with this,” explained the Count. “He\nsaw the saloonkeeper’s instruction to the piano player to start playing so\nsoon after the arrest as evidence of his indifference to the fates of other\nmen. But I wonder. . . .”\nThe following morning at half past eleven, two officers of the KGB arrived\nat the Metropol Hotel in order to question Headwaiter Alexander Rostov on\nan undisclosed matter.\nHaving been escorted by a bellhop to Rostov’s room on the sixth floor,\nthe officers found no sign of him there. Nor was he receiving a trim in the\nbarbershop, lunching at the Piazza, or reading the papers in the lobby.\nSeveral of Rostov’s closest associates, including Chef Zhukovsky and\nMaître d’ Duras, were questioned, but none had seen Rostov since the\nprevious night. (The officers also endeavored to speak with the hotel’s\nmanager, only to find that he had not yet reported to work—a fact that was\nduly noted in his file!) At one o’clock, two additional KGB men were\nsummoned so that a more thorough search could be made of the hotel. At\ntwo, the senior officer conducting the investigation was encouraged to\nspeak with Vasily, the concierge. Finding him at his desk in the lobby\n(where he was in the midst of securing theater tickets for a guest), the\nofficer did not beat about the bush. He put his question to the concierge\nunambiguously:\n“Do you know the whereabouts of Alexander Rostov?”\nTo which the concierge replied: “I haven’t the slightest idea.”', 'A\n1922\nAn Ambassador\nt half past six on the twenty-first of June 1922, when Count Alexander\nIlyich Rostov was escorted through the gates of the Kremlin onto Red\nSquare, it was glorious and cool. Drawing his shoulders back without\nbreaking stride, the Count inhaled the air like one fresh from a swim. The\nsky was the very blue that the cupolas of St. Basil’s had been painted for.\nTheir pinks, greens, and golds shimmered as if it were the sole purpose of a\nreligion to cheer its Divinity. Even the Bolshevik girls conversing before\nthe windows of the State Department Store seemed dressed to celebrate the\nlast days of spring.\n“Hello, my good man,” the Count called to Fyodor, at the edge of the\nsquare. “I see the blackberries have come in early this year!”\nGiving the startled fruit seller no time to reply, the Count walked briskly\non, his waxed moustaches spread like the wings of a gull. Passing through\nResurrection Gate, he turned his back on the lilacs of the Alexander\nGardens and proceeded toward Theatre Square, where the Hotel Metropol\nstood in all its glory. When he reached the threshold, the Count gave a wink\nto Pavel, the afternoon doorman, and turned with a hand outstretched to the\ntwo soldiers trailing behind him.\n“Thank you, gentlemen, for delivering me safely. I shall no longer be in\nneed of your assistance.”\nThough strapping lads, both of the soldiers had to look up from under\ntheir caps to return the Count’s gaze—for like ten generations of Rostov\nmen, the Count stood an easy six foot three.\n“On you go,” said the more thuggish of the two, his hand on the butt of\nhis rifle. “We’re to see you to your rooms.”', 'Russia galled his sense of justice. Or maybe it was simply the cold\nsatisfaction of the envious. For those who had difficulty in school or at\nmaking friends when they were young will forever recognize with a bitter\nglance those for whom life has seemed to come easy.\nGloating, righteousness, satisfaction, who can say? But the emotion the\nBishop felt upon opening the door to his office was almost certainly that of\nshock—for the adversary that he had left in the attic just minutes before was\nnow sitting behind the manager’s desk with a pistol in his hand.\nHow was this possible?\nWhen the Bishop left the Count’s bedroom, the Count was frozen in\nplace by a torrent of emotions—by feelings of fury, incredulity, self-\nrecrimination, and fear. Rather than burn the map, like a fool he had slipped\nit in his drawer. Six months of the most careful planning and painstaking\nexecution overturned by a single misstep. And what was worse, he had put\nSofia at risk. What price was she to pay for his carelessness?\nBut if the Count was frozen in place, he was frozen for all of five\nseconds. For these perfectly understandable sentiments, which threatened to\ndrain the blood from his heart, were swept aside by resolve.\nTurning on his heels, the Count went to the head of the belfry and\nlistened until the Bishop had descended the first two flights of stairs. Still in\nhis stocking feet, the Count began to follow in the Bishop’s footsteps; but\nwhen he got to the fifth floor, he exited the belfry, sped down the hallway,\nand ran down the main staircase, just as Sofia had at the age of thirteen.\nAs if he were still enshrouded in a mist, when the Count alit from the\nstairs, he ran down the hall and entered the executive offices without being\nseen by a soul; but upon reaching the Bishop’s door, he discovered it was\nlocked. Even as he was taking the Lord’s name in vain, the Count slapped\nhis hands against his vest with relief. For he still had Nina’s passkey in his\npocket. Letting himself in, the Count relocked the door and crossed to the\nwall where the filing cabinets had taken the place of Mr. Halecki’s chaise.\nCounting from the portrait of Karl Marx, the Count placed his hand in the\ncenter of the second panel to the right, gave a push, and popped it open.']",It is not mentioned in the context what punishment Count Rostov would face if he left the Metropol Hotel.,"The committee warned that if Count Rostov ever set foot outside the Metropol Hotel again, he would be shot.,",1.0,0.0,0.0,0.0,0.4999999975,0.5195017329506002
"How did the Count’s room in the attic compare to his previous quarters?,","['corridor servicing a bathroom and six bedrooms reminiscent of monastic\ncells. This attic was originally built to house the butlers and ladies’ maids of\nthe Metropol’s guests; but when the practice of traveling with servants fell\nout of fashion, the unused rooms had been claimed by the caprices of casual\nurgency—thenceforth warehousing scraps of lumber, broken furniture, and\nother assorted debris.\nEarlier that day, the room closest to the stairwell had been cleared of all\nbut a cast-iron bed, a three-legged bureau, and a decade of dust. In the\ncorner near the door was a small closet, rather like a telephone box, that had\nbeen dropped in the room as an afterthought. Reflecting the pitch of the\nroof, the ceiling sloped at a gradual incline as it moved away from the door,\nsuch that at the room’s outer wall the only place where the Count could\nstand to his full height was where a dormer accommodated a window the\nsize of a chessboard.\nAs the two guards looked on smugly from the hall, the good captain\nexplained that he had summoned the bellhops to help the Count move what\nfew belongings his new quarters would accommodate.\n“And the rest?”\n“Becomes the property of the People.”\nSo this is their game, thought the Count.\n“Very well.”\nBack down the belfry he skipped as the guards hurried behind him, their\nrifles clacking against the wall. On the third floor, he marched along the\nhallway and into his suite where the two bellhops looked up with woeful\nexpressions.\n“It’s all right, fellows,” the Count assured and then began pointing:\n“This. That. Those. All the books.”\nAmong the furnishings destined for his new quarters, the Count chose\ntwo high-back chairs, his grandmother’s oriental coffee table, and a favorite\nset of her porcelain plates. He chose the two table lamps fashioned from\nebony elephants and the portrait of his sister, Helena, which Serov had\npainted during a brief stay at Idlehour in 1908. He did not forget the leather\ncase that had been fashioned especially for him by Asprey in London and\nwhich his good friend Mishka had so appropriately christened the\nAmbassador.', 'Later that night, as he sat alone on his bed, the Count mulled over his visit\nto his old suite.\nWhat had stayed with him was not the sight of his family’s clock still\nticking by the door, nor the grandeur of the architecture, nor even the view\nfrom the northwest window. What had lingered with him was the sight of\nthe tea service on the table beside the folded paper.\nThat little tableaux, for all its innocence, was somehow suggestive of\nexactly what had been bearing down on the Count’s soul. For he understood\nevery aspect of the scene at a glance. Having returned from some outing at\nfour o’clock and having hung his jacket on the back of a chair, the room’s\ncurrent resident had called for tea and an afternoon edition. Then he had\nsettled himself down on the couch to while away a civilized hour before it\nwas time to dress for dinner. In other words, what the Count had observed\nin suite 317 was not simply an afternoon tea, but a moment in the daily life\nof a gentleman at liberty.\nIn light of these thoughts, the Count reviewed his new room—the one\nhundred square feet that had been assigned to him. Never had it seemed so\nsmall. The bed crowded the coffee table, the coffee table crowded the high-\nback chair, and the high-back chair had to be shoved aside every time one\nwished to open the closet. Simply put, there was not enough space to\naccommodate such a civilized hour.\nBut as the Count gazed around him with this forlorn thought, a voice\nonly half his own reminded him that in the Metropol there were rooms\nbehind rooms, and doors behind doors. . . .\nRising from his bed, the Count navigated his way around his\ngrandmother’s coffee table, set aside the high-back chair, and stood before\nhis telephone box of a closet. Running along the perimeter of where the\ncloset met the wall was an elegant molding. The Count had always thought\nthis flourish a little excessive; but what if the closet had been built in an old\ndoorframe? Opening the door, the Count parted his clothes and tentatively\nrapped on the back wall. The sound was promisingly thin. With three\nfingers he gave the barrier a push and could feel it flex. He took all his\njackets out and dumped them on the bed. Then holding the jambs of the\ndoor he kicked the inner wall with his heel. There came a pleasant crack.\nLeaning back, he kicked again and again until the barrier splintered. Then\nhe pulled the jagged planks back into his room and slipped through the gap.', 'He was now inside a dark, narrow space that smelled of dry cedar,\npresumably the interior of the neighboring closet. Taking a breath, he turned\nthe knob, opened the door, and entered a room that was the mirror image of\nhis own—but in which five unused bedframes had been stored. At some\npoint, two of the frames, which had been leaning against the wall, had\nfallen, pinning the hallway door shut. Pulling the frames aside, the Count\nopened the door, dragged everything out of the room, and began to\nrefurnish.\nFirst, he reunited the two high-back chairs with his grandmother’s coffee\ntable. Then, taking the belfry stairs, he went down to the basement. From\nthe cabinet of curiosities, he retrieved one of his rugs, the standing lamp,\nand the small bookcase in three separate trips. Then vaulting the steps two\nat a time, he made one final visit in order to claim ten of the weighty novels\nthat had been abandoned. Once his new study was furnished, he went down\nthe hall and borrowed the roofer’s hammer and five nails.\nThe Count had not wielded a hammer since he was a boy at Idlehour\nwhen he would help Tikhon, the old caretaker, repair the fencing in the first\nweeks of spring. What a fine feeling it had been to bring the hammer down\nsquarely on the head of a nail, driving it through a plank into a fence post as\nthe impact echoed in the morning air. But on the very first stroke of this\nhammer what the Count squarely hit was the back of his thumb. (Lest you\nhave forgotten, it is quite excruciating to hammer the back of your thumb. It\ninevitably prompts a hopping up and down and the taking of the Lord’s\nname in vain.)\nBut Fortune does favor the bold. So, while the next swing of the\nhammer glanced off the nail’s head, on the third the Count hit home; and by\nthe second nail, he had recovered the rhythm of set, drive, and sink—that\nancient cadence which is not to be found in quadrilles, or hexameters, or in\nVronsky’s saddlebags!\nSuffice it to say that within half an hour four of the nails had been driven\nthrough the edge of the door into the doorframe—such that from that\nmoment forward the only access to the Count’s new room would be through\nthe sleeves of his jackets. The fifth nail he saved for the wall above the\nbookcase so that he could hang the portrait of his sister.\nHis work completed, the Count sat down in one of the high-back chairs\nand felt an almost surprising sense of bliss. The Count’s bedroom and this']",The Count's room in the attic was much smaller and more cramped compared to his previous quarters.,"The attic room was small, with a sloped ceiling, a cast-iron bed, a three-legged bureau, and a small window the size of a chessboard.,",,0.9780727157527215,0.99999999995,1.0,0.16666666638888888,0.600438181243018
"What book did Count Rostov decide to read while in his attic quarters?,","['finer buildings had been lifted and set back, most had been razed and\nreplaced with towers, in accordance with a new ordinance that buildings on\nfirst-rate streets stand at least ten stories tall. As a result, the Count would\nhave had to strain to pick out other familiar landmarks as the van moved\nalong. But he had stopped looking for what was familiar, and instead was\nwatching the blur of facades and street lamps receding rapidly from his\nview, as if they were being pulled into the distance.\nBack in the attic of the Metropol, the Count found his door still open and\nMontaigne on the floor. Picking up his father’s book, the Count sat down on\nSofia’s bed. Then for the first time that night, he let himself weep, his chest\nheaving lightly with the release. But if tears fell freely down his face, they\nwere not tears of grief. They were the tears of the luckiest man in all of\nRussia.\nAfter a few minutes, the Count breathed deeply and felt a sense of\npeace. Realizing that his father’s book was still in his hand, he rose from\nSofia’s bed to set it down—and that’s when he saw the black leather case\nthat had been left on the Grand Duke’s desk. It was about a foot square and\nsix inches high with a handle in leather and clasps of chrome. Taped on top\nwas a note addressed to him in an unfamiliar script. Pulling the note free,\nthe Count unfolded it and read:\nAlexander,\nWhat a pleasure meeting you tonight. As I mentioned, I am\nheaded home for a spell. In the meantime, I thought you could make\ngood use of this. You might pay special attention to the contents in\nthe uppermost sleeve, as I think you will find it very apropos of our\nchat.\nWith warm regards till next we meet, \nRichard Vanderwhile\nThrowing the clasps, the Count opened the lid of the case. It was a\nportable phonograph. Inside there was a small stack of records in brown', 'good sense than by a rather childish impulse to impress the bellhops and put\nthe guards in their place. For the books were not even to the Count’s taste.\nHis personal library of majestic narratives by the likes of Balzac, Dickens,\nand Tolstoy had been left behind in Paris. The books the bellhops had\nlugged to the attic had been his father’s and, devoted as they were to studies\nof rational philosophy and the science of modern agriculture, each promised\nheft and threatened impenetrability.\nWithout a doubt, one more winnowing was called for.\nSo, having broken his fast, bathed, and dressed, the Count went about\nthe business. First, he tried the door of the adjacent room. It must have been\nblocked on the inside by something quite heavy, for under the force of the\nCount’s shoulder it barely budged. In the next three rooms, the Count found\nflotsam and jetsam from floor to ceiling. But in the last room, amidst tiles\nof slate and strips of flashing, an ample space had been cleared around a\ndented old samovar where some roofers had once taken their tea.\nBack in his room, the Count hung a few jackets in his closet. He\nunpacked some trousers and shirts into the back right corner of his bureau\n(to ensure that the three-legged beast wouldn’t topple). Down the hall he\ndragged his trunk, half of his furniture, and all of his father’s books but one.\nThus, within an hour he had reduced his room to its essentials: a desk and\nchair, a bed and bedside table, a high-back chair for guests, and a ten-foot\npassage just wide enough for a gentleman to circumambulate in reflection.\nWith satisfaction the Count looked toward the cat (who was busy licking\nthe cream from his paws in the comfort of the high-back chair). “What say\nyou now, you old pirate?”\nThen he sat at his desk and picked up the one volume that he had\nretained. It must have been a decade since the Count had first promised\nhimself to read this work of universal acclaim that his father had held so\ndear. And yet, every time he had pointed his finger at his calendar and\ndeclared: This is the month in which I shall devote myself to the Essays of\nMichel de Montaigne! some devilish aspect of life had poked its head in the\ndoor. From an unexpected corner had come an expression of romantic\ninterest, which could not in good conscience be ignored. Or his banker had\ncalled. Or the circus had come to town.\nLife will entice, after all.', 'Half an hour later, after the Count had sent Mishka off to a discussion on\nthe future of meter (at which Katerina from Kiev would presumably be in\nattendance), he headed to the Boyarsky, apparently destined to dine on duck\nalone. But just as he was leaving, Audrius beckoned.\nSliding a folded piece of paper across the bar, Audrius explained under\nhis breath: “I was instructed to relay this to you.”\n“To me? From whom?”\n“Miss Urbanova.”\n“Miss Urbanova?”\n“Anna Urbanova. The movie star.”\nSince the Count still showed no sign of understanding, the bartender\nexplained a little more loudly: “The one who was sitting at that table across\nfrom you.”\n“Ah, yes. Thank you.”\nAs Audrius returned to his work, the Count unfolded the piece of paper,\nwhich bore the following request in a willowy script:\nPlease allow me a second chance\nat a first impression\nin suite 208\nWhen the Count knocked on the door of suite 208, it was opened by an\nolder woman who regarded him with impatience.\n“Yes?”\n“I am Alexander Rostov. . . .”\n“You’re expected. Come in. Miss Urbanova will be a moment.”\nInstinctively, the Count prepared to offer the woman a witty remark\nabout the weather, but when he stepped inside she stepped out and closed\nthe door, leaving him alone in the entryway.\nDecorated in the style of a Venetian palazzo, suite 208 was one of the\nfinest accommodations on the floor and looked no worse for wear now that\nthe tireless typers of directives had finally moved to the Kremlin. With a\nbedroom and drawing room on either side of a grand salon, its ceilings were']",Count Rostov decided to read the Essays of Michel de Montaigne while in his attic quarters.,"Count Rostov decided to read The Essays of Michel de Montaigne, a book his father cherished.,",0.0,0.9783088824321915,0.99999999995,1.0,0.0,0.8357661940186918
"Who was Count Rostov’s trusted barber at the Metropol Hotel?,","['G\nAchilles Agonistes\nreetings, Arkady.”\n“Greetings to you, Count Rostov. Is there something I can do for\nyou this morning?”\n“If it wouldn’t be too much trouble, could you spare a bit of stationery?”\n“Certainly.”\nStanding at the front desk, the Count penned a one-sentence note under\nthe hotel’s moniker and addressed the envelope in an appropriately slanted\nscript; he waited until the bell captain was otherwise occupied, casually\ncrossed the lobby, slipped the note onto the bell captain’s desk, and then\nheaded downstairs for his weekly visit to the barber.\nIt had been many years since Yaroslav Yaroslavl had worked his magic\nin the barbershop of the Metropol, and in the interim any number of\nsuccessors had attempted to fill his shoes. The most recent fellow—Boris\nSomething-or-other-ovich—was perfectly qualified to shorten a man’s hair;\nbut he was neither the artist nor the conversationalist that Yaroslav had\nbeen. In fact, he went about his business with such mute efficiency, one\nsuspected he was part machine.\n“Trim?” he asked the Count, wasting no time with subjects, verbs, or the\nother superfluities of language.\nGiven the Count’s thinning hair and the barber’s predisposition to\nefficiency, a trim might take all of ten minutes.\n“Yes, a trim,” said the Count. “But perhaps a shave as well. . . .”\nThe barber furrowed his brow. The man in him, no doubt, was inclined\nto point out that the Count had obviously shaved a few hours before; but the\nmachinery in him was so finely tuned, it was already putting down the\nscissors and reaching for the shaving brush.\nHaving whipped a sufficient lather, Boris dabbed it on those areas of the\nCount’s face where whiskers would have been had the Count been in need\nof a shave. He sharpened one of his razors on his strop, leaned over the', 'A\n1922\nAn Ambassador\nt half past six on the twenty-first of June 1922, when Count Alexander\nIlyich Rostov was escorted through the gates of the Kremlin onto Red\nSquare, it was glorious and cool. Drawing his shoulders back without\nbreaking stride, the Count inhaled the air like one fresh from a swim. The\nsky was the very blue that the cupolas of St. Basil’s had been painted for.\nTheir pinks, greens, and golds shimmered as if it were the sole purpose of a\nreligion to cheer its Divinity. Even the Bolshevik girls conversing before\nthe windows of the State Department Store seemed dressed to celebrate the\nlast days of spring.\n“Hello, my good man,” the Count called to Fyodor, at the edge of the\nsquare. “I see the blackberries have come in early this year!”\nGiving the startled fruit seller no time to reply, the Count walked briskly\non, his waxed moustaches spread like the wings of a gull. Passing through\nResurrection Gate, he turned his back on the lilacs of the Alexander\nGardens and proceeded toward Theatre Square, where the Hotel Metropol\nstood in all its glory. When he reached the threshold, the Count gave a wink\nto Pavel, the afternoon doorman, and turned with a hand outstretched to the\ntwo soldiers trailing behind him.\n“Thank you, gentlemen, for delivering me safely. I shall no longer be in\nneed of your assistance.”\nThough strapping lads, both of the soldiers had to look up from under\ntheir caps to return the Count’s gaze—for like ten generations of Rostov\nmen, the Count stood an easy six foot three.\n“On you go,” said the more thuggish of the two, his hand on the butt of\nhis rifle. “We’re to see you to your rooms.”', 'As Viktor took a sip from his coffee, the Count watched the accordion\nplayer with interest.\n“Have you ever seen Casablanca?” he asked.\nSomewhat bewildered, Viktor admitted that he had not.\n“Ah. You must see it one day.”\nAnd so the Count told Viktor about his friend Osip and their recent\nviewing of the movie. In particular, he described the scene in which a small-\ntime crook was dragged away by the police and how the American\nsaloonkeeper, having assured his customers that everything was all right,\ncasually instructed his bandleader to play on.\n“My friend was very impressed with this,” explained the Count. “He\nsaw the saloonkeeper’s instruction to the piano player to start playing so\nsoon after the arrest as evidence of his indifference to the fates of other\nmen. But I wonder. . . .”\nThe following morning at half past eleven, two officers of the KGB arrived\nat the Metropol Hotel in order to question Headwaiter Alexander Rostov on\nan undisclosed matter.\nHaving been escorted by a bellhop to Rostov’s room on the sixth floor,\nthe officers found no sign of him there. Nor was he receiving a trim in the\nbarbershop, lunching at the Piazza, or reading the papers in the lobby.\nSeveral of Rostov’s closest associates, including Chef Zhukovsky and\nMaître d’ Duras, were questioned, but none had seen Rostov since the\nprevious night. (The officers also endeavored to speak with the hotel’s\nmanager, only to find that he had not yet reported to work—a fact that was\nduly noted in his file!) At one o’clock, two additional KGB men were\nsummoned so that a more thorough search could be made of the hotel. At\ntwo, the senior officer conducting the investigation was encouraged to\nspeak with Vasily, the concierge. Finding him at his desk in the lobby\n(where he was in the midst of securing theater tickets for a guest), the\nofficer did not beat about the bush. He put his question to the concierge\nunambiguously:\n“Do you know the whereabouts of Alexander Rostov?”\nTo which the concierge replied: “I haven’t the slightest idea.”']",Yaroslav Yaroslavl was Count Rostov's trusted barber at the Metropol Hotel.,"Count Rostov’s barber was Yaroslav Yaroslavl, who worked with precision and artistry.,",0.5,0.9865804267512327,0.9999999999,1.0,0.6666666644444444,0.5351231893342294
"How did Count Rostov describe his sister Helena’s embroidery habit?,","['Together, the Count and Marina were staring at a spool that she had just\ntaken from a drawer filled with threads in every possible shade of white.\n“I am so sorry, Your Excellency,” Marina replied. “Now that you bring it\nto my attention, it does seem more creamy than pearly.”\nThe Count looked up from the spool into Marina’s stationary eye, which\nwas filled with concern; but her wandering eye seemed filled with mirth.\nThen she laughed like a schoolgirl.\n“Oh, give me that,” he said.\n“Here,” she said in a conciliatory tone. “Let me.”\n“Absolutely not.”\n“Oh, come now.”\n“I’m perfectly capable of doing it myself, thank you.”\nBut to the Count’s credit, he was not simply making a peevish point. He\nwas, in fact, perfectly capable of doing it himself.\nIt stands to reason that if you wish to be a good waiter you must be\nmaster of your own appearance. You must be clean, well groomed, and\ngraceful. But you must also be neatly dressed. You certainly can’t wander\naround the dining room with fraying collars or cuffs. And God forbid you\nshould presume to serve with a dangling button—for next thing you knew,\nit would be floating in a customer’s vichyssoise. So, three weeks after\njoining the staff of the Boyarsky, the Count had asked Marina to teach him\nArachne’s art. To be conservative, the Count had set aside an hour for the\nlesson. It ended up taking eight hours over the course of four weeks.\nWho knew that there was such a plethora of stitches? The backstitch,\ncross-stitch, slip stitch, topstitch, whipstitch. Aristotle, Larousse, and\nDiderot—those great encyclopedists who spent their lives segmenting,\ncataloging, and defining all manner of phenomena—would never have\nimagined that there were so many, and each one suited to a different\npurpose!\nWith his creamy thread in hand, the Count settled himself into a chair;\nand when Marina held out her pincushion, he surveyed the needles as a\nchild surveys chocolates in a box.\n“This one,” he said.\nLicking the thread and closing an eye (just as Marina had taught him),\nthe Count threaded the needle faster than saints enter the gates of heaven.\nForming a double loop, tying off a knot, and snipping the thread from the', 'spool, the Count sat upright and set about his work as Marina set about hers\n(the repair of a pillowcase).\nAs with any sewing circle since the beginning of time, the two in this\none were accustomed to sharing observations from their day as they\nstitched. Most of these observations were met with a Hmm, or an Is that so?\nwithout a break in the rhythm of the work; but occasionally, some item that\nwarranted greater attention would bring the stitching to a stop. Just so,\nhaving exchanged remarks on the weather, and Pavel’s handsome new\ntopcoat, Marina’s needle suddenly froze in midstitch when the Count\nmentioned that he had run into Nina.\n“Nina Kulikova?” she asked in surprise.\n“None other.”\n“Where?”\n“In the lobby. She had been having lunch with three of her comrades.”\n“Did you speak?”\n“At some length.”\n“What did she have to say for herself?”\n“It seems they are off to Ivanovo to rationalize kulaks and collectivize\ntractors, and what have you.”\n“Never mind that, Alexander. How was she?”\nHere the Count stopped his stitching.\n“She was every bit herself,” he said after a moment. “Still full of\ncuriosity and passion and self-assurance.”\n“Wonderful,” Marina said with a smile.\nThe Count watched as she resumed her stitching.\n“And yet . . .”\nMarina stopped again and met his gaze.\n“And yet?”\n. . .\n“It’s nothing.”\n“Alexander. There is clearly something on your mind.”\n. . .\n“It’s just that to hear Nina talk of her upcoming journey, she is so\npassionate, so self-assured, and perhaps so single-minded, that she seems\nalmost humorless. Like some dauntless explorer, she seems ready to place\nher flag in a polar ice cap and claim it in the name of Inevitability. But I', 'A\nAddendum\nt the very moment that the Count heard the door to suite 208 clicking\nshut, Anna Urbanova was, in fact, falling asleep; but she did not sleep\nsoundly.\nWhen the actress first dismissed the Count (having rolled onto her side\nwith a languid sigh), she watched with cool pleasure as he gathered his\nclothes and drew the curtains. She even took some satisfaction when he\npaused to pick up her blouse and hang it in the closet.\nBut at some point during the night, this image of the Count picking up\nher blouse began to trouble her sleep. On the train back to St. Petersburg,\nshe found herself muttering about it. And by the time she returned home, it\npositively infuriated her. In the week that followed, if she had the slightest\nbreak in her demanding schedule, the image rushed forth, and her famous\nalabaster cheeks grew red with rage.\n“Who does he think he is, this Count Rostov? Pulling out chairs and\nwhistling at dogs? Putting on airs and looking down noses, is more like it.\nBut by what right? Who gave him permission to pick up a blouse and hang\nit on its hanger? If I drop my blouse on the floor, what of it? It’s my\nclothing and I can treat it as I please!”\nOr so she would find herself reasoning to no one in particular.\nOne night, returning from a party, the very thought of the Count’s\nprecious little gesture was so infuriating that when she undressed she not\nonly threw her red silk gown on the floor, she instructed her staff that it was\nnot to be touched. Each night that followed, she dumped another outfit on\nthe floor. Dresses and blouses of velvet and silk from London and Paris, the\nmore expensive the better. Dumped here on the bathroom floor and there by\nthe dustbin. In a word, wherever it suited her.\nAfter two weeks, her boudoir began to look like an Arabian tent with\nfabrics of every color underfoot.']","Count Rostov described his sister Helena's embroidery habit as ""Arachne's art.""","The Count teased Helena, saying she unwound her stitches at night like Penelope so that he would have to keep reading Pushkin to her.,",0.0,0.9933332105654443,0.0,0.0,0.249999999375,0.7197894230991235
"How did Count Rostov comfort the Metropol staff when he returned from his trial?,","['At ten o’clock the Count was seated in the Boyarsky with an empty plate\nand a nearly empty bottle of White on the table. With the day drawing\nrapidly to a close, he took some pride in knowing that everything was in\norder.\nThat morning, having received a visit from Konstantin Konstantinovich,\nthe Count had brought his accounts up to date at Muir & Mirrielees (now\nknown as the Central Universal Department Store), Filippov’s (the First\nMoscow Bakery), and, of course, the Metropol. At the Grand Duke’s desk,\nhe had written a letter to Mishka, which he had then entrusted to Petya with\ninstructions it be mailed on the following day. In the afternoon, he had paid\nhis weekly visit to the barber and tidied up his rooms. He had donned his\nburgundy smoking jacket (which, to be perfectly frank, was disconcertingly\nsnug), and in its pocket he placed a single gold coin for the undertaker with\ninstructions that he be dressed in the freshly pressed black suit (which had\nbeen laid out on his bed), and that his body be buried in the family plot at\nIdlehour.\nBut if the Count took pride in knowing that everything was in order, he\ntook comfort in knowing that the world would carry on without him—and,\nin fact, already had. The night before, he had happened to be standing at the\nconcierge’s desk when Vasily produced a map of Moscow for one of the\nhotel’s guests. As Vasily drew a zigzagging line from the center of the city\nto the Garden Ring, more than half of the streets he named were unfamiliar\nto the Count. Earlier that day, Vasily had informed him that the famed blue-\nand-gold lobby of the Bolshoi had been painted over in white, while in the\nArbat Andreyev’s moody statue of Gogol had been plucked from its\npedestal and replaced with a more uplifting one of Gorky. Just like that, the\ncity of Moscow could boast new street names, new lobbies, and new statues\n—and neither the tourists, the theatergoers, nor the pigeons seemed\nparticularly put out.\nThe staffing trend that had begun with the appointment of the Bishop\nhad continued unabated—such that any young man with more influence\nthan experience could now don the white jacket, clear from the left, and\npour wine into water glasses.\nMarina, who once had welcomed the Count’s company as she stitched in\nthe stitching room, now had a junior seamstress to watch over as well as a\ntoddler at home (God bless).', 'A\n1922\nAn Ambassador\nt half past six on the twenty-first of June 1922, when Count Alexander\nIlyich Rostov was escorted through the gates of the Kremlin onto Red\nSquare, it was glorious and cool. Drawing his shoulders back without\nbreaking stride, the Count inhaled the air like one fresh from a swim. The\nsky was the very blue that the cupolas of St. Basil’s had been painted for.\nTheir pinks, greens, and golds shimmered as if it were the sole purpose of a\nreligion to cheer its Divinity. Even the Bolshevik girls conversing before\nthe windows of the State Department Store seemed dressed to celebrate the\nlast days of spring.\n“Hello, my good man,” the Count called to Fyodor, at the edge of the\nsquare. “I see the blackberries have come in early this year!”\nGiving the startled fruit seller no time to reply, the Count walked briskly\non, his waxed moustaches spread like the wings of a gull. Passing through\nResurrection Gate, he turned his back on the lilacs of the Alexander\nGardens and proceeded toward Theatre Square, where the Hotel Metropol\nstood in all its glory. When he reached the threshold, the Count gave a wink\nto Pavel, the afternoon doorman, and turned with a hand outstretched to the\ntwo soldiers trailing behind him.\n“Thank you, gentlemen, for delivering me safely. I shall no longer be in\nneed of your assistance.”\nThough strapping lads, both of the soldiers had to look up from under\ntheir caps to return the Count’s gaze—for like ten generations of Rostov\nmen, the Count stood an easy six foot three.\n“On you go,” said the more thuggish of the two, his hand on the butt of\nhis rifle. “We’re to see you to your rooms.”', 'Nevsky Prospekt and wandered all the way to Tikhvin Cemetery where, on\na spot overlooking the Neva River, this firefly, this pinwheel, this wonder of\nthe world had suddenly taken his hand.\n“Ah, Count Rostov,” exclaimed Arkady in passing. “There you are. I\nbelieve I have a message for you. . . .” Returning to the front desk, Arkady\nquickly rifled through some notes. “Here.”\nThe message, which had been taken down by the hotel’s receptionist,\nconveyed Mishka’s apologies and explained that as Katerina was under the\nweather, he was returning to St. Petersburg earlier than planned. Taking a\nmoment to mask his disappointment, the Count looked up from the note to\nthank Arkady, but the desk captain had already turned his attention to\nanother guest.\n“Good evening, Count Rostov.” Andrey took a quick look in the Book. “A\nparty of two tonight, isn’t it?”\n“I’m afraid it’s going to be a party of one, Andrey.”\n“Nonetheless, it is our pleasure to have you. Your table should be ready\nin just a few minutes.”\nWith the recent recognition of the USSR by Germany, England, and\nItaly, a wait of a few minutes had become increasingly common at the\nBoyarsky; but such was the price of being welcomed back into the\nsisterhood of nations and the brotherhood of trade.\nAs the Count stepped aside, a man with a pointed beard came marching\ndown the hallway with a protégé in tow. Though the Count had only seen\nhim once or twice before, the Count could tell he was the Commissar of\nSomething-or-Other, for he walked with urgency, talked with urgency, and\neven came to a stop with urgency.\n“Good evening, comrade Soslovsky,” said Andrey with a welcoming\nsmile.\n“Yes,” pronounced Soslovsky—as if he’d just been asked whether he\nwanted to be seated immediately.\nWith a nod of understanding, Andrey signaled a waiter, handed him two\nmenus, and directed him to lead the gentlemen to table fourteen.']","Count Rostov comforted the Metropol staff by acknowledging that it was their pleasure to have him, even though he would be dining alone. This gesture showed his appreciation for their hospitality.","Count Rostov jokingly described his sentence as being required to live forever in the hotel, which the staff celebrated with cheers.,",0.25,0.9312842681787222,0.0,0.0,0.0,0.8583981665894276
"What historical event led to Count Rostov’s family possessions being seized?,","['In replying It is nothing to Andrey, the Count was not simply resorting to a\nGallic figure of speech. In point of fact, the Count deserved as much thanks\nfor his little intervention as a swallow deserves for its trill. For since the age\nof fifteen, Alexander Rostov had been a master of seating tables.\nWhenever he was home for the holidays, his grandmother would\ninevitably call him into the library, where she liked to knit by the fireplace\nin a solitary chair.\n“Come in, my boy, and sit with me a moment.”\n“Certainly, Grandmother,” replied the Count, balancing himself on the\nedge of the fire grate. “How can I be of assistance?”\n“The prelate is coming for dinner on Friday night—as are the Duchess\nObolensky, Count Keragin, and the Minsky-Polotovs. . . .”\nHere she would let her voice trail off without further explanation; but no\nfurther explanation was needed. The Countess was of a mind that dinner\nshould provide one with respite from life’s trials and tribulations. Thus, she\ncould not countenance discussions of religion, politics, or personal sorrows\nat her table. Further complicating matters, the prelate was deaf in his left\near, partial to Latin epigrams, and prone to stare at décolletage whenever he\ndrank a glass of wine; while the Duchess Obolensky, who was particularly\ncaustic in summer, frowned upon pithy sayings and could not abide\ndiscussions of the arts. And the Keragins? Their great-grandfather had been\ncalled a Bonapartist by Prince Minsky-Polotov in 1811, and they had not\nexchanged a word with a Minsky-Polotov since.\n“How many will be in attendance?” asked the Count.\n“Forty.”\n“The usual assembly?”\n“More or less.”\n“The Osipovs?”\n“Yes. But Pierre is in Moscow. . . .”\n“Ah,” said the Count with the smile of the chess champion who has been\nconfronted with a new gambit.\nThe Nizhny Novgorod Province had a hundred prominent families,\nwhich over the course of two centuries had intermarried and divorced,', 'in misery, he was visited by his grandmother. Taking a seat at the foot of the\nbed, the Countess expressed a measure of sympathy: “There is nothing\npleasant to be said about losing,” she began, “and the Obolensky boy is a\npill. But, Sasha, my dear, why on earth would you give him the\nsatisfaction?” It was in this spirit that he and his grandmother parted\nwithout tears on the docks in Peterhof. Then the Count returned to the\nfamily estate in order to administer its shuttering.\nIn quick succession came the sweeping of chimneys, the clearing of\npantries, and the shrouding of furniture. It was just as if the family were\nreturning to St. Petersburg for the season, except that the dogs were released\nfrom their kennels, the horses from their stables, and the servants from their\nduties. Then, having filled a single wagon with some of the finest of the\nRostovs’ furniture, the Count bolted the doors and set out for Moscow.\n’Tis a funny thing, reflected the Count as he stood ready to abandon his\nsuite. From the earliest age, we must learn to say good-bye to friends and\nfamily. We see our parents and siblings off at the station; we visit cousins,\nattend schools, join the regiment; we marry, or travel abroad. It is part of the\nhuman experience that we are constantly gripping a good fellow by the\nshoulders and wishing him well, taking comfort from the notion that we\nwill hear word of him soon enough.\nBut experience is less likely to teach us how to bid our dearest\npossessions adieu. And if it were to? We wouldn’t welcome the education.\nFor eventually, we come to hold our dearest possessions more closely than\nwe hold our friends. We carry them from place to place, often at\nconsiderable expense and inconvenience; we dust and polish their surfaces\nand reprimand children for playing too roughly in their vicinity—all the\nwhile, allowing memories to invest them with greater and greater\nimportance. This armoire, we are prone to recall, is the very one in which\nwe hid as a boy; and it was these silver candelabra that lined our table on\nChristmas Eve; and it was with this handkerchief that she once dried her\ntears, et cetera, et cetera. Until we imagine that these carefully preserved\npossessions might give us genuine solace in the face of a lost companion.\nBut, of course, a thing is just a thing.\nAnd so, slipping his sister’s scissors into his pocket, the Count looked\nonce more at what heirlooms remained and then expunged them from his\nheartache forever.', '21 June 1922\nAPPEARANCE OF COUNT ALEXANDER ILYICH ROSTOV\nBEFORE THE EMERGENCY COMMITTEE OF THE PEOPLE’S COMMISSARIAT FOR INTERNAL AFFAIRS\nPresiding: Comrades V. A. Ignatov, M. S. Zakovsky, A. N. Kosarev\nProsecuting: A. Y. Vyshinsky\nProsecutor Vyshinsky: State your name.\nRostov: Count Alexander Ilyich Rostov, recipient of the Order of Saint Andrew,\nmember of the Jockey Club, Master of the Hunt.\nVyshinsky: You may have your titles; they are of no use to anyone else. But for the\nrecord, are you not Alexander Rostov, born in St. Petersburg, 24 October 1889?\nRostov: I am he.\nVyshinsky: Before we begin, I must say, I do not think that I have ever seen a\njacket festooned with so many buttons.\nRostov: Thank you.\nVyshinsky: It was not meant as a compliment.\nRostov: In that case, I demand satisfaction on the field of honor.\n[Laughter.]\nSecretary Ignatov: Silence in the gallery.\nVyshinsky: What is your current address?\nRostov: Suite 317 at the Hotel Metropol, Moscow.\nVyshinsky: How long have you lived there?\nRostov: I have been in residence since the fifth of September 1918. Just under four\nyears.\nVyshinsky: And your occupation?\nRostov: It is not the business of gentlemen to have occupations.\nVyshinsky: Very well then. How do you spend your time?\nRostov: Dining, discussing. Reading, reflecting. The usual rigmarole.\nVyshinsky: And you write poetry?\nRostov: I have been known to fence with a quill.\nVyshinsky: [Holding up a pamphlet] Are you the author of this long poem of 1913:\nWhere Is It Now?\nRostov: It has been attributed to me.\nVyshinsky: Why did you write the poem?\nRostov: It demanded to be written. I simply happened to be sitting at the particular\ndesk on the particular morning when it chose to make its demands.\nVyshinsky: And where was that exactly?\nRostov: In the south parlor at Idlehour.\nVyshinsky: Idlehour?\nRostov: The Rostov estate in Nizhny Novgorod.\nVyshinsky: Ah, yes. Of course. How apt. But let us return our attention to your\npoem. Coming as it did—in the more subdued years after the failed revolt of 1905—\nmany considered it a call to action. Would you agree with that assessment?\nRostov: All poetry is a call to action.\nVyshinsky: [Checking notes] And it was in the spring of the following year that you\nleft Russia for Paris . . . ?\nRostov: I seem to remember blossoms on the apple trees. So, yes, in all likelihood\nit was spring.\nVyshinsky: May 16 to be precise. Now, we understand the reasons for your self-\nimposed exile; and we even have some sympathy with the actions that prompted your\nflight. What concerns us here is your return in 1918. One wonders if you came back\nwith the intention of taking up arms and, if so, whether for or against the\nRevolution.']",The historical event that led to Count Rostov's family possessions being seized was the Revolution.,"The Russian Revolution led to the confiscation of Count Rostov’s family estate and possessions, leaving him to live in a confined space.,",0.0,0.9844571446737923,0.3333333333,1.0,0.0,0.7326110450421502
"Why did Count Rostov’s grandmother advise him not to show despair when losing a game?,","['in misery, he was visited by his grandmother. Taking a seat at the foot of the\nbed, the Countess expressed a measure of sympathy: “There is nothing\npleasant to be said about losing,” she began, “and the Obolensky boy is a\npill. But, Sasha, my dear, why on earth would you give him the\nsatisfaction?” It was in this spirit that he and his grandmother parted\nwithout tears on the docks in Peterhof. Then the Count returned to the\nfamily estate in order to administer its shuttering.\nIn quick succession came the sweeping of chimneys, the clearing of\npantries, and the shrouding of furniture. It was just as if the family were\nreturning to St. Petersburg for the season, except that the dogs were released\nfrom their kennels, the horses from their stables, and the servants from their\nduties. Then, having filled a single wagon with some of the finest of the\nRostovs’ furniture, the Count bolted the doors and set out for Moscow.\n’Tis a funny thing, reflected the Count as he stood ready to abandon his\nsuite. From the earliest age, we must learn to say good-bye to friends and\nfamily. We see our parents and siblings off at the station; we visit cousins,\nattend schools, join the regiment; we marry, or travel abroad. It is part of the\nhuman experience that we are constantly gripping a good fellow by the\nshoulders and wishing him well, taking comfort from the notion that we\nwill hear word of him soon enough.\nBut experience is less likely to teach us how to bid our dearest\npossessions adieu. And if it were to? We wouldn’t welcome the education.\nFor eventually, we come to hold our dearest possessions more closely than\nwe hold our friends. We carry them from place to place, often at\nconsiderable expense and inconvenience; we dust and polish their surfaces\nand reprimand children for playing too roughly in their vicinity—all the\nwhile, allowing memories to invest them with greater and greater\nimportance. This armoire, we are prone to recall, is the very one in which\nwe hid as a boy; and it was these silver candelabra that lined our table on\nChristmas Eve; and it was with this handkerchief that she once dried her\ntears, et cetera, et cetera. Until we imagine that these carefully preserved\npossessions might give us genuine solace in the face of a lost companion.\nBut, of course, a thing is just a thing.\nAnd so, slipping his sister’s scissors into his pocket, the Count looked\nonce more at what heirlooms remained and then expunged them from his\nheartache forever.', 'In replying It is nothing to Andrey, the Count was not simply resorting to a\nGallic figure of speech. In point of fact, the Count deserved as much thanks\nfor his little intervention as a swallow deserves for its trill. For since the age\nof fifteen, Alexander Rostov had been a master of seating tables.\nWhenever he was home for the holidays, his grandmother would\ninevitably call him into the library, where she liked to knit by the fireplace\nin a solitary chair.\n“Come in, my boy, and sit with me a moment.”\n“Certainly, Grandmother,” replied the Count, balancing himself on the\nedge of the fire grate. “How can I be of assistance?”\n“The prelate is coming for dinner on Friday night—as are the Duchess\nObolensky, Count Keragin, and the Minsky-Polotovs. . . .”\nHere she would let her voice trail off without further explanation; but no\nfurther explanation was needed. The Countess was of a mind that dinner\nshould provide one with respite from life’s trials and tribulations. Thus, she\ncould not countenance discussions of religion, politics, or personal sorrows\nat her table. Further complicating matters, the prelate was deaf in his left\near, partial to Latin epigrams, and prone to stare at décolletage whenever he\ndrank a glass of wine; while the Duchess Obolensky, who was particularly\ncaustic in summer, frowned upon pithy sayings and could not abide\ndiscussions of the arts. And the Keragins? Their great-grandfather had been\ncalled a Bonapartist by Prince Minsky-Polotov in 1811, and they had not\nexchanged a word with a Minsky-Polotov since.\n“How many will be in attendance?” asked the Count.\n“Forty.”\n“The usual assembly?”\n“More or less.”\n“The Osipovs?”\n“Yes. But Pierre is in Moscow. . . .”\n“Ah,” said the Count with the smile of the chess champion who has been\nconfronted with a new gambit.\nThe Nizhny Novgorod Province had a hundred prominent families,\nwhich over the course of two centuries had intermarried and divorced,', 'Sofia continued to look at the keyboard. The Count hesitated. And then\nwith a touch of trepidation, he asked:\n“Is everything all right?”\nSofia looked up, a little surprised. Then seeing how grave her father’s\nexpression was, she smiled.\n“Of course, Papa. Why do you ask?”\nThe Count shook his head.\n“I’ve never played an instrument in my life, but I understand something\nof music. To have played the opening measures of that piece with feelings\nso perfectly evocative of heartache, one can only assume that you have\ndrawn on some wellspring of sorrow within yourself.”\n“Oh, I see,” she said. Then with the enthusiasm of a young scholar she\nbegan to explain: “Viktor Stepanovich calls that the mood. He says that\nbefore one plays a note, one must discover an example of the composition’s\nmood hidden away in one’s heart. So for this piece, I think about my\nmother. I think of how my few memories of her seem to be fading, and then\nI begin to play.”\nThe Count was quiet, overwhelmed by another wave of astonishment.\n“Does that make sense?” Sofia asked.\n“Abundantly,” he said. Then after a moment of reflection, he added: “As\na younger man, I used to feel the same way about my sister. Every year that\npassed, it seemed a little more of her had slipped away; and I began to fear\nthat one day I would come to forget her altogether. But the truth is: No\nmatter how much time passes, those we have loved never slip away from us\nentirely.”\nThey were both quiet now. Then looking about him, the Count gestured\nwith his hand.\n“This was a favorite room of hers.”\n“Of your sister’s?”\n“No, no. Of your mother’s.”\nSofia looked around with some surprise.\n“The ballroom . . . ?”\n“Most definitely. After the Revolution, all the old ways of doing things\nwere abandoned—which was the point, I suppose. But the new ways of\ndoing things had yet to be established. So all across Russia, all manner of']","Count Rostov's grandmother advised him not to show despair when losing a game because she believed that there was nothing pleasant about losing and that showing despair would give satisfaction to the person who won, in this case, the Obolensky boy.","She told him not to give his opponent the satisfaction, reinforcing the importance of dignity and composure.,",0.75,0.9478839889053327,0.9999999999,1.0,0.0,0.5413285248714996
"What advice did the Grand Duke Demidov give to Count Rostov?,","['In replying It is nothing to Andrey, the Count was not simply resorting to a\nGallic figure of speech. In point of fact, the Count deserved as much thanks\nfor his little intervention as a swallow deserves for its trill. For since the age\nof fifteen, Alexander Rostov had been a master of seating tables.\nWhenever he was home for the holidays, his grandmother would\ninevitably call him into the library, where she liked to knit by the fireplace\nin a solitary chair.\n“Come in, my boy, and sit with me a moment.”\n“Certainly, Grandmother,” replied the Count, balancing himself on the\nedge of the fire grate. “How can I be of assistance?”\n“The prelate is coming for dinner on Friday night—as are the Duchess\nObolensky, Count Keragin, and the Minsky-Polotovs. . . .”\nHere she would let her voice trail off without further explanation; but no\nfurther explanation was needed. The Countess was of a mind that dinner\nshould provide one with respite from life’s trials and tribulations. Thus, she\ncould not countenance discussions of religion, politics, or personal sorrows\nat her table. Further complicating matters, the prelate was deaf in his left\near, partial to Latin epigrams, and prone to stare at décolletage whenever he\ndrank a glass of wine; while the Duchess Obolensky, who was particularly\ncaustic in summer, frowned upon pithy sayings and could not abide\ndiscussions of the arts. And the Keragins? Their great-grandfather had been\ncalled a Bonapartist by Prince Minsky-Polotov in 1811, and they had not\nexchanged a word with a Minsky-Polotov since.\n“How many will be in attendance?” asked the Count.\n“Forty.”\n“The usual assembly?”\n“More or less.”\n“The Osipovs?”\n“Yes. But Pierre is in Moscow. . . .”\n“Ah,” said the Count with the smile of the chess champion who has been\nconfronted with a new gambit.\nThe Nizhny Novgorod Province had a hundred prominent families,\nwhich over the course of two centuries had intermarried and divorced,', 'A\n1922\nAn Ambassador\nt half past six on the twenty-first of June 1922, when Count Alexander\nIlyich Rostov was escorted through the gates of the Kremlin onto Red\nSquare, it was glorious and cool. Drawing his shoulders back without\nbreaking stride, the Count inhaled the air like one fresh from a swim. The\nsky was the very blue that the cupolas of St. Basil’s had been painted for.\nTheir pinks, greens, and golds shimmered as if it were the sole purpose of a\nreligion to cheer its Divinity. Even the Bolshevik girls conversing before\nthe windows of the State Department Store seemed dressed to celebrate the\nlast days of spring.\n“Hello, my good man,” the Count called to Fyodor, at the edge of the\nsquare. “I see the blackberries have come in early this year!”\nGiving the startled fruit seller no time to reply, the Count walked briskly\non, his waxed moustaches spread like the wings of a gull. Passing through\nResurrection Gate, he turned his back on the lilacs of the Alexander\nGardens and proceeded toward Theatre Square, where the Hotel Metropol\nstood in all its glory. When he reached the threshold, the Count gave a wink\nto Pavel, the afternoon doorman, and turned with a hand outstretched to the\ntwo soldiers trailing behind him.\n“Thank you, gentlemen, for delivering me safely. I shall no longer be in\nneed of your assistance.”\nThough strapping lads, both of the soldiers had to look up from under\ntheir caps to return the Count’s gaze—for like ten generations of Rostov\nmen, the Count stood an easy six foot three.\n“On you go,” said the more thuggish of the two, his hand on the butt of\nhis rifle. “We’re to see you to your rooms.”', 'Half an hour later, after the Count had sent Mishka off to a discussion on\nthe future of meter (at which Katerina from Kiev would presumably be in\nattendance), he headed to the Boyarsky, apparently destined to dine on duck\nalone. But just as he was leaving, Audrius beckoned.\nSliding a folded piece of paper across the bar, Audrius explained under\nhis breath: “I was instructed to relay this to you.”\n“To me? From whom?”\n“Miss Urbanova.”\n“Miss Urbanova?”\n“Anna Urbanova. The movie star.”\nSince the Count still showed no sign of understanding, the bartender\nexplained a little more loudly: “The one who was sitting at that table across\nfrom you.”\n“Ah, yes. Thank you.”\nAs Audrius returned to his work, the Count unfolded the piece of paper,\nwhich bore the following request in a willowy script:\nPlease allow me a second chance\nat a first impression\nin suite 208\nWhen the Count knocked on the door of suite 208, it was opened by an\nolder woman who regarded him with impatience.\n“Yes?”\n“I am Alexander Rostov. . . .”\n“You’re expected. Come in. Miss Urbanova will be a moment.”\nInstinctively, the Count prepared to offer the woman a witty remark\nabout the weather, but when he stepped inside she stepped out and closed\nthe door, leaving him alone in the entryway.\nDecorated in the style of a Venetian palazzo, suite 208 was one of the\nfinest accommodations on the floor and looked no worse for wear now that\nthe tireless typers of directives had finally moved to the Kremlin. With a\nbedroom and drawing room on either side of a grand salon, its ceilings were']",The Grand Duke Demidov advised Count Rostov to allow Miss Urbanova a second chance at a first impression in suite 208.,"The Grand Duke advised Count Rostov that a man must master his circumstances, or he will be mastered by them.,",0.3333333333333333,0.953077907595035,0.0,0.0,0.0,0.4688777935654274
"What was Count Rostov’s reaction to being moved to the attic room?,","['As Yuri arranged the breakfast on top of the trunk, the Count sat at the\nGrand Duke’s desk and penned a quick note to one Konstantin\nKonstantinovich of Durnovksi Street.\n“Would you be so kind as to have this delivered, my boy?”\nNever one to shirk, Yuri happily took the note, promised to relay it by\nhand, and accepted a tip with a bow. Then at the threshold he paused.\n“Shall I . . . leave the door ajar?”\nIt was a reasonable question. For the room was rather stuffy, and on the\nsixth floor there was hardly much risk of one’s privacy being compromised.\n“Please do.”\nAs Yuri’s steps sounded down the belfry, the Count placed his napkin in\nhis lap, poured a cup of coffee, and graced it with a few drops of cream.\nTaking his first sip, he noted with satisfaction that young Yuri must have\nsprinted up the extra three flights of stairs because the coffee was not one\ndegree colder than usual.\nBut while he was liberating a wedge of the plum from its pit with his\nparing knife, the Count happened to note a silvery shadow, as seemingly\ninsubstantial as a puff of smoke, slipping behind his trunk. Leaning to his\nside in order to peer around a high-back chair, the Count discovered that\nthis will-o’-the-wisp was none other than the Metropol’s lobby cat. A one-\neyed Russian blue who let nothing within the hotel’s walls escape his\nnotice, he had apparently come to the attic to review the Count’s new\nquarters for himself. Stepping from the shadows, he leapt from the floor to\nthe Ambassador, from the Ambassador to the side table, and from the side\ntable to the top of the three-legged bureau, without making a sound. Having\nachieved this vantage point, he gave the room a good hard look then shook\nhis head in feline disappointment.\n“Yes,” said the Count after completing his own survey. “I see what you\nmean.”\nThe crowded confusion of furniture gave the Count’s little domain the\nlook of a consignment shop in the Arbat. In a room this size, he could have\nmade do with a single high-back chair, a single bedside table, and a single\nlamp. He could have made do without his grandmother’s Limoges\naltogether.\nAnd the books? All of them! he had said with such bravado. But in the\nlight of day, he had to admit that this instruction had been prompted less by', 'When he came into the room, Sofia was sitting in one of the high-back\nchairs.\nWith a bit of theatricality, the Count put his hands behind his back and\ncircled the room while saying hmmm. But after two circuits, the little silver\nthimble had yet to reveal itself. So he began to search a bit more in earnest.\nTaking a page from Sofia’s book, he divided the room into quadrants and\nreviewed them systematically, but to no avail.\nRecalling that he had heard one of the chairs being moved, and\naccounting for Sofia’s height and arm extension, the Count estimated that\nshe could have reached a spot at least five feet off the ground. So, he looked\nbehind the frame of his sister’s portrait; he looked under the mechanics of\nthe little window; he even looked above the doorframe.\nStill no thimble.\nOccasionally, he would look back at Sofia in the hopes that she would\ngive herself away by glancing at her hiding place. But she maintained an\ninfuriatingly disinterested expression, as if she hadn’t the slightest\nawareness of the hunt that was underway. And all the while, swinging her\nlittle feet back and forth.\nAs a student of psychology, the Count decided he must attempt to solve\nthe problem from his opponent’s point of view. Just as he had wanted to\ntake advantage of her limited height, perhaps she had taken advantage of his\nstature. Of course, he thought. The sound of moving furniture didn’t have to\nmean that she was climbing up on a chair; it could have been her pulling\nsomething aside in order to hide something beneath it. The Count dropped\nto the floor and crawled like a lizard from the bookcase to the Ambassador\nand back again.\nAnd still she sat there swinging her little feet.\nThe Count stood to his full height, banging his head against the slope of\nthe ceiling. What’s more, his kneecaps hurt from the hardwood floor, and\nhis jacket was covered in dust. Suddenly, as he looked a little wildly around\nthe room, he became aware of a quietly encroaching eventuality. It was\nslinking slowly toward him like a cat across the lawn; and the name of this\ncat was Defeat.\nCould it be?\nWas he, a Rostov, preparing to surrender?\nWell, in a word: Yes.', 'Half an hour later, after the Count had sent Mishka off to a discussion on\nthe future of meter (at which Katerina from Kiev would presumably be in\nattendance), he headed to the Boyarsky, apparently destined to dine on duck\nalone. But just as he was leaving, Audrius beckoned.\nSliding a folded piece of paper across the bar, Audrius explained under\nhis breath: “I was instructed to relay this to you.”\n“To me? From whom?”\n“Miss Urbanova.”\n“Miss Urbanova?”\n“Anna Urbanova. The movie star.”\nSince the Count still showed no sign of understanding, the bartender\nexplained a little more loudly: “The one who was sitting at that table across\nfrom you.”\n“Ah, yes. Thank you.”\nAs Audrius returned to his work, the Count unfolded the piece of paper,\nwhich bore the following request in a willowy script:\nPlease allow me a second chance\nat a first impression\nin suite 208\nWhen the Count knocked on the door of suite 208, it was opened by an\nolder woman who regarded him with impatience.\n“Yes?”\n“I am Alexander Rostov. . . .”\n“You’re expected. Come in. Miss Urbanova will be a moment.”\nInstinctively, the Count prepared to offer the woman a witty remark\nabout the weather, but when he stepped inside she stepped out and closed\nthe door, leaving him alone in the entryway.\nDecorated in the style of a Venetian palazzo, suite 208 was one of the\nfinest accommodations on the floor and looked no worse for wear now that\nthe tireless typers of directives had finally moved to the Kremlin. With a\nbedroom and drawing room on either side of a grand salon, its ceilings were']","Count Rostov was initially disappointed with his new attic room, describing it as crowded and cluttered with furniture. He felt that the room looked like a consignment shop and that he could have made do with much less furniture. He also noted that the room contained his grandmother's Limoges, which he could have done without. Ultimately, he seemed resigned to his new living situation, as he acknowledged that he was preparing to surrender to the reality of his attic room.","Count Rostov initially surveyed the room with a sense of irony but quickly decided to make it comfortable and efficient.,",0.875,0.9609863499188865,0.99999999995,0.0,0.0,0.22678477322037194
"Why did Count Rostov call his desk his “castle”?,","['Someone had shown the courtesy of having one of the Count’s traveling\ntrunks brought to his bedroom. So, as the bellhops carried the\naforementioned upward, the Count filled the trunk with clothes and\npersonal effects. Noting that the guards were eyeing the two bottles of\nbrandy on the console, the Count tossed them in as well. And once the trunk\nhad been carried upstairs, he finally pointed to the desk.\nThe two bellhops, their bright blue uniforms already smudged from their\nefforts, took hold of it by the corners.\n“But it weighs a ton,” said one to the other.\n“A king fortifies himself with a castle,” observed the Count, “a\ngentleman with a desk.”\nAs the bellhops lugged it into the hall, the Rostovs’ grandfather clock,\nwhich was fated to be left behind, tolled a doleful eight. The captain had\nlong since returned to his post and the guards, having swapped their\nbelligerence for boredom, now leaned against the wall and let the ashes\nfrom their cigarettes fall on the parquet floor while into the grand salon\npoured the undiminished light of the Moscow summer solstice.\nWith a wistful eye, the Count approached the windows at the suite’s\nnorthwest corner. How many hours had he spent before them? How many\nmornings dressed in his robe with his coffee in hand had he observed the\nnew arrivals from St. Petersburg disembarking from their cabs, worn and\nweary from the overnight train? On how many winter eves had he watched\nthe snow slowly descending as some lone silhouette, stocky and short,\npassed under a street lamp? At that very instant, at the square’s northern\nextreme a young Red Army officer rushed up the steps of the Bolshoi,\nhaving missed the first half hour of the evening’s performance.\nThe Count smiled to remember his own youthful preference for arriving\nentr’acte. Having insisted at the English Club that he could only stay for\none more drink, he stayed for three. Then leaping into the waiting carriage,\nhe’d flash across the city, vault the fabled steps, and like this young fellow\nslip through the golden doors. As the ballerinas danced gracefully across\nthe stage, the Count would be whispering his excusez-moi’s, making his\nway to his usual seat in the twentieth row with its privileged view of the\nladies in the loges.\nArriving late, thought the Count with a sigh. What a delicacy of youth.', 'Taking the inlaid box from its chamber, the Count set it on the desk and\nopened the lid.\n“Simply marvelous,” he said.\nThen sitting in the manager’s chair, the Count removed the two pistols,\nloaded them, and waited. He guessed that he had only a matter of seconds\nbefore the door would open, but he used them as best he could to moderate\nhis breathing, lower his heart rate, and calm his nerves; such that by the\ntime the Bishop’s key turned in the lock, he was as cold as a killer.\nSo unanticipated was the Count’s presence behind the desk that the\nBishop had swung the door closed before even noticing that he was there.\nBut if every man has his strengths, one of the Bishop’s was that he was\nnever more than a step away from petty protocol and an inherent sense of\nsuperiority.\n“Headwaiter Rostov,” he said almost peevishly, “you have no business\nbeing in this office. I insist that you leave immediately.”\nThe Count raised one of the pistols.\n“Sit down.”\n“How dare you!”\n“Sit down,” the Count repeated more slowly.\nThe Bishop would be the first to admit that he had no experience with\nfirearms. In fact, he could barely distinguish between a revolver and a\nsemiautomatic. But any fool could see that what the Count was holding was\nan antique. A museum piece. A curiosity.\n“You leave me no choice but to alert the authorities,” he said. Then\nstepping forward, he took up the receiver from one of his two telephones.\nThe Count shifted his aim from the Bishop to the portrait of Stalin and\nshot the former Premier between the eyes.\nShocked by either the sound or the sacrilege, the Bishop jumped back,\ndropping the receiver with a clatter.\nThe Count raised the second pistol and leveled it at the Bishop’s chest.\n“Sit down,” he said again.\nThis time, the Bishop obliged.\nWith the second gun still trained on the Bishop’s chest, the Count now\nstood. He replaced the telephone receiver in its cradle. He backed around\nthe Bishop’s chair and locked the office door. Then he returned to his seat\nbehind the desk.', 'As Yuri arranged the breakfast on top of the trunk, the Count sat at the\nGrand Duke’s desk and penned a quick note to one Konstantin\nKonstantinovich of Durnovksi Street.\n“Would you be so kind as to have this delivered, my boy?”\nNever one to shirk, Yuri happily took the note, promised to relay it by\nhand, and accepted a tip with a bow. Then at the threshold he paused.\n“Shall I . . . leave the door ajar?”\nIt was a reasonable question. For the room was rather stuffy, and on the\nsixth floor there was hardly much risk of one’s privacy being compromised.\n“Please do.”\nAs Yuri’s steps sounded down the belfry, the Count placed his napkin in\nhis lap, poured a cup of coffee, and graced it with a few drops of cream.\nTaking his first sip, he noted with satisfaction that young Yuri must have\nsprinted up the extra three flights of stairs because the coffee was not one\ndegree colder than usual.\nBut while he was liberating a wedge of the plum from its pit with his\nparing knife, the Count happened to note a silvery shadow, as seemingly\ninsubstantial as a puff of smoke, slipping behind his trunk. Leaning to his\nside in order to peer around a high-back chair, the Count discovered that\nthis will-o’-the-wisp was none other than the Metropol’s lobby cat. A one-\neyed Russian blue who let nothing within the hotel’s walls escape his\nnotice, he had apparently come to the attic to review the Count’s new\nquarters for himself. Stepping from the shadows, he leapt from the floor to\nthe Ambassador, from the Ambassador to the side table, and from the side\ntable to the top of the three-legged bureau, without making a sound. Having\nachieved this vantage point, he gave the room a good hard look then shook\nhis head in feline disappointment.\n“Yes,” said the Count after completing his own survey. “I see what you\nmean.”\nThe crowded confusion of furniture gave the Count’s little domain the\nlook of a consignment shop in the Arbat. In a room this size, he could have\nmade do with a single high-back chair, a single bedside table, and a single\nlamp. He could have made do without his grandmother’s Limoges\naltogether.\nAnd the books? All of them! he had said with such bravado. But in the\nlight of day, he had to admit that this instruction had been prompted less by']","Count Rostov called his desk his ""castle"" because he believed that a gentleman fortifies himself with a desk, just as a king fortifies himself with a castle. This implies that the desk was a symbol of strength, security, and importance to the Count.","He remarked that a king fortifies himself with a castle, but a gentleman fortifies himself with a desk.,",0.3333333333333333,0.9834462304422571,0.9999999999,1.0,0.0,0.7300339245882195
"What does Count Rostov’s attic window overlook?,","['Though the bottle was far from empty, the Count did not refill the glass;\nnor did he toss it over his shoulder. Rather, he placed it with care on the\nchimney top and then approached the parapet, where he stood to his full\nheight.\nBefore him sprawled the city, glorious and grandiose. Its legions of\nlights shimmered and reeled until they mixed with the movement of the\nstars. In one dizzy sphere they spun, confusing the works of man with the\nworks of heaven.\nPlacing his right foot on the parapet’s edge, Count Alexander Ilyich\nRostov said, “Good-bye, my country.”\nAs if in reply, the beacon on Mishka’s tower blinked.\nIt was now the simplest of matters. Like one who stands on a dock in\nspring preparing to take the first plunge of the season, all that remained was\na leap. Starting just six stories off the ground and falling at the speed of a\nkopek, a teacup, or a pineapple, the entire journey would only take a matter\nof seconds; and then the circle would be complete. For as sunrise leads to\nsunset and dust to dust, as every river returns to the sea, just so a man must\nreturn to the embrace of oblivion, from whence—\n“Your Excellency!”\nTurning in dismay at the interruption, the Count discovered Abram\nstanding behind him in a state of excitement. In fact, Abram was in such a\nstate of excitement that he showed not the slightest surprise at finding the\nCount poised on the spot where the roof met the ether.\n“I thought I heard your voice,” said the old handyman. “I’m so glad\nyou’re here. You must come with me at once.”\n“Abram, my friend,” the Count began to explain, but the old man\ncontinued unabated:\n“You will not believe it, if I tell you. You will have to see it for\nyourself.” Then without waiting for a response, he hurried with surprising\nagility toward his encampment.\nThe Count let out a sigh. Assuring the city that he would be back in a\nmoment, he followed Abram across the roof to the brazier, where the old\nman stopped and pointed to the northeast corner of the hotel. And there,\nagainst the brightly lit backdrop of the Bolshoi, one could just make out a\nfrenzy of tiny shadows darting through the air.\n“They’ve returned!” Abram exclaimed.', 'largest pipe, he discovered a wall-mounted ladder that led to a hatch in the\nroof—which someone had left open. Putting on his shoes, the Count quietly\nclimbed up the ladder and out into the night.\nThe summer breeze that had beckoned the Count now wrapped him in\nits full embrace. Warm and forgiving, it called up feelings of summer nights\nfrom earlier in his life—from when he was five and ten and twenty on the\nstreets of St. Petersburg or the pastures of Idlehour. Nearly overcome by the\nsurge of old sentiments, he needed to pause a moment before continuing to\nthe western edge of the roof.\nBefore him lay the ancient city of Moscow, which, after waiting\npatiently for two hundred years, was once again the seat of Russian\ngovernance. Despite the hour, the Kremlin shimmered with electric light\nfrom every window, as if its newest denizens were still too drunk with\npower to sleep. But if the lights of the Kremlin shimmered brightly, like all\nearthly lights before them they were diminished in their beauty by the\nmajesty of the constellations overhead.\nCraning his neck, the Count tried to identify the few that he had learned\nin his youth: Perseus, Orion, the Great Bear, each flawless and eternal. To\nwhat end, he wondered, had the Divine created the stars in heaven to fill a\nman with feelings of inspiration one day and insignificance the next?\nLowering his gaze to the horizon, the Count looked out beyond the\nlimits of the city—to where that ancient comfort of sailors, the Morning\nStar, burned brightest in all the firmament.\nAnd then blinked.\n“Good morning, Your Excellency.”\nThe Count spun about.\nStanding a few feet behind him was a man in his early sixties wearing a\ncanvas cap. When the man took a step forward, the Count recognized him\nas one of the handymen who battled the hotel’s leaky pipes and creaky\ndoors.\n“That’s the Shukhov all right,” he said.\n“The Shukhov?”\n“The radio tower.”\nHe pointed in the distance toward the comfort of sailors.\nAh, thought the Count with a smile. Mishka’s spiraling structure of steel\nbroadcasting the latest news and intelligence . . .', 'Alexander Rostov. A man inclined to see the best in all of us.”\n“Hear! Hear!”\n“And you needn’t worry, Papa,” Sofia continued. “For no matter who\ncomes knocking at our door, I have no intention of ever leaving the\nMetropol.”\nAfter joining in a cheer, the members of the gathering emptied their\nglasses, stumbled through the closet, and exited into the hall. Opening the\ndoor to the belfry, the Count gave a slight bow and gestured for everyone to\nproceed. But just as the Count was about to follow the others into the\nstairwell, a woman in late middle age with a satchel on her shoulder and a\nkerchief in her hair stepped from the shadows at the end of the hall. Though\nthe Count had never seen her before, it was clear from her demeanor that\nshe had been waiting to speak with him alone.\n“Andrey,” the Count called into the belfry, “I’ve forgotten something in\nthe room. You all go ahead. I’ll be down in a moment. . . .”\nOnly when the last sound of voices had receded down the stairs did the\nwoman approach. In the light, the Count could see that she had an almost\nsevere beauty about her—like one for whom there would be no half\nmeasures in matters of the heart.\n“I’m Katerina Litvinova,” she said without a smile.\nIt took a moment for the Count to realize that this was none other than\nMishka’s Katerina, the poet from Kiev whom he had lived with back in the\n1920s.\n“Katerina Litvinova! How extraordinary. To what do I owe—”\n“Is there somewhere we could talk?”\n“Why, yes . . . Of course . . .”\nThe Count led Katerina into the bedroom and then, after a moment’s\nhesitation, took her through the jackets into the study. Apparently, he\nneedn’t have hesitated, for she looked around the room as one who had\nheard descriptions of it before, nodding lightly to herself as her gaze shifted\nfrom the bookcase to the coffee table to the Ambassador. Taking her satchel\nfrom her shoulder, she suddenly appeared tired.\n“Here,” said the Count, offering a chair.\nShe sat down, putting the satchel in her lap. Then passing a hand over\nher head, she removed her kerchief, revealing light brown hair cut as short\nas a man’s.']",The attic window overlooks the ancient city of Moscow.,"The small dormer window in Count Rostov’s attic room overlooks the city rooftops and the square below.,",0.0,0.9134190902596989,0.49999999995,0.0,0.0,0.5304654418045645
"What did the Count believe was the purpose of a mirror?,","['A bathroom mirror with a black box on it', 'A boy reaching towards his reflection in a mirror like object.', 'a bath room sink with  large mirror']",The Count believed that the purpose of a mirror was for a boy to reach towards his reflection in it.,"The Count believed mirrors revealed not who one imagines oneself to be, but who one has truly become.,",0.0,0.9912686726972549,0.0,0.0,0.0,0.22015292449441914
"What nickname did Count Rostov give to the Metropol dining hall?,","['“Can we expect more verses from you?”\nThe Count offered an appreciative smile.\n“I am sorry to say, Konstantin, that my days of poetry are behind me.”\n“If your days of poetry are behind you, Count Rostov, then it is we who\nare sorry.”\nTucked discreetly into the northeast corner of the hotel’s second floor was\nthe Boyarsky—the finest restaurant in Moscow, if not in all of Russia. With\nvaulted ceilings and dark red walls reminiscent of a boyar’s retreat, the\nBoyarsky boasted the city’s most elegant décor, its most sophisticated\nwaitstaff, and its most subtle chef de cuisine.\nSo renowned was the experience of dining at the Boyarsky that on any\ngiven night one might have to elbow one’s way through a crowd of hopefuls\njust to catch the eye of Andrey, as he presided over the large black book in\nwhich the names of the fortunate were set down; and when beckoned ahead\nby the maître d’, one could expect to be stopped five times in four\nlanguages on the way to one’s table in the corner, where one would be\nserved flawlessly by a waiter in a white dinner jacket.\nThat is, one could expect this until 1920 when, having already sealed the\nborders, the Bolsheviks decided to prohibit the use of rubles in fine\nrestaurants—effectively closing them to 99 percent of the population. So\ntonight, as the Count began to eat his entrée, water glasses clinked against\ncutlery, couples whispered awkwardly, and even the best of waiters found\nhimself staring at the ceiling.\nBut every period has its virtues, even a time of turmoil. . . .\nWhen Emile Zhukovsky was lured to the Metropol as chef de cuisine in\n1912, he was given command of a seasoned staff and a sizable kitchen. In\naddition, he had the most celebrated larder east of Vienna. On his spice\nshelves was a compendium of the world’s predilections and in his cooler a\ncomprehensive survey of birds and beasts hanging from hooks by their feet.\nAs such, one might naturally leap to the conclusion that 1912 had been a\nperfect year in which to measure the chef’s talents. But in a period of\nabundance any half-wit with a spoon can please a palate. To truly test a', 'T\nAn Acquaintanceship\nhere were two restaurants in the Hotel Metropol: the Boyarsky, that\nfabled retreat on the second floor that we have already visited, and the\ngrand dining room off the lobby known officially as the Metropol, but\nreferred to affectionately by the Count as the Piazza.\nAdmittedly, the Piazza could not challenge the elegance of the\nBoyarsky’s décor, the sophistication of its service, or the subtlety of its\ncuisine. But the Piazza did not aspire to elegance, service, or subtlety. With\neighty tables scattered around a marble fountain and a menu offering\neverything from cabbage piroghi to cutlets of veal, the Piazza was meant to\nbe an extension of the city—of its gardens, markets, and thoroughfares. It\nwas a place where Russians cut from every cloth could come to linger over\ncoffee, happen upon friends, stumble into arguments, or drift into dalliances\n—and where the lone diner seated under the great glass ceiling could\nindulge himself in admiration, indignation, suspicion, and laughter without\ngetting up from his chair.\nAnd the waiters? Like those of a Parisian café, the Piazza’s waiters\ncould best be complimented as “efficient.” Accustomed to navigating\ncrowds, they could easily seat your party of eight at a table for four. Having\nnoted your preferences over the sound of the orchestra, within minutes they\nwould return with the various drinks balanced on a tray and dispense them\nround the table in rapid succession without misplacing a glass. If, with your\nmenu in hand, you hesitated for even a second to place your order, they\nwould lean over your shoulder and poke at a specialty of the house. And\nwhen the last morsel of dessert had been savored, they would whisk away\nyour plate, present your check, and make your change in under a minute. In\nother words, the waiters of the Piazza knew their trade to the crumb, the\nspoon, and the kopek.\nAt least, that was how things were before the war. . . .', 'In replying It is nothing to Andrey, the Count was not simply resorting to a\nGallic figure of speech. In point of fact, the Count deserved as much thanks\nfor his little intervention as a swallow deserves for its trill. For since the age\nof fifteen, Alexander Rostov had been a master of seating tables.\nWhenever he was home for the holidays, his grandmother would\ninevitably call him into the library, where she liked to knit by the fireplace\nin a solitary chair.\n“Come in, my boy, and sit with me a moment.”\n“Certainly, Grandmother,” replied the Count, balancing himself on the\nedge of the fire grate. “How can I be of assistance?”\n“The prelate is coming for dinner on Friday night—as are the Duchess\nObolensky, Count Keragin, and the Minsky-Polotovs. . . .”\nHere she would let her voice trail off without further explanation; but no\nfurther explanation was needed. The Countess was of a mind that dinner\nshould provide one with respite from life’s trials and tribulations. Thus, she\ncould not countenance discussions of religion, politics, or personal sorrows\nat her table. Further complicating matters, the prelate was deaf in his left\near, partial to Latin epigrams, and prone to stare at décolletage whenever he\ndrank a glass of wine; while the Duchess Obolensky, who was particularly\ncaustic in summer, frowned upon pithy sayings and could not abide\ndiscussions of the arts. And the Keragins? Their great-grandfather had been\ncalled a Bonapartist by Prince Minsky-Polotov in 1811, and they had not\nexchanged a word with a Minsky-Polotov since.\n“How many will be in attendance?” asked the Count.\n“Forty.”\n“The usual assembly?”\n“More or less.”\n“The Osipovs?”\n“Yes. But Pierre is in Moscow. . . .”\n“Ah,” said the Count with the smile of the chess champion who has been\nconfronted with a new gambit.\nThe Nizhny Novgorod Province had a hundred prominent families,\nwhich over the course of two centuries had intermarried and divorced,']",The Count referred to the Metropol dining hall as the Piazza.,"He affectionately called it the “Piazza.”,",1.0,0.9390512079179515,0.49999999995,1.0,0.9999999900000002,0.9747230135032765
"What dish demonstrated Chef Zhukovsky’s creativity during food shortages?,","['chef’s ingenuity, one must instead look to a period of want. And what\nprovides want better than war?\nIn the Revolution’s aftermath—with its economic declines, failed crops,\nand halted trade—refined ingredients became as scarce in Moscow as\nbutterflies at sea. The Metropol’s larder was depleted bushel by bushel,\npound by pound, dash by dash, and its chef was left to meet the\nexpectations of his audience with cornmeal, cauliflower, and cabbage—that\nis to say, with whatever he could get his hands on.\nYes, some claimed Emile Zhukovsky was a curmudgeon and others\ncalled him abrupt. Some said he was a short man with a shorter temper. But\nnone could dispute his genius. Just consider the dish the Count was\nfinishing at that very moment: a saltimbocca fashioned from necessity. In\nplace of a cutlet of veal, Emile had pounded flat a breast of chicken. In\nplace of prosciutto de Parma, he had shaved a Ukrainian ham. And in place\nof sage, that delicate leaf that binds the flavors together? He had opted for\nan herb that was as soft and aromatic as sage, but more bitter to the\ntaste. . . . It wasn’t basil or oregano, of that the Count was certain, but he\nhad definitely encountered it somewhere before. . . .\n“How is everything this evening, Your Excellency?”\n“Ah, Andrey. As usual, everything is perfect.”\n“And the saltimbocca?”\n“Inspired. But I do have one question: The herb that Emile has tucked\nunder the ham—I know it isn’t sage. By any chance, is it nettle?”\n“Nettle? I don’t believe so. But I will inquire.”\nThen with a bow, the maître d’ excused himself.\nWithout a doubt Emile Zhukovsky was a genius, reflected the Count, but\nthe man who secured the Boyarsky’s reputation for excellence by ensuring\nthat all within its walls ran smoothly was Andrey Duras.\nBorn in the south of France, Andrey was handsome, tall, and graying at\nthe temples, but his most distinguishing feature was not his looks, his\nheight, or his hair. It was his hands. Pale and well manicured, his fingers\nwere half an inch longer than the fingers of most men his height. Had he\nbeen a pianist, Andrey could easily have straddled a twelfth. Had he been a\npuppeteer, he could have performed the sword fight between Macbeth and\nMacduff as all three witches looked on. But Andrey was neither a pianist\nnor a puppeteer—or at least not in the traditional sense. He was the captain', '“Can we expect more verses from you?”\nThe Count offered an appreciative smile.\n“I am sorry to say, Konstantin, that my days of poetry are behind me.”\n“If your days of poetry are behind you, Count Rostov, then it is we who\nare sorry.”\nTucked discreetly into the northeast corner of the hotel’s second floor was\nthe Boyarsky—the finest restaurant in Moscow, if not in all of Russia. With\nvaulted ceilings and dark red walls reminiscent of a boyar’s retreat, the\nBoyarsky boasted the city’s most elegant décor, its most sophisticated\nwaitstaff, and its most subtle chef de cuisine.\nSo renowned was the experience of dining at the Boyarsky that on any\ngiven night one might have to elbow one’s way through a crowd of hopefuls\njust to catch the eye of Andrey, as he presided over the large black book in\nwhich the names of the fortunate were set down; and when beckoned ahead\nby the maître d’, one could expect to be stopped five times in four\nlanguages on the way to one’s table in the corner, where one would be\nserved flawlessly by a waiter in a white dinner jacket.\nThat is, one could expect this until 1920 when, having already sealed the\nborders, the Bolsheviks decided to prohibit the use of rubles in fine\nrestaurants—effectively closing them to 99 percent of the population. So\ntonight, as the Count began to eat his entrée, water glasses clinked against\ncutlery, couples whispered awkwardly, and even the best of waiters found\nhimself staring at the ceiling.\nBut every period has its virtues, even a time of turmoil. . . .\nWhen Emile Zhukovsky was lured to the Metropol as chef de cuisine in\n1912, he was given command of a seasoned staff and a sizable kitchen. In\naddition, he had the most celebrated larder east of Vienna. On his spice\nshelves was a compendium of the world’s predilections and in his cooler a\ncomprehensive survey of birds and beasts hanging from hooks by their feet.\nAs such, one might naturally leap to the conclusion that 1912 had been a\nperfect year in which to measure the chef’s talents. But in a period of\nabundance any half-wit with a spoon can please a palate. To truly test a', 'were standing at the ready in their freshly bleached coats; he could see that\nthe sauces were simmering on the stovetop and the garnishes ready for\nplating. But what of that notorious misanthrope of a chef? With the opening\nof the Boyarsky’s doors just minutes away, wasn’t he railing against his\nstaff, his customers, and all his fellow men?\nIn point of fact, Emile Zhukovsky began his days in a state of the\nblackest pessimism. The very moment he looked out from under his covers,\nhe met existence with a scowl, knowing it to be a cold and unforgiving\ncondition. Having had his worst suspicions confirmed by the morning\npapers, at eleven o’clock he would be waiting at the curb for a crowded\ntram to rattle him to the hotel while muttering, “What a world.”\nBut as the day unfolded, hour by hour Emile’s pessimism would slowly\ngive way to the possibility that all was not lost. This rosier perspective\nwould begin building quietly around noon, when he came into his kitchen\nand saw his copper pots. Hanging from their hooks, still shining from the\nprevious night’s scrubbing, they seemed to suggest an indisputable sense of\npossibility. Stepping into the cooler, he would hoist a side of lamb over his\nshoulder, and when he dropped it on the counter with a satisfying thump,\nhis worldview would brighten by another hundred lumens. Such that by\n3:00, when he heard the sound of root vegetables being chopped and\nsmelled the aroma of garlic being sizzled, Emile might begrudgingly\nacknowledge that existence had its consolations. Then at 5:30, if everything\nseemed in order, he might allow himself to sample the wine that he’d been\ncooking with—just to polish off the bottle, you understand; waste not want\nnot; neither a borrower nor a lender be. And at around 6:25, that dark\nhumor which had seemed at dawn to be the very foundation of Emile’s soul,\nwould become irreversibly sanguine when the first order was delivered to\nhis kitchen.\nSo, what did the Count see when he looked through the window at 5:55?\nHe saw Emile dip a spoon into a bowl of chocolate mousse and lick it clean.\nWith that confirmation, the Count turned to Andrey and nodded. Then he\nassumed his station between table one and table two as the maître d’ threw\nthe bolts to open the Boyarsky’s doors.']","The dish that demonstrated Chef Zhukovsky's creativity during food shortages was a saltimbocca made with pounded chicken breast, Ukrainian ham, and a bitter herb instead of sage.","Chef Zhukovsky made a saltimbocca using chicken breast, Ukrainian ham, and nettle in place of traditional ingredients.,",1.0,0.9892191912628254,0.9999999999,1.0,0.3999999992,0.9870928582725162
"How did the Count describe Andrey, the maître d’ of the Boyarsky?,","['He was about to explain to his new neighbor the cause of his unexpected\narrival, when from the hallway came the delicate clearing of a throat.\nWithout turning, the Count could tell that this was Andrey, the maître d’ of\nthe Boyarsky, for it was his trademark interruption.\nNodding once to the pigeon to indicate that they would resume their\ndiscussion anon, the Count rebuttoned his jacket and turned to find that it\nwas not Andrey alone who had paid a visit: three members of the hotel’s\nstaff were crowded in the doorway.\nThere was Andrey with his perfect poise and long judicious hands;\nVasily, the hotel’s inimitable concierge; and Marina, the shy delight with the\nwandering eye who had recently been promoted from chambermaid to\nseamstress. The three of them exhibited the same bewildered gaze that the\nCount had noticed on the faces of Arkady and Valentina a few hours before,\nand finally it struck him: When he had been carted off that morning, they\nhad all assumed that he would never return. He had emerged from behind\nthe walls of the Kremlin like an aviator from the wreckage of a crash.\n“My dear friends,” said the Count, “no doubt you are curious as to the\nday’s events. As you may know, I was invited to the Kremlin for a tête-à-\ntête. There, several duly goateed officers of the current regime determined\nthat for the crime of being born an aristocrat, I should be sentenced to spend\nthe rest of my days . . . in this hotel.”\nIn response to the cheers, the Count shook hands with his guests one by\none, expressing to each his appreciation for their fellowship and his\nheartfelt thanks.\n“Come in, come in,” he said.\nTogether, the three staff members squeezed their way between the\nteetering towers of furniture.\n“If you would be so kind,” said the Count, handing Andrey one of the\nbottles of brandy. Then he kneeled before the Ambassador, threw the clasps,\nand opened it like a giant book. Carefully secured inside were fifty-two\nglasses—or more precisely, twenty-six pairs of glasses—each shaped to its\npurpose, from the grand embrace of the Burgundy glass down to those\ncharming little vessels designed for the brightly colored liqueurs of\nsouthern Europe. In the spirit of the hour, the Count picked four glasses at\nrandom and passed them around as Andrey, having plucked the cork from\nthe bottle, performed the honors.', 'of the Boyarsky, and one watched in wonder as his hands fulfilled their\npurpose at every turn.\nHaving just led a group of women to their table, for instance, Andrey\nseemed to pull back their chairs all at once. When one of the ladies\nproduced a cigarette, he had a lighter in one hand and was guarding the\nflame with the other (as if a draft had ever been felt within the walls of the\nBoyarsky!). And when the woman holding the wine list asked for a\nrecommendation, he didn’t point to the 1900 Bordeaux—at least not in the\nTeutonic sense. Rather, he slightly extended his index finger in a manner\nreminiscent of that gesture on the Sistine Chapel’s ceiling with which the\nPrime Mover transmitted the spark of life. Then, excusing himself with a\nbow, he crossed the room and went through the kitchen door.\nBut before a minute could pass, the door swung open again—and there\nwas Emile.\nFive foot five and two hundred pounds, the chef glanced quickly about\nthe room then marched toward the Count with Andrey trailing behind. As\nhe crossed the dining room, the chef knocked into a customer’s chair and\nnearly toppled a busboy with his tray. Coming to an abrupt stop at the\nCount’s table, he looked him up and down as one might measure an\nopponent before challenging him to a duel.\n“Bravo, monsieur,” he said in a tone of indignation. “Bravo!”\nThen he turned on his heels and disappeared back into his kitchen.\nAndrey, a little breathless, bowed to express both apologies and\ncongratulations.\n“Nettle it was, Your Excellency. Your palate remains unsurpassed.”\nThough the Count was not a man to gloat, he could not repress a smile\nof satisfaction.\nKnowing that the Count had a sweet tooth, Andrey gestured toward the\ndessert cart.\n“May I bring you a slice of plum tart with our compliments . . . ?”\n“Thank you for the thought, Andrey. Normally, I would leap at the\nchance. But tonight, I am otherwise committed.”', 'In replying It is nothing to Andrey, the Count was not simply resorting to a\nGallic figure of speech. In point of fact, the Count deserved as much thanks\nfor his little intervention as a swallow deserves for its trill. For since the age\nof fifteen, Alexander Rostov had been a master of seating tables.\nWhenever he was home for the holidays, his grandmother would\ninevitably call him into the library, where she liked to knit by the fireplace\nin a solitary chair.\n“Come in, my boy, and sit with me a moment.”\n“Certainly, Grandmother,” replied the Count, balancing himself on the\nedge of the fire grate. “How can I be of assistance?”\n“The prelate is coming for dinner on Friday night—as are the Duchess\nObolensky, Count Keragin, and the Minsky-Polotovs. . . .”\nHere she would let her voice trail off without further explanation; but no\nfurther explanation was needed. The Countess was of a mind that dinner\nshould provide one with respite from life’s trials and tribulations. Thus, she\ncould not countenance discussions of religion, politics, or personal sorrows\nat her table. Further complicating matters, the prelate was deaf in his left\near, partial to Latin epigrams, and prone to stare at décolletage whenever he\ndrank a glass of wine; while the Duchess Obolensky, who was particularly\ncaustic in summer, frowned upon pithy sayings and could not abide\ndiscussions of the arts. And the Keragins? Their great-grandfather had been\ncalled a Bonapartist by Prince Minsky-Polotov in 1811, and they had not\nexchanged a word with a Minsky-Polotov since.\n“How many will be in attendance?” asked the Count.\n“Forty.”\n“The usual assembly?”\n“More or less.”\n“The Osipovs?”\n“Yes. But Pierre is in Moscow. . . .”\n“Ah,” said the Count with the smile of the chess champion who has been\nconfronted with a new gambit.\nThe Nizhny Novgorod Province had a hundred prominent families,\nwhich over the course of two centuries had intermarried and divorced,']",The Count described Andrey as having perfect poise and long judicious hands.,"The Count described Andrey as a master with elegant hands that moved like a conductor’s.,",1.0,0.960196685275459,0.99999999995,0.0,0.6666666644444444,0.4498530336769148
"What was Count Rostov’s response to his barber when his mustache was ruined?,","['G\nAchilles Agonistes\nreetings, Arkady.”\n“Greetings to you, Count Rostov. Is there something I can do for\nyou this morning?”\n“If it wouldn’t be too much trouble, could you spare a bit of stationery?”\n“Certainly.”\nStanding at the front desk, the Count penned a one-sentence note under\nthe hotel’s moniker and addressed the envelope in an appropriately slanted\nscript; he waited until the bell captain was otherwise occupied, casually\ncrossed the lobby, slipped the note onto the bell captain’s desk, and then\nheaded downstairs for his weekly visit to the barber.\nIt had been many years since Yaroslav Yaroslavl had worked his magic\nin the barbershop of the Metropol, and in the interim any number of\nsuccessors had attempted to fill his shoes. The most recent fellow—Boris\nSomething-or-other-ovich—was perfectly qualified to shorten a man’s hair;\nbut he was neither the artist nor the conversationalist that Yaroslav had\nbeen. In fact, he went about his business with such mute efficiency, one\nsuspected he was part machine.\n“Trim?” he asked the Count, wasting no time with subjects, verbs, or the\nother superfluities of language.\nGiven the Count’s thinning hair and the barber’s predisposition to\nefficiency, a trim might take all of ten minutes.\n“Yes, a trim,” said the Count. “But perhaps a shave as well. . . .”\nThe barber furrowed his brow. The man in him, no doubt, was inclined\nto point out that the Count had obviously shaved a few hours before; but the\nmachinery in him was so finely tuned, it was already putting down the\nscissors and reaching for the shaving brush.\nHaving whipped a sufficient lather, Boris dabbed it on those areas of the\nCount’s face where whiskers would have been had the Count been in need\nof a shave. He sharpened one of his razors on his strop, leaned over the', 'hand. Then, with the deftness of a much slighter man, he turned, took the\nCount by the collar, and severed the right wing of his moustaches with a\nsingle snip. Tightening his hold, he pulled the Count forward until they\nwere nearly nose to nose.\n“You’ll have your appointment soon enough,” he said.\nThen shoving the Count back in the chair, he tossed the scissors on the\nfloor and strolled from the shop.\n“Your Excellency,” exclaimed Yaroslav, aghast. “I have never seen the\nman in my life. I don’t even know if he resides in the hotel. But he is not\nwelcome here again, I assure you of that.”\nThe Count, who was standing now, was inclined to echo Yaroslav’s\nindignation and commend a punishment that fit the crime. But then, what\ndid the Count know about his assailant?\nWhen he had first seen him sitting on the bench in his rumpled jacket,\nthe Count had summed him up in an instant as some hardworking sort who,\nhaving stumbled upon the barbershop, had decided to treat himself to a cut.\nBut for all the Count knew, this fellow could have been one of the new\nresidents of the second floor. Having come of age in an ironworks, he could\nhave joined a union in 1912, led a strike in 1916, captained a Red battalion\nin 1918, and now found himself in command of an entire industry.\n“He was perfectly right,” the Count said to Yaroslav. “He had been\nwaiting in good faith. You only wished to honor my appointment. It was for\nme to cede the chair and suggest that you attend to him first.”\n“But what are we to do?”\nThe Count turned to the mirror and surveyed himself. He surveyed\nhimself, perhaps, for the first time in years.\nLong had he believed that a gentleman should turn to a mirror with a\nsense of distrust. For rather than being tools of self-discovery, mirrors\ntended to be tools of self-deceit. How many times had he watched as a\nyoung beauty turned thirty degrees before her mirror to ensure that she saw\nherself to the best advantage? (As if henceforth all the world would see her\nsolely from that angle!) How often had he seen a grande dame don a hat\nthat was horribly out of fashion, but that seemed au courant to her because\nher mirror had been framed in the style of the same bygone era? The Count\ntook pride in wearing a well-tailored jacket; but he took greater pride in', '“Splendid, Yaroslav. At my utmost.”\n“And what is on the docket for today?”\n“Just a trim, my friend. Just a trim.”\nAs the scissors began their delicate snipping, it seemed to the Count that\nthe heavyset customer on the bench had undergone something of a\ntransformation. Although the Count had given his friendly nod just\nmoments before, in the interim the fellow’s face seemed to have taken on a\nrosier hue. The Count was sure of it, in fact, because the color was\nspreading to his ears.\nThe Count tried to make eye contact again, intending to offer another\nfriendly nod, but the fellow had fixed his gaze on Yaroslav’s back.\n“I was next,” he said.\nYaroslav, who like most artists tended to lose himself in his craft,\ncontinued clipping away with efficiency and grace. So, the fellow was\nforced to repeat himself, if a little more emphatically.\n“I was next.”\nDrawn from his artistic spell by the sharper intonation, Yaroslav offered\na courteous reply:\n“I will be with you in just a moment, sir.”\n“That is what you said when I arrived.”\nThis was said with such unmistakable hostility that Yaroslav paused in\nhis clipping and turned to meet his customer’s glare with a startled\nexpression.\nThough the Count had been raised never to interrupt a conversation, he\nfelt that the barber should not be put in the position of having to explain the\nsituation on his behalf. So, he interceded:\n“Yaroslav meant no offense, my good man. It just so happens that I have\na standing appointment at twelve o’clock on Tuesdays.”\nThe fellow now turned his glare upon the Count.\n“A standing appointment,” he repeated.\n“Yes.”\nThen he rose so abruptly that he knocked his bench back into the wall.\nAt full height, he was no more than five foot six. His fists, which jutted\nfrom the cuffs of his jacket, were as red as his ears. When he advanced a\nstep, Yaroslav backed against the edge of his counter. The fellow took\nanother step toward the barber and wrested one of the scissors from his']","The Count did not respond to his barber when his mustache was ruined. Instead, he stood up and was inclined to echo Yaroslav’s indignation and commend a punishment that fit the crime, but then realized he did not know anything about his assailant.","The Count calmly requested a “clean shave,” demonstrating his grace under pressure.,",0.6666666666666666,0.0,0.49999999995,0.0,0.0,0.22032315290599774
"What significance did the Grand Duke’s desk hold for Count Rostov?,","['epaulettes, the Grand Duke spoke four languages and read six. Never to\nwed, he represented his country at Portsmouth, managed three estates, and\ngenerally prized industry over nonsense. But before all of that, he had\nserved alongside the Count’s father as a devil-may-care cadet in the cavalry.\nThus had the Grand Duke become the Count’s watchful guardian. And\nwhen the Count’s parents succumbed to cholera within hours of each other\nin 1900, it was the Grand Duke who took the young Count aside and\nexplained that he must be strong for his sister’s sake; that adversity presents\nitself in many forms; and that if a man does not master his circumstances\nthen he is bound to be mastered by them.\nThe Count ran his hand across the desk’s dimpled surface.\nHow many of the Grand Duke’s words did those faint indentations\nreflect? Here over forty years had been written concise instructions to\ncaretakers; persuasive arguments to statesmen; exquisite counsel to friends.\nIn other words, it was a desk to be reckoned with.\nEmptying his glass, the Count pushed his chair back and sat on the floor.\nHe ran his hand behind the desk’s right front leg until he found the catch.\nWhen he pressed it, a seamless door opened to reveal a velvet-lined hollow\nthat, like the hollows in the other three legs, was stacked with pieces of\ngold.', 'Someone had shown the courtesy of having one of the Count’s traveling\ntrunks brought to his bedroom. So, as the bellhops carried the\naforementioned upward, the Count filled the trunk with clothes and\npersonal effects. Noting that the guards were eyeing the two bottles of\nbrandy on the console, the Count tossed them in as well. And once the trunk\nhad been carried upstairs, he finally pointed to the desk.\nThe two bellhops, their bright blue uniforms already smudged from their\nefforts, took hold of it by the corners.\n“But it weighs a ton,” said one to the other.\n“A king fortifies himself with a castle,” observed the Count, “a\ngentleman with a desk.”\nAs the bellhops lugged it into the hall, the Rostovs’ grandfather clock,\nwhich was fated to be left behind, tolled a doleful eight. The captain had\nlong since returned to his post and the guards, having swapped their\nbelligerence for boredom, now leaned against the wall and let the ashes\nfrom their cigarettes fall on the parquet floor while into the grand salon\npoured the undiminished light of the Moscow summer solstice.\nWith a wistful eye, the Count approached the windows at the suite’s\nnorthwest corner. How many hours had he spent before them? How many\nmornings dressed in his robe with his coffee in hand had he observed the\nnew arrivals from St. Petersburg disembarking from their cabs, worn and\nweary from the overnight train? On how many winter eves had he watched\nthe snow slowly descending as some lone silhouette, stocky and short,\npassed under a street lamp? At that very instant, at the square’s northern\nextreme a young Red Army officer rushed up the steps of the Bolshoi,\nhaving missed the first half hour of the evening’s performance.\nThe Count smiled to remember his own youthful preference for arriving\nentr’acte. Having insisted at the English Club that he could only stay for\none more drink, he stayed for three. Then leaping into the waiting carriage,\nhe’d flash across the city, vault the fabled steps, and like this young fellow\nslip through the golden doors. As the ballerinas danced gracefully across\nthe stage, the Count would be whispering his excusez-moi’s, making his\nway to his usual seat in the twentieth row with its privileged view of the\nladies in the loges.\nArriving late, thought the Count with a sigh. What a delicacy of youth.', 'At ten o’clock the Count was seated in the Boyarsky with an empty plate\nand a nearly empty bottle of White on the table. With the day drawing\nrapidly to a close, he took some pride in knowing that everything was in\norder.\nThat morning, having received a visit from Konstantin Konstantinovich,\nthe Count had brought his accounts up to date at Muir & Mirrielees (now\nknown as the Central Universal Department Store), Filippov’s (the First\nMoscow Bakery), and, of course, the Metropol. At the Grand Duke’s desk,\nhe had written a letter to Mishka, which he had then entrusted to Petya with\ninstructions it be mailed on the following day. In the afternoon, he had paid\nhis weekly visit to the barber and tidied up his rooms. He had donned his\nburgundy smoking jacket (which, to be perfectly frank, was disconcertingly\nsnug), and in its pocket he placed a single gold coin for the undertaker with\ninstructions that he be dressed in the freshly pressed black suit (which had\nbeen laid out on his bed), and that his body be buried in the family plot at\nIdlehour.\nBut if the Count took pride in knowing that everything was in order, he\ntook comfort in knowing that the world would carry on without him—and,\nin fact, already had. The night before, he had happened to be standing at the\nconcierge’s desk when Vasily produced a map of Moscow for one of the\nhotel’s guests. As Vasily drew a zigzagging line from the center of the city\nto the Garden Ring, more than half of the streets he named were unfamiliar\nto the Count. Earlier that day, Vasily had informed him that the famed blue-\nand-gold lobby of the Bolshoi had been painted over in white, while in the\nArbat Andreyev’s moody statue of Gogol had been plucked from its\npedestal and replaced with a more uplifting one of Gorky. Just like that, the\ncity of Moscow could boast new street names, new lobbies, and new statues\n—and neither the tourists, the theatergoers, nor the pigeons seemed\nparticularly put out.\nThe staffing trend that had begun with the appointment of the Bishop\nhad continued unabated—such that any young man with more influence\nthan experience could now don the white jacket, clear from the left, and\npour wine into water glasses.\nMarina, who once had welcomed the Count’s company as she stitched in\nthe stitching room, now had a junior seamstress to watch over as well as a\ntoddler at home (God bless).']","The Grand Duke's desk held great significance for Count Rostov as it contained over forty years of written instructions, persuasive arguments, and counsel. It was a desk to be reckoned with and reflected the wisdom and guidance that the Grand Duke had provided to the Count over the years.","The desk was a family heirloom where the Count stored gold coins and cherished his father’s legacy.,",1.0,0.9796223346305144,0.0,0.0,0.1999999996,0.3578853334398556
"What role did the Metropol Hotel play in Count Rostov’s life after his house arrest?,","['As Viktor took a sip from his coffee, the Count watched the accordion\nplayer with interest.\n“Have you ever seen Casablanca?” he asked.\nSomewhat bewildered, Viktor admitted that he had not.\n“Ah. You must see it one day.”\nAnd so the Count told Viktor about his friend Osip and their recent\nviewing of the movie. In particular, he described the scene in which a small-\ntime crook was dragged away by the police and how the American\nsaloonkeeper, having assured his customers that everything was all right,\ncasually instructed his bandleader to play on.\n“My friend was very impressed with this,” explained the Count. “He\nsaw the saloonkeeper’s instruction to the piano player to start playing so\nsoon after the arrest as evidence of his indifference to the fates of other\nmen. But I wonder. . . .”\nThe following morning at half past eleven, two officers of the KGB arrived\nat the Metropol Hotel in order to question Headwaiter Alexander Rostov on\nan undisclosed matter.\nHaving been escorted by a bellhop to Rostov’s room on the sixth floor,\nthe officers found no sign of him there. Nor was he receiving a trim in the\nbarbershop, lunching at the Piazza, or reading the papers in the lobby.\nSeveral of Rostov’s closest associates, including Chef Zhukovsky and\nMaître d’ Duras, were questioned, but none had seen Rostov since the\nprevious night. (The officers also endeavored to speak with the hotel’s\nmanager, only to find that he had not yet reported to work—a fact that was\nduly noted in his file!) At one o’clock, two additional KGB men were\nsummoned so that a more thorough search could be made of the hotel. At\ntwo, the senior officer conducting the investigation was encouraged to\nspeak with Vasily, the concierge. Finding him at his desk in the lobby\n(where he was in the midst of securing theater tickets for a guest), the\nofficer did not beat about the bush. He put his question to the concierge\nunambiguously:\n“Do you know the whereabouts of Alexander Rostov?”\nTo which the concierge replied: “I haven’t the slightest idea.”', '“Meet me here,” she said, “tomorrow at two.”\nAnd thus began the Count’s education.\nHaving lived at the Metropol for four years, the Count considered himself\nsomething of an expert on the hotel. He knew its staff by name, its services\nby experience, and the decorative styles of its suites by heart. But once Nina\nhad taken him in hand, he realized what a novice he had been.\nIn the ten months that Nina had lived at the Metropol, she had been\nconfronted with her own version of confinement. For, as her father had been\nposted only “temporarily” to Moscow, he had not bothered to enroll her in\nschool. And as Nina’s governess still had one foot set firmly in the\nhinterlands, she preferred that her charge remain on the hotel’s premises\nwhere she was less likely to be corrupted by street lamps and trolley cars.\nSo, if the door of the Metropol was known the world over for spinning\nwithout stop, it spun not for Nina. But, an enterprising and tireless spirit, the\nyoung lady had made the most of her situation by personally investigating\nthe hotel until she knew every room, its purpose, and how it might be put to\nbetter use.\nYes, the Count had gone to the little window at the back of the lobby to\nask for his mail, but had he been to the sorting room where the incoming\nenvelopes were spilled on a table at ten and at two—including those that\nwere stamped in red with the unambiguous instruction For Immediate\nDelivery?\nAnd yes, he had visited Fatima’s in the days when it was open, but had\nhe been inside the cutting room? Through a narrow door at the back of her\nshop was that niche with a light green counter where stems had been\nsnipped and roses dethorned, where even now one could find scattered\nacross the floor the dried petals of ten perennials essential to the making of\npotions.\nOf course, exclaimed the Count to himself. Within the Metropol there\nwere rooms behind rooms and doors behind doors. The linen closets. The\nlaundries. The pantries. The switchboard!\nIt was like sailing on a steamship. Having enjoyed an afternoon shooting\nclay pigeons off the starboard bow, a passenger dresses for dinner, dines at', 'Once his guests had their brandy in hand, the Count raised his own on\nhigh.\n“To the Metropol,” he said.\n“To the Metropol!” they replied.\nThe Count was something of a natural-born host and in the hour that\nensued, as he topped a glass here and sparked a conversation there, he had\nan instinctive awareness of all the temperaments in the room. Despite the\nformality appropriate to his position, tonight Andrey exhibited a ready\nsmile and an occasional wink. Vasily, who spoke with such pointed\naccuracy when providing directions to the city’s sights, suddenly had the lilt\nof one who may or may not remember tomorrow what he had said today.\nAnd at every jest, the shy Marina allowed herself to giggle without placing\na hand in front of her lips.\nOn this of all nights, the Count deeply appreciated their good cheer; but\nhe was not so vain as to imagine it was founded solely on news of his\nnarrow escape. For as he knew better than most, it was in September of\n1905 that the members of the Delegation had signed the Treaty of\nPortsmouth to end the Russo-Japanese War. In the seventeen years since the\nmaking of that peace—hardly a generation—Russia had suffered a world\nwar, a civil war, two famines, and the so-called Red Terror. In short, it had\nbeen through an era of upheaval that had spared none. Whether one’s\nleanings were left or right, Red or White, whether one’s personal\ncircumstances had changed for the better or changed for the worse, surely at\nlong last it was time to drink to the health of the nation.\nAt ten o’clock, the Count walked his guests to the belfry and bid them\ngoodnight with the same sense of ceremony that he would have exhibited at\nthe door of his family’s residence in St. Petersburg. Returning to his\nquarters, he opened the window (though it was only the size of a postage\nstamp), poured the last of the brandy, and took a seat at the desk.\nBuilt in the Paris of Louis XVI with the gilded accents and leather top of\nthe era, the desk had been left to the Count by his godfather, Grand Duke\nDemidov. A man of great white sideburns, pale blue eyes, and golden']",The Metropol Hotel became Count Rostov's home after his house arrest. He lived there for several years and became very familiar with the hotel and its staff.,"The hotel became both his prison and his refuge, where he maintained his routine and dignity.,",0.75,0.9122569525736904,0.49999999995,0.0,0.0,0.6289460848701526
"Why did the Count smile when he recognized nettle in his saltimbocca?,","['Looking up from the fennel, Emile met the eye of the Count and smiled.\nIn an instant the Count could see that the chef was in rosy form. Having\nsensed at two that all might not be lost, at half past midnight the chef hadn’t\nthe slightest doubt that the sun would shine tomorrow, that most people\nwere generous at heart, and that, when all was said and done, things tended\nto work out for the best.\nThe chef wasted no time on salutations. Instead, without pausing his\nchopper, he tilted his head toward the little table, which had been moved\nfrom his office into the kitchen and which had been waiting patiently to be\nset.\nBut first things first.\nCarefully, the Count removed the little cordial glass from his back\npocket and placed it on the counter.\n“Ah,” said the chef, wiping his hands on his apron.\n“Is it enough?”\n“It is only meant to be a hint. An aside. An innuendo. If it is the real\nthing, it should be plenty.”\nEmile dipped his pinkie in the absinthe and gave it a lick.\n“Perfect,” he said.\nSelecting an appropriate tablecloth from the linen closet, the Count\nunfurled it with a snap and let it billow to the table. As he set the places, the\nchef began to whistle a tune and the Count smiled to realize it was the very\nsame song that he had heard in the Shalyapin regarding the absence of\nbananas. As if on cue, the door to the back stair opened and in rushed\nAndrey with a pile of oranges about to tumble from his arms. Reaching\nEmile’s side, he bowed at the waist and spilled them onto the counter.\nWith the instincts of convicts who discover the gates of their prison\nopen, the individual oranges rolled in every direction to maximize their\nchances of escape. In a flash, Andrey had extended his arms in a grand\ncircumference to fence them in. But one of the oranges dodged the maître\nd’s reach and shot across the counter—headed straight for the absinthe!\nDropping his chopper, Emile lunged and plucked the glass from the counter\nin the nick of time. The orange, which was gaining in confidence, dashed\nbehind the fennel, jumped from the counter, thudded to the floor, and made\na break for the exit. But at the last moment, that door that separated Emile’s\nkitchen from the rest of the world swung inward, sending the orange', 'anticipated earlier that day. But having long since lost interest in the fickle\ncourse of human affairs, they dropped to the floor and trotted back to the\ndrawing room without a second glance.\nThe actress watched them retire a little wistfully.\n“I confess that I am not a dog lover.”\n“Then why do you have them?”\n“They were . . . a gift.”\n“Ah. From an admirer.”\nShe responded with a wry smile. “I would have settled for a necklace.”\nThe Count returned the smile.\n“Well,” she said. “Let’s see what we’ve got.”\nRemoving the silver dome from the serving plate, the actress revealed\none of Emile’s signature dishes: whole bass roasted with black olives,\nfennel, and lemon.\n“Lovely,” she said.\nAnd the Count could not agree more. For by setting his oven to 450˚,\nEmile ensured that the flesh of the fish was tender, the fennel aromatic, and\nthe lemon slices blackened and crisp.\n“So, two restaurants and a bar without having a bite to eat . . .”\nThus began the Count, with the natural intention of letting the actress\nrecount her day while he prepared her plate. But before he could lift a\nfinger, she had taken the knife and serving fork in hand. And as she began\nto relate the professional obligations that had commandeered her afternoon,\nshe scored the fish’s spine with the tip of the knife and made diagonal cuts\nat its head and tail. Then slipping the serving fork between the fish’s spine\nand its flesh, she deftly liberated the filet. In a few succinct movements, she\nhad served portions of the fennel and olives, and topped the filet with the\ncharred lemon. Handing the Count this perfectly prepared plate, she\nplucked the spine from the fish and served herself the second filet with\naccompaniments—an operation that took no more than a minute. Then\nplacing the serving utensils on the platter, she turned her attention to the\nwine.\nGood God, thought the Count. So engrossed had he been in watching\nher technique, he had neglected his own responsibilities. Leaping from his\nchair, he took the bottle by the neck.\n“May I?”', 'for his part, the Count cast off any thoughts of herb-encrusted lamb at the\nBoyarsky. Instead, he summoned Petya to take Nina’s present to his room\nand ordered the Latvian stew and a bottle of the Mukuzani for himself.\nAnd just as he’d suspected, it was the perfect dish for the season. The\nonions thoroughly caramelized, the pork slowly braised, and the apricots\nbriefly stewed, the three ingredients came together in a sweet and smoky\nmedley that simultaneously suggested the comfort of a snowed-in tavern\nand the jangle of a Gypsy tambourine.\nAs the Count took a sip of his wine, the young couple caught his eye and\nraised their own glasses in a toast of gratitude and kinship. Then they\nreturned to their conversation, which had grown so intimate, it could no\nlonger be heard over the sound of the accordion.\nYoung love, thought the Count with a smile. There is nothing novaya\nabout it.\n“Will there be anything else?”\nIt was the Bishop addressing the Count. He considered for a moment,\nthen he asked for a single scoop of vanilla ice cream.\nAs the Count entered the lobby, he noticed four men in evening dress\ncoming through the door with black leather cases in hand, clearly one of the\nstring quartets that occasionally played in the private dining rooms upstairs.\nThree of the musicians looked as if they had been performing together\nsince the nineteenth century, sharing the same white hair and weary\nprofessionalism. But the second violinist stood out from the others as he\ncouldn’t have been more than twenty-two and retained a certain brightness\nto his step. It was only as the quartet approached the elevator that the Count\nrecognized him.\nThe Count probably hadn’t seen Nikolai Petrov since 1914 when the\nPrince had been no more than a lad of thirteen; and given the passage of\ntime, the Count might not have recognized him at all were it not for his\nunassuming smile—a distinguishing feature of the Petrov line for\ngenerations.\n“Nikolai?”']",The question is not relevant to the context provided. The Count did not recognize nettle in his saltimbocca in the given context.,"The Count appreciated the chef’s resourcefulness and the creative substitution of ingredients during food shortages.,",0.6666666666666666,0.0,0.0,0.0,0.3333333322222222,0.19863213710917976
"How did the Count entertain his sister Helena in their childhood?,","['courtship. I suffered through lunches and teas. I ground my teeth as I\nwatched them stroll through the gardens. But as I bided my time, his self-\ncontrol surpassed my wildest expectations. He pulled out her chair; he\npicked blossoms; he read verses; he wrote verses! And always when he\ncaught my eye there was that little twist in his smile.\n“But then on the afternoon of my sister’s twentieth birthday, when he\nwas off on maneuvers and we were paying a visit to a neighbor, we returned\nat dusk to find his troika in front of our house. From a glance at Helena, I\ncould sense her elation. He has rushed back all the way from his battalion,\nshe was thinking, to wish me well on my day. She nearly jumped from her\nhorse and ran up the steps; and I followed her like a condemned man to the\nnoose.”\nThe Count emptied his glass and slowly set it back onto the bar.\n“But there inside the entry hall, I did not find my sister in his arms. I\nfound her two steps from the door, trembling. Against the wall was\nNadezhda, my sister’s handmaiden. Her bodice torn open, her arms across\nher chest, her face scarlet with humiliation, she looked briefly at my sister\nthen ran up the stairs. In horror, my sister stumbled across the hall,\ncollapsed in a chair, and covered her face with her hands. And our noble\nlieutenant? He grinned at me like a cat.\n“When I began to express my outrage, he said: ‘Oh, come now,\nAlexander. It is Helena’s birthday. In her honor, let us call it even.’ Then\nroaring with laughter, he walked out the door without giving my sister a\nglance.”\nCharles whistled softly.\nThe Count nodded.\n“But at this juncture, Charles, I did not do nothing. I crossed the\nentryway to the wall where a pair of pistols hung beneath the family crest.\nWhen my sister grabbed at my sleeve and asked where I was going, I too\nwalked out the door without giving her a glance.”\nThe Count shook his head in condemnation of his own behavior.\n“He had a one-minute head start, but he hadn’t used it to put distance\nbetween us. He had casually climbed into his troika and set his horses\nmoving at little more than a trot. And there you have him in a nutshell, my\nfriend: a man who raced toward parties, and trotted from his own\nmisdeeds.”', '“Natasha,” confirmed the Count with gratitude. “I’ll speak to her first\nthing tomorrow. And a reasonable wage, absolutely. Thank you so much,\nMarina. I’ll send you and Sofia dinner from the Boyarsky around seven;\nand if last night is any indication, she will be sound asleep by nine.”\nThe Count turned to go, then turned back again.\n“And, I’m sorry about earlier. . . .”\n“It’s all right, Alexander. You were anxious because you haven’t spent\ntime with children before. But I am certain that you are up to the challenge.\nIf you are ever in doubt, just remember that unlike adults, children want to\nbe happy. So they still have the ability to take the greatest pleasure in the\nsimplest things.” By way of example, the seamstress placed something\nsmall and seemingly insignificant in the Count’s hand with an assurance\nand a few words of instruction.\nAs a result, when the Count and Sofia had climbed the five flights back\nto their rooms and she had turned her deep blue gaze of expectation upon\nhim, the Count was ready.\n“Would you like to play a game?” he asked.\n“I would,” she said.\n“Then come this way.”\nWith a touch of ceremony, the Count ushered Sofia through the closet\ndoor into the study.\n“Ooo,” she said as she emerged on the other side. “Is this your secret\nroom?”\n“It is our secret room,” the Count replied.\nSofia nodded gravely to show that she understood.\nBut then children understand the purpose of secret rooms better than\nthey understand the purpose of congresses, courtrooms, and banks.\nSomewhat shyly, Sofia pointed at the painting.\n“Is that your sister?”\n“Yes. Helena.”\n“I like peaches too.” She ran a hand along the coffee table. “Is this\nwhere your grandma had tea?”\n“Exactly.”\nSofia nodded gravely again.\n“I am ready for the game.”', 'The Count looked into his empty glass. He hadn’t spoken of these events\nin so many years.\n“It is late,” he said. “And the story is long.”\nBy way of response, Charles refilled their glasses.\nSo the Count took Charles all the way back to the fall of 1913, when on\nan inclement night he had set out for the twenty-first birthday of the\nPrincess Novobaczky. He described the ice on the driveway, and Mrs.\nTrent’s roast, and the torn IOU—and how a few degrees here and there had\nlanded him on the terrace in the arms of the Princess while the rash\nlieutenant retched in the grass.\nCharles laughed.\n“But, Alexander, that sounds splendid. Surely, it’s not the reason you left\nRussia.”\n“No,” admitted the Count, but then he continued with his fateful tale:\n“Seven months pass, Charles. It is the spring of 1914, and I return to the\nfamily estate for a visit. Having paid my respects to my grandmother in the\nlibrary, I venture outside in search of my sister, Helena, who likes to read\nunder the great elm at the bend in the river. From a hundred feet away, I can\ntell that she is not herself—that is, I can tell that she is more than herself.\nUpon seeing me she sits up with a sparkle in her eye and a smile on her lips,\nclearly eager to share some piece of news, which I am now equally eager to\nhear. But just as I cross the lawn toward her, she looks over my shoulder\nand smiles even more brightly to see a lone figure approaching on a steed—\na lone figure in the uniform of the Hussars. . . .\n“You see the dilemma the fox had put me in, Charles. While I had been\ncarousing back in Moscow, he had sought my sister out. He had arranged an\nintroduction and then courted her carefully, patiently, successfully. And\nwhen he swung down from the saddle and our eyes met, he could barely\nkeep the twist of mirth from his lips. But how was I to explain the situation\nto Helena? This angel of a thousand virtues? How was I to tell her that the\nman she has fallen in love with has sought her affections not due to an\nappreciation of her qualities, but to settle a score?”\n“What did you do?”\n“Ah, Charles. What did I do? I did nothing. I thought surely his true\nnature would find occasion to express itself—much as it had at the\nNovobaczkys’. So in the weeks that followed, I hovered at the edge of their']","The Count entertained his sister Helena by playing games with her in their secret room, showing her paintings, and talking about their grandmother having tea in the room.","He read Pushkin aloud while she embroidered, a routine that strengthened their bond.,",0.0,0.9430929545073141,0.0,0.0,0.0,0.20957487974250932
"Why did Count Rostov choose not to focus on revenge or grand dreams?,","['In replying It is nothing to Andrey, the Count was not simply resorting to a\nGallic figure of speech. In point of fact, the Count deserved as much thanks\nfor his little intervention as a swallow deserves for its trill. For since the age\nof fifteen, Alexander Rostov had been a master of seating tables.\nWhenever he was home for the holidays, his grandmother would\ninevitably call him into the library, where she liked to knit by the fireplace\nin a solitary chair.\n“Come in, my boy, and sit with me a moment.”\n“Certainly, Grandmother,” replied the Count, balancing himself on the\nedge of the fire grate. “How can I be of assistance?”\n“The prelate is coming for dinner on Friday night—as are the Duchess\nObolensky, Count Keragin, and the Minsky-Polotovs. . . .”\nHere she would let her voice trail off without further explanation; but no\nfurther explanation was needed. The Countess was of a mind that dinner\nshould provide one with respite from life’s trials and tribulations. Thus, she\ncould not countenance discussions of religion, politics, or personal sorrows\nat her table. Further complicating matters, the prelate was deaf in his left\near, partial to Latin epigrams, and prone to stare at décolletage whenever he\ndrank a glass of wine; while the Duchess Obolensky, who was particularly\ncaustic in summer, frowned upon pithy sayings and could not abide\ndiscussions of the arts. And the Keragins? Their great-grandfather had been\ncalled a Bonapartist by Prince Minsky-Polotov in 1811, and they had not\nexchanged a word with a Minsky-Polotov since.\n“How many will be in attendance?” asked the Count.\n“Forty.”\n“The usual assembly?”\n“More or less.”\n“The Osipovs?”\n“Yes. But Pierre is in Moscow. . . .”\n“Ah,” said the Count with the smile of the chess champion who has been\nconfronted with a new gambit.\nThe Nizhny Novgorod Province had a hundred prominent families,\nwhich over the course of two centuries had intermarried and divorced,', 'Russia galled his sense of justice. Or maybe it was simply the cold\nsatisfaction of the envious. For those who had difficulty in school or at\nmaking friends when they were young will forever recognize with a bitter\nglance those for whom life has seemed to come easy.\nGloating, righteousness, satisfaction, who can say? But the emotion the\nBishop felt upon opening the door to his office was almost certainly that of\nshock—for the adversary that he had left in the attic just minutes before was\nnow sitting behind the manager’s desk with a pistol in his hand.\nHow was this possible?\nWhen the Bishop left the Count’s bedroom, the Count was frozen in\nplace by a torrent of emotions—by feelings of fury, incredulity, self-\nrecrimination, and fear. Rather than burn the map, like a fool he had slipped\nit in his drawer. Six months of the most careful planning and painstaking\nexecution overturned by a single misstep. And what was worse, he had put\nSofia at risk. What price was she to pay for his carelessness?\nBut if the Count was frozen in place, he was frozen for all of five\nseconds. For these perfectly understandable sentiments, which threatened to\ndrain the blood from his heart, were swept aside by resolve.\nTurning on his heels, the Count went to the head of the belfry and\nlistened until the Bishop had descended the first two flights of stairs. Still in\nhis stocking feet, the Count began to follow in the Bishop’s footsteps; but\nwhen he got to the fifth floor, he exited the belfry, sped down the hallway,\nand ran down the main staircase, just as Sofia had at the age of thirteen.\nAs if he were still enshrouded in a mist, when the Count alit from the\nstairs, he ran down the hall and entered the executive offices without being\nseen by a soul; but upon reaching the Bishop’s door, he discovered it was\nlocked. Even as he was taking the Lord’s name in vain, the Count slapped\nhis hands against his vest with relief. For he still had Nina’s passkey in his\npocket. Letting himself in, the Count relocked the door and crossed to the\nwall where the filing cabinets had taken the place of Mr. Halecki’s chaise.\nCounting from the portrait of Karl Marx, the Count placed his hand in the\ncenter of the second panel to the right, gave a push, and popped it open.', 'Rostov: By that point, I’m afraid that my days of taking up arms were behind me.\nVyshinsky: Why then did you come back?\nRostov: I missed the climate.\n[Laughter.]\nVyshinsky: Count Rostov, you do not seem to appreciate the gravity of your position.\nNor do you show the respect that is due the men convened before you.\nRostov: The Tsarina had the same complaints about me in her day.\nIgnatov: Prosecutor Vyshinsky. If I may . . .\nVyshinsky: Secretary Ignatov.\nIgnatov: I have no doubt, Count Rostov, that many in the gallery are surprised to\nfind you so charming; but I, for one, am not surprised in the least. History has\nshown charm to be the final ambition of the leisure class. What I do find\nsurprising is that the author of the poem in question could have become a man so\nobviously without purpose.\nRostov: I have lived under the impression that a man’s purpose is known only to God.\nIgnatov: Indeed. How convenient that must have been for you.\n[The Committee recesses for twelve minutes.]\nIgnatov: Alexander Ilyich Rostov, taking into full account your own testimony, we\ncan only assume that the clear-eyed spirit who wrote the poem Where Is It Now? has\nsuccumbed irrevocably to the corruptions of his class—and now poses a threat to\nthe very ideals he once espoused. On that basis, our inclination would be to have\nyou taken from this chamber and put against the wall. But there are those within\nthe senior ranks of the Party who count you among the heroes of the\nprerevolutionary cause. Thus, it is the opinion of this committee that you should\nbe returned to that hotel of which you are so fond. But make no mistake: should\nyou ever set foot outside of the Metropol again, you will be shot. Next matter.\nBearing the signatures of\nV. A. Ignatov\nM. S. Zakovsky\nA. N. Kosarev']",Count Rostov chose not to focus on revenge or grand dreams because he believed that a man's purpose is known only to God.,"He preferred to master his circumstances practically, similar to Robinson Crusoe surviving on his island.,",0.3333333333333333,0.9908603174701321,0.0,0.0,0.0,0.20185898145511963
"What is the person doing on the beach in the images?,","['a person sitting on a surf board on a beach', 'an image of a group of people that are walking on beach', 'A man in shorts is laying on the beach.']",The person in the images is sitting on a surfboard.,"The person is likely surfing or preparing to surf in the water.,",0.0,0.9249831270874657,0.9999999999,1.0,0.0,0.22882053871358027
"What is in the sky during the day?,","['A colorful kite flying through a cloudy blue sky.', 'A kite is flying in a clear sky.', 'A yellow and blue plant flying in a gray sky.']",A colorful kite flying through a cloudy blue sky.,"During the day, you can see various things in the sky like kites, plants, airplanes, jets, and clouds.,",1.0,0.8626880994627105,0.99999999995,1.0,0.0,0.5209701562133936
"What is in the orange room?,","['An orange room with a bookcase, chairs, and a TV.', 'a bathroom with some orange walls on it ', 'A living room with orange colored walls, and a purple chair.']","A bookcase, chairs, and a TV.","The orange room contains a bookcase, chairs, and a TV.,",1.0,0.8316859152385444,0.9999999999,1.0,0.9999999975,0.9815828445818138
"Who is sitting at the table with a small cake?,","['A woman sitting at a table with a cake in front of her.', 'A smiling man sitting by a very nice looking cake.', 'A man and woman cutting a cake at a table.']",A woman,"A woman is sitting at the table with a small cake in front of her.,",,0.7753797886292498,0.8333333332916666,1.0,0.9999999966666667,0.21123061037299645
"Where is the brown horse?,","['A brown horse walking outside in the grass', 'A brown horse with blonde hair standing in an open field.', 'A man riding on the back of a brown horse.']",The brown horse is outside in the grass or standing in an open field.,"The brown horse is outside in an open field, walking, standing, being ridden by a man, standing over a light-colored dog, or next to a woman.,",1.0,0.0,0.49999999995,0.6666666666666666,0.3333333322222222,0.5707869108828809
"What is the man doing on the skateboard? ,","['A man who is performing a trick on a skateboard.', 'A man is performing tricks on a skateboard.', 'A man on a skateboard doing a trick.']",The man is performing tricks on the skateboard.,"The man is performing tricks or stunts on the skateboard.,",1.0,0.9791109856188189,0.9999999999666667,1.0,0.7499999981250001,0.8441114985141985
"What is the man doing with skis?,","['A man with skis and poles is on snow.', 'A man is in his skiing gear, while posing for the camera. ', 'A man flying through the air while riding skis.']",The man is skiing.,"A man with skis and ski poles is standing next to a hill covered in snow.,",1.0,0.8971511654029309,0.9999999999,1.0,0.3999999992,0.5288031184987873
"What is walking through the grass and leaves?,","['The woman is walking carefully through the leaves.', 'a couple of animals walk through the grass ', 'a bunch of kids walking through some grass']",A couple of animals.,"The woman, a bunch of kids, a couple of animals, a train, some animals in the distance, and a bear are all walking through the grass and leaves.,",1.0,0.0,0.3333333333,0.0,0.7142857132653061,0.4264693144883499
"What is in the room filled with bananas?,","['A bunch of bananas is displayed on a counter top.', 'A closeup of a group of bananas on a table ', 'A bunch of green bananas laying on a stone table. ']",There is a stone table in the room filled with bananas.,"It seems like the room is filled with different displays of bananas: on countertops, tables, hanging from trees, and arranged on display cases.,",0.0,0.8935182332156463,0.0,0.0,0.9999999900000002,0.22761346826900808
"What is the young man doing on the tennis court?,","['The young man in the blue shirt is playing tennis', 'A young man standing on top of a tennis court holding a racquet.', 'A young man holds up a tennis racket.']",The young man is playing tennis.,"The young man is playing tennis.,",,0.9245660686217686,0.9999999999666667,1.0,0.4999999975,0.9937791152170778
" What food item is on the fryer assembly line?,","['a burger and some fries on a tray', 'A metal tray filled with fried donuts and metal tongs.', 'A sandwich, french fries, bowl of ketchup, onion slice, lettuce slice, tomato slice, and knife sit on the white plate.']",Fried donuts,"Rows and rows of doughnuts are on the fryer assembly line.,",,0.8112372817717496,0.49999999995,1.0,0.0,0.21743737377723066
"What is happening at the zoo involving a giraffe and a bird?,","['Zebra and giraffe at a zoo in a cage. ', 'A giraffe walking about in a large enclosure. ', 'Some giraffes are standing in the middle of the zoo exhibit. ']",There is no mention of a bird in the context provided.,"A giraffe is standing in a field with a bird.,",1.0,0.8256399461734863,0.0,0.0,0.4999999975,0.20598353065625158
"What does the sign with cartoon characters appear near?,","['a sign showing a place beside the road ', 'A street sign in the city with various symbols on it. ', 'There is a street sign on top of a stop sign near a tree. ']",The sign with cartoon characters appears near a tree.,"The sign with cartoon characters appears near a park.,",0.0,0.9341151584229861,0.0,0.0,0.0,0.242304605591499
"Where is the orange fire hydrant located?,","['An orange fire hydrant with a face and bow tie drawn on it. ', 'An orange fire hydrant with graffiti to look like a face.', 'A red and yellow fire hydrant sitting on the side of a road.']",The orange fire hydrant with a face and bow tie drawn on it is located on the side of a road.,"The orange fire hydrant with a face and bow tie drawn on it and the one with graffiti to look like a face are not specified in a particular location. So, the exact location of those fire hydrants cannot be determined from the given context.,",0.5,0.9395079229028154,0.0,1.0,0.4999999975,0.47766293900422135
"What is happening in the snow-covered scene with the three skiers?,","['Three people with ski poles standing in the snow', 'Two skiers in dark clothing are standing on the snowy slope. ', 'Three people on packed snow trail, two skiing, one walking.']",Two skiers are skiing on the snowy slope while one person is walking on the packed snow trail.,"Three skiers are either standing, skiing, or posing for a picture in the snow-covered scene.,",0.5,0.8961841542282176,0.8333333332916666,1.0,0.4999999975,
"What is the small cat sitting on in the bathroom?,","['A black cat sitting on top of a bathroom sink.', 'A cat sitting in a bathroom sink next to a mirror.', 'A cat sits on the edge of a toilet.']",The small cat is sitting on top of a bathroom sink.,"The cat is sitting in various places in the bathroom such as the sink, toilet, and the edge of the toilet seat.,",0.0,0.9308858904174815,0.8333333332916666,1.0,0.1999999996,0.5330328104128572
"Where is the man standing while holding a motorcycle?,","['a man is standing next to a motorcycle outside', 'A man standing next to a parked motorcycle.', 'Man in black jacket standing in front of a black motorcycle. ']",The man is standing next to the motorcycle outside.,"The man is standing next to or in front of the motorcycle in all the provided contexts.,",1.0,0.9104919714570371,0.9999999999666667,1.0,0.999999995,0.608990571688572
"Why might a bus be painted half pink and half yellow?,","['A pink city bus at a bus stop.', 'a big yellow school bus shown through the rear view of another school bus', 'A yellow and blue bus drives down the road.']","A bus might be painted half pink and half yellow for promotional or advertising purposes, to stand out and attract attention.","A bus might be painted half pink and half yellow for promotional reasons, to stand out as a unique design or branding strategy, or to raise awareness for a cause or event.,",0.0,0.0,0.0,0.0,0.0,0.8882356616352827
"What could the room with a table, laptops, monitor screens, keyboards, and phones represent?,","['A desk with a laptop, two monitors, two speakers, a keyboard, a mouse and a cell phone on it. ', 'A table with a laptop, phone and other devices sitting on it.', 'a table with a laptop and various other items.']",The room could represent a workspace or an office.,"The room with a table, laptops, monitor screens, keyboards, and phones could represent a workspace or an office setting where people work on computers and communicate electronically.,",1.0,0.0,0.99999999995,1.0,0.12499999984374999,0.7319289183471405
"What is the dense cake described as containing, and where is it placed?,","['Small pieces of cake have been arranged on a plate', 'a plate filled with some little tiny cakes ', 'Closeup of a cake that is sitting on a white plate.']",The dense cake is described as sitting on a white plate.,"The dense cake contains fruit and is sitting on a blue plate.,",0.0,0.9193252639904363,0.0,0.0,0.3333333322222222,0.2331564008504421
